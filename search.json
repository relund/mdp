[{"path":"http://relund.github.io/mdp/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"http://relund.github.io/mdp/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"http://relund.github.io/mdp/articles/introduction.html","id":"an-infinite-semi-mdp","dir":"Articles","previous_headings":"","what":"An infinite Semi-MDP","title":"An introduction to the MDP2 package in R","text":"infinite-horizon semi-MDP considers sequential decision problem infinite number stages. Let \\(\\) denote finite set system states stage \\(n\\). Note assume semi-MDP homogeneous, .e state space independent stage number. state \\(\\\\) observed, action \\(\\) finite set allowable actions \\(()\\) must chosen generates reward \\(r(,)\\). Moreover, let \\(\\tau(,)\\) denote stage length action \\(\\), .e. expected time next decision epoch (stage \\(n+1\\)) given action \\(\\) state \\(\\). Finally, let \\(p_{ij}()\\) denote transition probability obtaining state \\(j\\\\) stage \\(n+1\\) given action \\(\\) chosen state \\(\\) stage \\(n\\). policy decision rule/function assigns state process action. Let us consider example 6.1.1 Tijms (2003). beginning day piece equipment inspected reveal actual working condition. equipment found one working conditions \\(= 1,\\ldots, N\\) working condition \\(\\) better working condition \\(+1\\). equipment deteriorates time. present working condition \\(\\) repair done, beginning next day equipment working condition \\(j\\) probability \\(q_{ij}\\). assumed \\(q_{ij}=0\\) \\(j<\\) \\(\\sum_{j\\geq }q_{ij}=1\\). working condition \\(=N\\) represents malfunction requires enforced repair taking two days. intermediate states \\(\\) \\(1<<N\\) choice preventively repairing equipment letting equipment operate present day. preventive repair takes one day. repaired system working condition \\(=1\\). cost enforced repair upon failure \\(C_{f}\\) cost preemptive repair working condition \\(\\) \\(C_{p}()\\). wish determine maintenance rule minimizes long-run average repair cost per day. formulate problem infinite horizon semi-MDP set possible states system chosen \\[ =\\{1,2,\\ldots,N\\}. \\] State \\(\\) corresponds situation inspection reveals working condition \\(\\). Define actions \\[ =\\left\\{\\begin{array}{ll} 0 & \\text{repair.}\\\\ 1 & \\text{preventive repair.}\\\\ 2 & \\text{forced repair.}\\\\ \\end{array}\\right. \\] set possible actions state \\(\\) chosen \\((1)=\\{0\\},\\ ()=\\{0,1\\}\\) \\(1<<N, (N)=\\{2\\}\\). one-step transition probabilities \\(p_{ij}()\\) given \\(p_{ij}(0) = q_{ij}\\) \\(1\\leq <N\\), \\(p_{i1}(1) = 1\\) \\(1<<N\\), \\(p_{N1}(2)=1\\) zero otherwise. one-step costs \\(c_{}()\\) given \\(c_{}(0)=0,\\ c_{}(1)=C_{p}()\\) \\(c_{N}(2)=C_{f}\\). stage length next decision epoch \\(\\tau(,) = 1, 0\\leq < N\\) \\(\\tau(N,) = 2\\). Assume number possible working conditions equals \\(N=5\\). repair costs given \\(C_{f}=10,\\ C_{p}(2)=7,\\ C_{p}(3)=7\\) \\(C_{p}(4)=5\\). deterioration probabilities \\(q_{ij}\\) given state-expanded hypergraph representing semi-MDP infinite time-horizon shown . node corresponds specific state MDP given stage assigned unique id (id must always start zero). directed hyperarc defined possible action. instance, state/node id 1 corresponds working condition \\(=2\\) two hyperarcs head node corresponds two actions preventive repair. Note tails hyperare represent possible transition (\\(p_{ij}()>0\\)).  build semi-MDP R, use binaryMDPWriter model can built using either matrices hierarchical structure. first illustrate use hierarchical structure. First, load parameters: make data frame states: build model need transition probabilities state ids corresponding transitions. using function: can now build model using binaryMDPWriter: Note build model two weights applied action “Duration” “Net reward”. , specify action, must add two weights. “Duration” equals 1 day except state \\(=N\\) forced repair takes 2 days. process built using first process contains stage (specify one stage, since homogeneous semi-MDP infinite horizon) contains states contains actions. Transitions action specified using pr id parameter. model saved set files prefix “hct611-1_”. model can loaded using Note loaded model gives node state-expanded hypergraph unique id can identify states. ids equal ids used built model, since order nodes hypergraph data structure optimized! Given model memory, now can find optimal policy various policies. Let us first try optimize average reward per time unit. Note optimal preventive repair state \\(=4\\). Let us try optimize expected total discounted reward using policy iteration value iteration. Note given discount factor 0.5, optimal preventive repair state \\(=4\\). model can also built specifying set matrices. Note way specifying work infinite-horizon semi-MDPs (finite-horizon hierarchical models). Specify list probability matrices (one action) row/state contains transition probabilities (zero action used state), matrix rewards matrix stage lengths (row = state, column = action). Let us try build solve model .","code":"> N<-5; Cf<- -10; Cp<-c(0,-7,-7,-5) # use negative numbers since the MDP optimize based on rewards > Q <- matrix(c( >    0.90, 0.10, 0, 0, 0, >    0, 0.80, 0.10, 0.05, 0.05, >    0, 0, 0.70, 0.10, 0.20, >    0, 0, 0, 0.50, 0.50), nrow=4, byrow=T) > states<-data.frame(id=1:N-1,label=paste0(\"i=\",1:N), stringsAsFactors = F) > states id label 1  0   i=1 2  1   i=2 3  2   i=3 4  3   i=4 5  4   i=5 > # transform state to id > state2Id<-function(i) return(i-1) >  > # input state i and action a > transPr<-function(a,i) { +    if (a==0) { +       pr<-Q[i,] +       iN<-which(pr>0) +       pr<-pr[iN]       # only consider trans pr > 0 +    } +    if (a>0) { +       pr<-1 +       iN<-1 +    } +    return(list(pr=pr,id=state2Id(iN))) + } > transPr(0,1) $pr   1   2  0.9 0.1   $id 1 2  0 1 > # Build the model which is stored in a set of binary files > w<-binaryMDPWriter(\"hct611-1_\") > w$setWeights(c(\"Duration\",\"Net reward\")) > w$process() >    w$stage() >       w$state(label=\"i=1\") >          dat<-transPr(0,1) >          w$action(label=\"no repair\", weights=c(1,0), pr=dat$pr, id=dat$id, end=T) >       w$endState() >       for (ii in 2:(N-1) ) { +          w$state(label=states$label[ii]) +             dat<-transPr(0,ii) +             w$action(label=\"no repair\", weights=c(1,0), pr=dat$pr, id=dat$id, end=T) +             dat<-transPr(1,ii) +             w$action(label=\"preventive repair\", weights=c(1,Cp[ii]), pr=dat$pr, id=dat$id, end=T) +          w$endState() +       } >       w$state(label=paste0(\"i=\",N)) >          dat<-transPr(2,N) >          w$action(label=\"forced repair\", weights=c(2,Cf), pr=dat$pr, id=dat$id, end=T) >       w$endState() >    w$endStage() > w$endProcess() > w$closeWriter() > mdp<-loadMDP(\"hct611-1_\") Read binary files (0.000142001 sec.) Build the HMDP (3.6401e-05 sec.) Checking MDP and found no errors (2.1e-06 sec.) > mdp # overall info $binNames [1] \"hct611-1_stateIdx.bin\"          \"hct611-1_stateIdxLbl.bin\"       [3] \"hct611-1_actionIdx.bin\"         \"hct611-1_actionIdxLbl.bin\"      [5] \"hct611-1_actionWeight.bin\"      \"hct611-1_actionWeightLbl.bin\"   [7] \"hct611-1_transProb.bin\"         \"hct611-1_externalProcesses.bin\"  $timeHorizon [1] Inf  $states [1] 5  $founderStatesLast [1] 5  $actions [1] 8  $levels [1] 1  $weightNames [1] \"Duration\"   \"Net reward\"  $ptr C++ object <0x55dec67b82a0> of class 'HMDP' <0x55dec5bc8370>  attr(,\"class\") [1] \"MDP:C++\" > info<-infoMDP(mdp)  # more detailed info > info$actionDF sId aIdx             label weights   trans                pr 1   5    0         no repair     1,0     0,1           0.9,0.1 2   6    0         no repair     1,0 1,2,3,4 0.8,0.1,0.05,0.05 3   6    1 preventive repair    1,-7       0                 1 4   7    0         no repair     1,0   2,3,4       0.7,0.1,0.2 5   7    1 preventive repair    1,-7       0                 1 6   8    0         no repair     1,0     3,4           0.5,0.5 7   8    1 preventive repair    1,-5       0                 1 8   9    0     forced repair   2,-10       0                 1 > info$stateDF sId stateStr label 1    0      1,0       2    1      1,1       3    2      1,2       4    3      1,3       5    4      1,4       6    5      0,0   i=1 7    6      0,1   i=2 8    7      0,2   i=3 9    8      0,3   i=4 10   9      0,4   i=5 > # Optimal policy under average reward per time unit criterion > policyIteAve(mdp,\"Net reward\",\"Duration\") Run policy iteration under average reward criterion using  reward 'Net reward' over 'Duration'. Iterations (g):  1 (-0.512821) 2 (-0.446154) 3 (-0.43379) 4 (-0.43379) finished. Cpu time: 2.1e-06 sec. [1] -0.43379 > getPolicy(mdp) sId stateLabel aIdx       actionLabel   weight 1   5        i=1    0         no repair 9.132420 2   6        i=2    0         no repair 4.794521 3   7        i=3    0         no repair 2.968037 4   8        i=4    1 preventive repair 4.566210 5   9        i=5    0     forced repair 0.000000 > # Optimal policy under expected discounted reward criterion (use both policy and value ite) > policyIteDiscount(mdp,\"Net reward\",\"Duration\", discountFactor = 0.5) Run policy iteration using quantity 'Net reward' under discounting criterion  with 'Duration' as duration using discount factor 0.5.  Iteration(s): 1 2 finished. Cpu time: 2.1e-06 sec. > getPolicy(mdp) sId stateLabel aIdx   actionLabel       weight 1   5        i=1    0     no repair  -0.06420546 2   6        i=2    0     no repair  -0.70626003 3   7        i=3    0     no repair  -1.79775281 4   8        i=4    0     no repair  -3.33868379 5   9        i=5    0 forced repair -10.01605136 > valueIte(mdp,\"Net reward\",\"Duration\", discountFactor = 0.5, eps = 1e-10, maxIte = 1000) Run value iteration with epsilon = 1e-10 at most 1000 time(s) using quantity 'Net reward' under expected discounted reward criterion  with 'Duration' as duration using discount factor 0.5. Iterations: 33 Finished. Cpu time 2.69e-05 sec. > getPolicy(mdp) sId stateLabel aIdx   actionLabel       weight 1   5        i=1    0     no repair  -0.06420546 2   6        i=2    0     no repair  -0.70626003 3   7        i=3    0     no repair  -1.79775281 4   8        i=4    0     no repair  -3.33868379 5   9        i=5    0 forced repair -10.01605136 > ## define probability matrices > P<-list() > # a=1 (no repair) > P[[1]]<-as.matrix(rbind(Q,0)) > # a=2 (preventive repair) > Z <- matrix(0, nrow = N, ncol = N) > Z[2,1]<-Z[3,1]<-Z[4,1]<-1 > P[[2]]<-Z > # a=3 (forced repair) > Z <- matrix(0, nrow = N, ncol = N) > Z[5,1]<-1 > P[[3]]<-Z > # reward 6x3 matrix with one column for each action > R <- matrix(0, nrow = N, ncol = 3) > R[2:4,2]<-Cp[2:4] > R[5,3]<-Cf > # state lengths > D <- matrix(1, nrow = N, ncol = 3) > D[5,3]<-2 >  > # build model > w<-binaryMDPWriter(\"hct611-2_\") > w$setWeights(c(\"Duration\",\"Net reward\")) > w$process(P,R,D) > w$closeWriter() Statistics:     states : 5      actions: 8      weights: 2     Closing binary MDP writer. > mdp<-loadMDP(\"hct611-2_\") Read binary files (0.000161701 sec.) Build the HMDP (3.44e-05 sec.) Checking MDP and found no errors (1.6e-06 sec.) > policyIteAve(mdp,\"Net reward\",\"Duration\") Run policy iteration under average reward criterion using  reward 'Net reward' over 'Duration'. Iterations (g):  1 (-0.512821) 2 (-0.446154) 3 (-0.43379) 4 (-0.43379) finished. Cpu time: 1.6e-06 sec. [1] -0.43379 > getPolicy(mdp) sId stateLabel aIdx actionLabel   weight 1   5          1    0           1 9.132420 2   6          2    0           1 4.794521 3   7          3    0           1 2.968037 4   8          4    1           2 4.566210 5   9          5    0           3 0.000000"},{"path":"http://relund.github.io/mdp/articles/introduction.html","id":"a-finite-horizon-semi-mdp","dir":"Articles","previous_headings":"","what":"A finite-horizon Semi-MDP","title":"An introduction to the MDP2 package in R","text":"finite-horizon semi-MDP considers sequential decision problem \\(N\\) stages. Let \\(I_{n}\\) denote finite set system states stage \\(n\\). state \\(\\I_{n}\\) observed, action \\(\\) finite set allowable actions \\(A_n()\\) must chosen, decision generates reward \\(r_{n}(,)\\). Moreover, let \\(\\tau_n(,)\\) denote stage length action \\(\\), .e. expected time next decision epoch (stage \\(n+1\\)) given action \\(\\) state \\(\\). Finally, let \\(p_{ij}(,n)\\) denote transition probability obtaining state \\(j\\I_{n+1}\\) stage \\(n+1\\) given action \\(\\) chosen state \\(\\) stage \\(n\\). Consider small machine repair problem used example Nielsen Kristensen (2006) machine always replaced 4 years. state machine may : good, average, working. Given machine’s state may maintain machine. case machine’s state good next decision epoch. Otherwise, machine’s state better next decision epoch. machine bought may either state good average. Moreover, machine working must replaced. problem replace machine can modelled using Markov decision process \\(N=5\\) decision epochs. use system states good, average, working dummy state replaced together actions buy (buy), maintain (mt), maintenance (nmt), replace (rep). set states stage zero \\(S_{0}\\) contains single dummy state dummy representing machine knowing initial state. possible action buy. cost buying machine 100 transition probability 0.7 state good 0.3 state average. reward (scrap value) replacing machine 30, 10, 5 state good, average working, respectively. reward machine given action mt 55, 40, 30 state good, average working, respectively. Moreover, system enters state 0 probability 1 next stage. Finally, reward, transition states probabilities given action \\(=\\)nmt given : semi-MDP time-horizon \\(N=5\\) illustrated . node corresponds specific state directed hyperarc defined possible action. instance, action mt (maintain) corresponds deterministic transition state good action nmt (maintain) corresponds transition condition/state better current condition/state. buy machine stage 1 may choose replace machine.  build semi-MDP using binaryMDPWriter: Note stage states numbered using id’s starting zero e.g.  w$action(label=\"nmt\", weights=50, pr=c(0.2,0.8), id=c(1,2), end=TRUE) define action transition states id’s 1 2 next stage probability 0.2 0.8, respectively. Let us try load model get info: Let us use value iteration find optimal policy maximizing expected total reward assumption terminal values 30,10,5,0. optimal policy illustrated :  Note given optimal policy machine never make transition states working replaced. may evaluate certain policy, e.g. policy always maintain machine: policy specified setPolicy contain states actions previous optimal policy used. output can see policy now maintain always. However, reward policy updated. Let us calculate expected reward: , expected reward 90.5 compared 102.2 reward optimal policy.","code":"> prefix<-\"machine1_\" > w <- binaryMDPWriter(prefix) > w$setWeights(c(\"Net reward\")) > w$process() >   w$stage()   # stage n=0 >       w$state(label=\"Dummy\")        >           w$action(label=\"buy\", weights=-100, pr=c(0.7,0.3), id=c(0,1), end=TRUE) >       w$endState() >   w$endStage() >   w$stage()   # stage n=1 >       w$state(label=\"good\")            >           w$action(label=\"mt\", weights=55, pr=1, id=0, end=TRUE) >           w$action(label=\"nmt\", weights=70, pr=c(0.6,0.4), id=c(0,1), end=TRUE) >       w$endState() >       w$state(label=\"average\")         >           w$action(label=\"mt\", weights=40, pr=1, id=0, end=TRUE) >           w$action(label=\"nmt\", weights=50, pr=c(0.6,0.4), id=c(1,2), end=TRUE) >       w$endState() >   w$endStage() >   w$stage()   # stage n=2 >       w$state(label=\"good\")           >           w$action(label=\"mt\", weights=55, pr=1, id=0, end=TRUE) >           w$action(label=\"nmt\", weights=70, pr=c(0.5,0.5), id=c(0,1), end=TRUE) >       w$endState() >       w$state(label=\"average\")        >           w$action(label=\"mt\", weights=40, pr=1, id=0, end=TRUE) >           w$action(label=\"nmt\", weights=50, pr=c(0.5,0.5), id=c(1,2), end=TRUE) >       w$endState() >       w$state(label=\"not working\")     >           w$action(label=\"mt\", weights=30, pr=1, id=0, end=TRUE) >           w$action(label=\"rep\", weights=5, pr=1, id=3, end=TRUE) >       w$endState() >   w$endStage() >   w$stage()   # stage n=3 >       w$state(label=\"good\")            >           w$action(label=\"mt\", weights=55, pr=1, id=0, end=TRUE) >           w$action(label=\"nmt\", weights=70, pr=c(0.2,0.8), id=c(0,1), end=TRUE) >       w$endState() >       w$state(label=\"average\")        >           w$action(label=\"mt\", weights=40, pr=1, id=0, end=TRUE) >           w$action(label=\"nmt\", weights=50, pr=c(0.2,0.8), id=c(1,2), end=TRUE) >       w$endState() >       w$state(label=\"not working\")     >           w$action(label=\"mt\", weights=30, pr=1, id=0, end=TRUE) >           w$action(label=\"rep\", weights=5, pr=1, id=3, end=TRUE) >       w$endState() >       w$state(label=\"replaced\")        >           w$action(label=\"Dummy\", weights=0, pr=1, id=3, end=TRUE) >       w$endState() >   w$endStage() >   w$stage()   # stage n=4 >       w$state(label=\"good\", end=TRUE)         >       w$state(label=\"average\", end=TRUE)      >       w$state(label=\"not working\", end=TRUE)  >       w$state(label=\"replaced\", end=TRUE)    >   w$endStage() > w$endProcess() > w$closeWriter() > mdp<-loadMDP(\"machine1_\") Read binary files (0.000180401 sec.) Build the HMDP (5.1501e-05 sec.) Checking MDP and found no errors (1.4e-06 sec.) > mdp # overall info $binNames [1] \"machine1_stateIdx.bin\"          \"machine1_stateIdxLbl.bin\"       [3] \"machine1_actionIdx.bin\"         \"machine1_actionIdxLbl.bin\"      [5] \"machine1_actionWeight.bin\"      \"machine1_actionWeightLbl.bin\"   [7] \"machine1_transProb.bin\"         \"machine1_externalProcesses.bin\"  $timeHorizon [1] 5  $states [1] 14  $founderStatesLast [1] 4  $actions [1] 18  $levels [1] 1  $weightNames [1] \"Net reward\"  $ptr C++ object <0x55dec375ae50> of class 'HMDP' <0x55dec5bc8370>  attr(,\"class\") [1] \"MDP:C++\" > info<-infoMDP(mdp)  # more detailed info > info$actionDF sId aIdx label weights trans      pr 1    4    0    mt      55     0       1 2    4    1   nmt      70   0,1 0.2,0.8 3    5    0    mt      40     0       1 4    5    1   nmt      50   1,2 0.2,0.8 5    6    0    mt      30     0       1 6    6    1   rep       5     3       1 7    7    0 Dummy       0     3       1 8    8    0    mt      55     4       1 9    8    1   nmt      70   4,5 0.5,0.5 10   9    0    mt      40     4       1 11   9    1   nmt      50   5,6 0.5,0.5 12  10    0    mt      30     4       1 13  10    1   rep       5     7       1 14  11    0    mt      55     8       1 15  11    1   nmt      70   8,9 0.6,0.4 16  12    0    mt      40     8       1 17  12    1   nmt      50  9,10 0.6,0.4 18  13    0   buy    -100 11,12 0.7,0.3 > info$stateDF sId stateStr       label 1    0      4,0        good 2    1      4,1     average 3    2      4,2 not working 4    3      4,3    replaced 5    4      3,0        good 6    5      3,1     average 7    6      3,2 not working 8    7      3,3    replaced 9    8      2,0        good 10   9      2,1     average 11  10      2,2 not working 12  11      1,0        good 13  12      1,1     average 14  13      0,0       Dummy > scrapValues<-c(30,10,5,0)   # scrap values (the values of the 4 states at stage 4) > valueIte(mdp, \"Net reward\" , termValues=scrapValues) Run value iteration with epsilon = 0 at most 1 time(s) using quantity 'Net reward' under reward criterion.  Finished. Cpu time 1.36e-05 sec. > getPolicy(mdp) sId  stateLabel aIdx actionLabel weight 1    0        good   -1               30.0 2    1     average   -1               10.0 3    2 not working   -1                5.0 4    3    replaced   -1                0.0 5    4        good    0          mt   85.0 6    5     average    0          mt   70.0 7    6 not working    0          mt   60.0 8    7    replaced    0       Dummy    0.0 9    8        good    1         nmt  147.5 10   9     average    0          mt  125.0 11  10 not working    0          mt  115.0 12  11        good    1         nmt  208.5 13  12     average    0          mt  187.5 14  13       Dummy    0         buy  102.2 > policy<-data.frame(sId=c(8,11),aIdx=c(0,0)) > setPolicy(mdp, policy) > getPolicy(mdp) sId  stateLabel aIdx actionLabel weight 1    0        good   -1               30.0 2    1     average   -1               10.0 3    2 not working   -1                5.0 4    3    replaced   -1                0.0 5    4        good    0          mt   85.0 6    5     average    0          mt   70.0 7    6 not working    0          mt   60.0 8    7    replaced    0       Dummy    0.0 9    8        good    0          mt  147.5 10   9     average    0          mt  125.0 11  10 not working    0          mt  115.0 12  11        good    0          mt  208.5 13  12     average    0          mt  187.5 14  13       Dummy    0         buy  102.2 > calcWeights(mdp, \"Net reward\", termValues=scrapValues) > getPolicy(mdp) sId  stateLabel aIdx actionLabel weight 1    0        good   -1               30.0 2    1     average   -1               10.0 3    2 not working   -1                5.0 4    3    replaced   -1                0.0 5    4        good    0          mt   85.0 6    5     average    0          mt   70.0 7    6 not working    0          mt   60.0 8    7    replaced    0       Dummy    0.0 9    8        good    0          mt  140.0 10   9     average    0          mt  125.0 11  10 not working    0          mt  115.0 12  11        good    0          mt  195.0 13  12     average    0          mt  180.0 14  13       Dummy    0         buy   90.5"},{"path":"http://relund.github.io/mdp/articles/introduction.html","id":"an-infinite-horizon-hmdp","dir":"Articles","previous_headings":"","what":"An infinite-horizon HMDP","title":"An introduction to the MDP2 package in R","text":"hierarchical MDP MDP parameters defined special way, nevertheless accordance usual rules conditions relating processes (Kristensen Jørgensen (2000)). basic idea hierarchical structure stages process can expanded -called child process, may expand stages new child processes leading multiple levels. illustrate consider HMDP shown figure . process three levels. Level 2 set finite-horizon semi-MDPs (one oval box) can represented using state-expanded hypergraph (hyperarcs shown, hyperarcs connecting processes shown). semi-MDP Level 2 uniquely defined given state \\(s\\) action \\(\\) parent process Level 1 (illustrated arcs head tail node Level 1 Level 2, respectively). Moreover, child process Level 2 terminates transition state \\(s\\\\mathcal{S}_{N}\\) child process state next stage parent process occur (illustrated (hyper)arcs head tail Level 2 Level 1, respectively). hypergraph representation first stage hierarchical MDP. Level 0 indicate founder level, nodes indicates states different levels. child process (oval box) represented using state-expanded hypergraph (hyperarcs shown) uniquely defined given state action parent process. Since child process always defined stage, state action parent process instance state Level 1 can identified using index vector \\(\\nu=(n_{0},s_{0},a_{0},n_{1},s_{1})\\) \\(s_1\\) state id given stage \\(n_1\\) process defined action \\(a_0\\) state \\(s_0\\) stage \\(n_0\\). Note values ids starting zero, e.g. \\(s_1=0\\) first state corresponding stage \\(a_0=2\\) third action corresponding state. general state \\(s\\) action \\(\\) level \\(l\\) can uniquely identified using \\[ \\begin{aligned} \\nu_{s}&=(n_{0},s_{0},a_{0},n_{1},s_{1},\\ldots,n_{l},s_{l}) \\\\ \\nu_{}&=(n_{0},s_{0},a_{0},n_{1},s_{1},\\ldots,n_{l},s_{l},a_{l}). \\end{aligned} \\] index vectors state \\(v_0\\), \\(v_1\\) \\(v_2\\) illustrated figure. semi-MDP another way identify state state-expanded hypergraph using unique id. Let us try solve small problem livestock farming, namely cow replacement problem want represent age cow, .e. lactation number cow. lactation cow may high, average low yield. assume cow always replaced 4 lactations. addition lactation milk yield also want take genetic merit account either bad, average good. cow replaced assume probability bad, average good heifer equal. formulate problem HMDP 2 levels. level 0 states genetic merit length stage life cow. level 1 stage describe lactation states describe yield. Decisions level 1 keep replace. Note MDP runs infinite time-horizon founder level state (genetic merit) define semi-MDP level 1 4 lactations. generate MDP need know weights transition probabilities provided csv file. ease understanding provide 2 functions reading csv: can now generate model three weights Note model built using prob parameter contains triples (scope,id,pr). Scope can : 2 = transition child process (stage zero child process), 1 = transition next stage current process 0 = transition next stage father process. instance prob = c(1,2,0.3, 0,0,0.7) specify transitions third state (id = 2) next stage process first state (id = 0) next stage father process probabilities 0.3 0.7, respectively. plot state-expanded hypergraph given action keep drawn orange color action replace blue color.  find optimal policy expected discounted reward criterion HMDP using policy iteration: plot optimal policy can seen .  may also find policy maximizing average reward per lactation: Since weights defined action can calculate average reward per litre milk optimal policy: average yield per lactation:","code":"> cowDf<-read.csv(\"vignette_files/cow.csv\") > head(cowDf) s0 n1 s1   label Duration Reward Output scp0 idx0       pr0 scp1 idx1       pr1 scp2 1  0  0  0   Dummy        0      0      0    1    0 0.3333333    1    1 0.3333333    1 2  0  1  0    Keep        1   6000   3000    1    0 0.6000000    1    1 0.3000000    1 3  0  1  0 Replace        1   5000   3000    0    0 0.3333333    0    1 0.3333333    0 4  0  1  1    Keep        1   8000   4000    1    0 0.2000000    1    1 0.6000000    1 5  0  1  1 Replace        1   7000   4000    0    0 0.3333333    0    1 0.3333333    0 6  0  1  2    Keep        1  10000   5000    1    0 0.1000000    1    1 0.3000000    1   idx2       pr2 1    2 0.3333333 2    2 0.1000000 3    2 0.3333333 4    2 0.2000000 5    2 0.3333333 6    2 0.6000000 > lev1W<-function(s0Idx,n1Idx,s1Idx,a1Lbl) { +   r<-subset(cowDf,s0==s0Idx & n1==n1Idx & s1==s1Idx & label==a1Lbl) +   return(as.numeric(r[5:7])) + } > lev1W(2,2,1,'Keep')     # good genetic merit, lactation 2, avg yield, keep action [1]     1 14000  7000 > lev1Pr<-function(s0Idx,n1Idx,s1Idx,a1Lbl) { +   r<-subset(cowDf,s0==s0Idx & n1==n1Idx & s1==s1Idx & label==a1Lbl) +   return(as.numeric(r[8:16])) + } > lev1Pr(2,2,1,'Replace') # good genetic merit, lactation 2, avg yield, replace action [1] 0.0000000 0.0000000 0.3333333 0.0000000 1.0000000 0.3333333 0.0000000 2.0000000 [9] 0.3333333 > lblS0<-c('Bad genetic level','Avg genetic level','Good genetic level') > lblS1<-c('Low yield','Avg yield','High yield') > prefix<-\"cow_\" > w<-binaryMDPWriter(prefix) > w$setWeights(c(\"Duration\", \"Net reward\", \"Yield\")) > w$process() >   w$stage()   # stage 0 at founder level >       for (s0 in 0:2) { +           w$state(label=lblS0[s0+1])   # state at founder +               w$action(label=\"Keep\", weights=c(0,0,0), prob=c(2,0,1))   # action at founder +                   w$process() +                       w$stage()   # dummy stage at level 1 +                            w$state(label=\"Dummy\") +                               w$action(label=\"Dummy\", weights=c(0,0,0),  +                                        prob=c(1,0,1/3, 1,1,1/3, 1,2,1/3), end=TRUE) +                            w$endState() +                       w$endStage() +                       for (d1 in 1:4) { +                           w$stage()   # stage at level 1 +                               for (s1 in 0:2) { +                                   w$state(label=lblS1[s1+1]) +                                       if (d1!=4) { +                                           w$action(label=\"Keep\", weights=lev1W(s0,d1,s1,\"Keep\"),  +                                                    prob=lev1Pr(s0,d1,s1,\"Keep\"), end=TRUE) +                                       } +                                       w$action(label=\"Replace\", weights=lev1W(s0,d1,s1,\"Replace\"),  +                                                prob=lev1Pr(s0,d1,s1,\"Replace\"), end=TRUE) +                                   w$endState() +                               } +                           w$endStage() +                       } +                   w$endProcess() +               w$endAction() +           w$endState() +       } >   w$endStage() > w$endProcess() > w$closeWriter() Statistics:     states : 42      actions: 69      weights: 3     Closing binary MDP writer. > ## solve under discount criterion > mdp<-loadMDP(prefix) Read binary files (0.000288902 sec.) Build the HMDP (0.000299403 sec.) Checking MDP and found no errors (3.7e-06 sec.) > wLbl<-\"Net reward\"         # the weight we want to optimize (net reward) > durLbl<-\"Duration\"         # the duration/time label > policyIteDiscount(mdp, wLbl, durLbl, rate=0.1) Run policy iteration using quantity 'Net reward' under discounting criterion  with 'Duration' as duration using discount factor 0.904837.  Iteration(s): 1 2 3 4 finished. Cpu time: 3.7e-06 sec. > getPolicy(mdp) sId         stateLabel aIdx actionLabel   weight 1    3          Low yield    0     Replace 118594.1 2    4          Avg yield    0     Replace 120594.1 3    5         High yield    0     Replace 122594.1 4    6          Low yield    0        Keep 120213.2 5    7          Avg yield    0        Keep 123118.1 6    8         High yield    0        Keep 126022.9 7    9          Low yield    0        Keep 122087.6 8   10          Avg yield    0        Keep 125401.8 9   11         High yield    0        Keep 128716.0 10  12          Low yield    0        Keep 121968.9 11  13          Avg yield    0        Keep 125468.3 12  14         High yield    0        Keep 128967.7 13  15              Dummy    0       Dummy 125468.3 14  16          Low yield    0     Replace 116594.1 15  17          Avg yield    0     Replace 118594.1 16  18         High yield    0     Replace 120594.1 17  19          Low yield    1     Replace 117594.1 18  20          Avg yield    1     Replace 119594.1 19  21         High yield    0        Keep 122213.2 20  22          Low yield    1     Replace 117594.1 21  23          Avg yield    0        Keep 120325.3 22  24         High yield    0        Keep 123454.2 23  25          Low yield    0        Keep 115675.2 24  26          Avg yield    0        Keep 118946.8 25  27         High yield    0        Keep 122326.4 26  28              Dummy    0       Dummy 118982.8 27  29          Low yield    0     Replace 114594.1 28  30          Avg yield    0     Replace 116594.1 29  31         High yield    0     Replace 118594.1 30  32          Low yield    1     Replace 115594.1 31  33          Avg yield    1     Replace 117594.1 32  34         High yield    1     Replace 119594.1 33  35          Low yield    1     Replace 115594.1 34  36          Avg yield    1     Replace 117594.1 35  37         High yield    1     Replace 119594.1 36  38          Low yield    1     Replace 113594.1 37  39          Avg yield    1     Replace 115594.1 38  40         High yield    1     Replace 117594.1 39  41              Dummy    0       Dummy 115594.1 40  42  Bad genetic level    0        Keep 115594.1 41  43  Avg genetic level    0        Keep 118982.8 42  44 Good genetic level    0        Keep 125468.3 > # rpo<-calcRPO(mdp, wLbl, iA=rep(0,42), criterion=\"discount\", dur=durLbl, rate=rate, rateBase=rateBase) > # policy<-merge(policy,rpo) > # policy > wLbl<-\"Net reward\"         # the weight we want to optimize (net reward) > durLbl<-\"Duration\"         # the duration/time label > policyIteAve(mdp, wLbl, durLbl) Run policy iteration under average reward criterion using  reward 'Net reward' over 'Duration'. Iterations (g):  1 (11000) 2 (11517.5) 3 (11543.8) 4 (11543.8) finished. Cpu time: 3.7e-06 sec. [1] 11543.83 > getPolicy(mdp) sId         stateLabel aIdx actionLabel        weight 1    3          Low yield    0     Replace -7.368515e+03 2    4          Avg yield    0     Replace -5.368515e+03 3    5         High yield    0     Replace -3.368515e+03 4    6          Low yield    0        Keep -5.912343e+03 5    7          Avg yield    0        Keep -2.912343e+03 6    8         High yield    0        Keep  8.765653e+01 7    9          Low yield    0        Keep -3.956172e+03 8   10          Avg yield    0        Keep -4.561717e+02 9   11         High yield    0        Keep  3.043828e+03 10  12          Low yield    0        Keep -3.750000e+03 11  13          Avg yield    0        Keep  7.958079e-13 12  14         High yield    0        Keep  3.750000e+03 13  15              Dummy    0       Dummy  9.094947e-13 14  16          Low yield    0     Replace -9.368515e+03 15  17          Avg yield    0     Replace -7.368515e+03 16  18         High yield    0     Replace -5.368515e+03 17  19          Low yield    1     Replace -8.368515e+03 18  20          Avg yield    1     Replace -6.368515e+03 19  21         High yield    0        Keep -3.912343e+03 20  22          Low yield    1     Replace -8.368515e+03 21  23          Avg yield    0        Keep -5.821109e+03 22  24         High yield    0        Keep -2.638640e+03 23  25          Low yield    1     Replace -1.036852e+04 24  26          Avg yield    0        Keep -7.237925e+03 25  27         High yield    0        Keep -3.710197e+03 26  28              Dummy    0       Dummy -7.105546e+03 27  29          Low yield    0     Replace -1.136852e+04 28  30          Avg yield    0     Replace -9.368515e+03 29  31         High yield    0     Replace -7.368515e+03 30  32          Low yield    1     Replace -1.036852e+04 31  33          Avg yield    1     Replace -8.368515e+03 32  34         High yield    1     Replace -6.368515e+03 33  35          Low yield    1     Replace -1.036852e+04 34  36          Avg yield    1     Replace -8.368515e+03 35  37         High yield    1     Replace -6.368515e+03 36  38          Low yield    1     Replace -1.236852e+04 37  39          Avg yield    1     Replace -1.036852e+04 38  40         High yield    1     Replace -8.368515e+03 39  41              Dummy    0       Dummy -1.036852e+04 40  42  Bad genetic level    0        Keep -1.036852e+04 41  43  Avg genetic level    0        Keep -7.105546e+03 42  44 Good genetic level    0        Keep  9.094947e-13 > calcWeights(mdp, w=wLbl, criterion=\"average\", dur = \"Yield\") [1] 1.932615 > calcWeights(mdp, w=\"Yield\", criterion=\"average\", dur = durLbl) [1] 5973.166"},{"path":[]},{"path":"http://relund.github.io/mdp/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Lars Relund Nielsen. Author, maintainer.","code":""},{"path":"http://relund.github.io/mdp/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Relund Nielsen L (2022). MDP2: Markov Decision Processes (MDPs) R. https://relund.github.io/mdp/, https://github.com/relund/mdp/, http://relund.github.io/mdp/.","code":"@Manual{,   title = {MDP2: Markov Decision Processes (MDPs) in R},   author = {Lars {Relund Nielsen}},   year = {2022},   note = {https://relund.github.io/mdp/, https://github.com/relund/mdp/, http://relund.github.io/mdp/}, }"},{"path":"http://relund.github.io/mdp/index.html","id":"markov-decision-processes-mdps-in-r","dir":"","previous_headings":"","what":"Markov Decision Processes (MDPs) in R","title":"Markov Decision Processes (MDPs) in R","text":"R package building solving Markov decision processes (MDP). Create optimize MDPs hierarchical MDPs discrete time steps state space.","code":""},{"path":"http://relund.github.io/mdp/reference/MDP2.html","id":null,"dir":"Reference","previous_headings":"","what":"Markov Decision Processes (MDPs) in R — MDP2","title":"Markov Decision Processes (MDPs) in R — MDP2","text":"Create optimize MDPs hierarchical MDPs discrete time steps state space.","code":""},{"path":"http://relund.github.io/mdp/reference/MDP2.html","id":"to-do","dir":"Reference","previous_headings":"","what":"To do","title":"Markov Decision Processes (MDPs) in R — MDP2","text":"Nested loading memory (specify HMDP special actions containing child + father jump actions) Idea define proc external nested process use w$includeProcess(prefix, transPr, index) (specify child jump action) w$stage() w$state() w$action(...) (specify father jump action scope must zeor) w$action(...) (specify father jump action scope must zeor) w$endState() w$endStage() w$endIncludeProcess() hgf formed subprocess mimic 1. last stage external proc, .e. include jump pr hgf need new binary file \"externalProcess.bin\" storing nested process format \"n0 s0 a0 n1 s1 prefix -1 ...\" specify stage contain states corresponding 1. stage nested process (nested hfg must loaded calculated","code":""},{"path":"http://relund.github.io/mdp/reference/MDP2.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Markov Decision Processes (MDPs) in R — MDP2","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/actionIdxDf.html","id":null,"dir":"Reference","previous_headings":"","what":"Info about the actions in the HMDP model under consideration. — actionIdxDf","title":"Info about the actions in the HMDP model under consideration. — actionIdxDf","text":"Info actions HMDP model consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/actionIdxDf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Info about the actions in the HMDP model under consideration. — actionIdxDf","text":"","code":"actionIdxDf(prefix = \"\", file = \"actionIdx.bin\", labels = \"actionIdxLbl.bin\")"},{"path":"http://relund.github.io/mdp/reference/actionIdxDf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Info about the actions in the HMDP model under consideration. — actionIdxDf","text":"prefix character string prefix added til file(s). file HMDP binary file containing description consideration. labels HMDP binary file containing labels consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/actionIdxDf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Info about the actions in the HMDP model under consideration. — actionIdxDf","text":"data frame columns actionIdxMat plus another column containing labels.","code":""},{"path":"http://relund.github.io/mdp/reference/actionIdxDf.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Info about the actions in the HMDP model under consideration. — actionIdxDf","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/actionIdxMat.html","id":null,"dir":"Reference","previous_headings":"","what":"Info about the actions in the HMDP model under consideration. — actionIdxMat","title":"Info about the actions in the HMDP model under consideration. — actionIdxMat","text":"Info actions HMDP model consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/actionIdxMat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Info about the actions in the HMDP model under consideration. — actionIdxMat","text":"","code":"actionIdxMat(prefix = \"\", file = \"actionIdx.bin\")"},{"path":"http://relund.github.io/mdp/reference/actionIdxMat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Info about the actions in the HMDP model under consideration. — actionIdxMat","text":"prefix character string prefix added til file(s). file HMDP binary file containing description consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/actionIdxMat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Info about the actions in the HMDP model under consideration. — actionIdxMat","text":"matrix columns (aId, ...) aId action row id ... alternating pairs (scp, idx), one possible transition scp scope can 4 values: 2 - transition child process (stage zero child process), 1 - transition next stage current process, 0 - transition next stage father process. idx pair denote index state stage considered. Finally, scope = 3 transition state sId = idx considered.","code":""},{"path":"http://relund.github.io/mdp/reference/actionIdxMat.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Info about the actions in the HMDP model under consideration. — actionIdxMat","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/actionInfo.html","id":null,"dir":"Reference","previous_headings":"","what":"Info about the actions in the HMDP model under consideration. — actionInfo","title":"Info about the actions in the HMDP model under consideration. — actionInfo","text":"Info actions HMDP model consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/actionInfo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Info about the actions in the HMDP model under consideration. — actionInfo","text":"","code":"actionInfo(   prefix = \"\",   file = \"actionIdx.bin\",   weightFile = \"actionWeight.bin\",   transPrFile = \"transProb.bin\",   labels = \"actionIdxLbl.bin\" )"},{"path":"http://relund.github.io/mdp/reference/actionInfo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Info about the actions in the HMDP model under consideration. — actionInfo","text":"prefix character string prefix added til file(s). file HMDP binary file containing description consideration. weightFile HMDP binary file containing action costs. transPrFile HMDP binary file containing transition probabilities. labels HMDP binary file containing labels consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/actionInfo.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Info about the actions in the HMDP model under consideration. — actionInfo","text":"matrix columns actionIdxMat, actionCostMat transProbMat labels NULL. labels NULL data frame returned label column .","code":""},{"path":"http://relund.github.io/mdp/reference/actionInfo.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Info about the actions in the HMDP model under consideration. — actionInfo","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/actionWeightMat.html","id":null,"dir":"Reference","previous_headings":"","what":"Info about the weights of the actions in the HMDP model under consideration. — actionWeightMat","title":"Info about the weights of the actions in the HMDP model under consideration. — actionWeightMat","text":"Info weights actions HMDP model consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/actionWeightMat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Info about the weights of the actions in the HMDP model under consideration. — actionWeightMat","text":"","code":"actionWeightMat(   prefix = \"\",   file = \"actionWeight.bin\",   labels = \"actionWeightLbl.bin\" )"},{"path":"http://relund.github.io/mdp/reference/actionWeightMat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Info about the weights of the actions in the HMDP model under consideration. — actionWeightMat","text":"prefix character string prefix added til file(s). file HMDP binary file containing description consideration. labels HMDP binary file containing labels consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/actionWeightMat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Info about the weights of the actions in the HMDP model under consideration. — actionWeightMat","text":"matrix columns (aId, ...) aId action row id ... weights action.","code":""},{"path":"http://relund.github.io/mdp/reference/actionWeightMat.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Info about the weights of the actions in the HMDP model under consideration. — actionWeightMat","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/binInfoActions.html","id":null,"dir":"Reference","previous_headings":"","what":"Info about the actions in the HMDP model under consideration. — binInfoActions","title":"Info about the actions in the HMDP model under consideration. — binInfoActions","text":"Info actions HMDP model consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/binInfoActions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Info about the actions in the HMDP model under consideration. — binInfoActions","text":"","code":"binInfoActions(   prefix = \"\",   labels = TRUE,   fileA = \"actionIdx.bin\",   filePr = \"transProb.bin\",   fileW = \"actionWeight.bin\",   fileLabelW = \"actionWeightLbl.bin\",   fileLabelA = \"actionIdxLbl.bin\" )"},{"path":"http://relund.github.io/mdp/reference/binInfoActions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Info about the actions in the HMDP model under consideration. — binInfoActions","text":"prefix character string prefix added til binary files. labels labels extracted. fileA binary file containing description actions. filePr binary file containing description transition probabilities. fileW binary file containing description weights. labelA binary file containing action labels. labelW binary file containing weight labels.","code":""},{"path":"http://relund.github.io/mdp/reference/binInfoActions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Info about the actions in the HMDP model under consideration. — binInfoActions","text":"data frame information. Scope string contain scope transitions can 4 values: 2 - transition child process (stage zero child process), 1 - transition next stage current process, 0 - transition next stage father process. Finally, scope = 3 transition state sId = idx considered. index string denote index (id scope = 3) state next stage.","code":""},{"path":"http://relund.github.io/mdp/reference/binInfoActions.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Info about the actions in the HMDP model under consideration. — binInfoActions","text":"model loaded, .e read binary files. state id (sId) loaded model!","code":""},{"path":"http://relund.github.io/mdp/reference/binInfoActions.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Info about the actions in the HMDP model under consideration. — binInfoActions","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/binInfoStates.html","id":null,"dir":"Reference","previous_headings":"","what":"Info about the states in the binary files of the HMDP model under consideration. — binInfoStates","title":"Info about the states in the binary files of the HMDP model under consideration. — binInfoStates","text":"Info states binary files HMDP model consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/binInfoStates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Info about the states in the binary files of the HMDP model under consideration. — binInfoStates","text":"","code":"binInfoStates(   prefix = \"\",   labels = TRUE,   stateStr = TRUE,   fileS = \"stateIdx.bin\",   labelS = \"stateIdxLbl.bin\" )"},{"path":"http://relund.github.io/mdp/reference/binInfoStates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Info about the states in the binary files of the HMDP model under consideration. — binInfoStates","text":"prefix character string prefix added til binary files. labels labels extracted. stateStr state strings extracted. false add columns (n0, s0, a0, ...) n0 index stage level 0, s0 index state a0 index action. HMDP one level columns index (d1, s1, a1, ...) added. fileS binary file containing description states. labelS binary file containing state labels.","code":""},{"path":"http://relund.github.io/mdp/reference/binInfoStates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Info about the states in the binary files of the HMDP model under consideration. — binInfoStates","text":"data frame information.","code":""},{"path":"http://relund.github.io/mdp/reference/binInfoStates.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Info about the states in the binary files of the HMDP model under consideration. — binInfoStates","text":"model loaded, .e read binary files. state id (sId) loaded model!","code":""},{"path":"http://relund.github.io/mdp/reference/binInfoStates.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Info about the states in the binary files of the HMDP model under consideration. — binInfoStates","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/binaryActionWriter.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for writing actions of a HMDP model to binary files. The function defineds\nsubfunctions which can be used to define actions saved in a set of binary\nfiles. It is assumed that the states have been defined using binaryMDPWriter\nand that the id of the states is known (can be retrived using e.g. stateIdxDf). — binaryActionWriter","title":"Function for writing actions of a HMDP model to binary files. The function defineds\nsubfunctions which can be used to define actions saved in a set of binary\nfiles. It is assumed that the states have been defined using binaryMDPWriter\nand that the id of the states is known (can be retrived using e.g. stateIdxDf). — binaryActionWriter","text":"Binary files efficent storing large models. Compared HMP (XML) format binary files use less storage space loading model faster.","code":""},{"path":"http://relund.github.io/mdp/reference/binaryActionWriter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for writing actions of a HMDP model to binary files. The function defineds\nsubfunctions which can be used to define actions saved in a set of binary\nfiles. It is assumed that the states have been defined using binaryMDPWriter\nand that the id of the states is known (can be retrived using e.g. stateIdxDf). — binaryActionWriter","text":"","code":"binaryActionWriter(   prefix = \"\",   binNames = c(\"actionIdx.bin\", \"actionIdxLbl.bin\", \"actionWeight.bin\",     \"actionWeightLbl.bin\", \"transProb.bin\"),   append = TRUE )"},{"path":"http://relund.github.io/mdp/reference/binaryActionWriter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for writing actions of a HMDP model to binary files. The function defineds\nsubfunctions which can be used to define actions saved in a set of binary\nfiles. It is assumed that the states have been defined using binaryMDPWriter\nand that the id of the states is known (can be retrived using e.g. stateIdxDf). — binaryActionWriter","text":"prefix character string prefix added binNames. binNames character vector length 5 giving names binary files storing model. append Logical indicating whether keep currents actions (default - TRUE) defined delete start (FALSE).","code":""},{"path":"http://relund.github.io/mdp/reference/binaryActionWriter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for writing actions of a HMDP model to binary files. The function defineds\nsubfunctions which can be used to define actions saved in a set of binary\nfiles. It is assumed that the states have been defined using binaryMDPWriter\nand that the id of the states is known (can be retrived using e.g. stateIdxDf). — binaryActionWriter","text":"list functions.","code":""},{"path":"http://relund.github.io/mdp/reference/binaryActionWriter.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Function for writing actions of a HMDP model to binary files. The function defineds\nsubfunctions which can be used to define actions saved in a set of binary\nfiles. It is assumed that the states have been defined using binaryMDPWriter\nand that the id of the states is known (can be retrived using e.g. stateIdxDf). — binaryActionWriter","text":"functions can used : setWeights(labels, ...): Set labels weights used actions. labels vector label names, ... used. function must called starting building model. addAction(label=NULL, sIdx, weights, prob, ...): Add action. Parameter sIdx id state defining action, weights must vector action weights, prob matrix (sIdx,pr)  first column contain id transition state (see description actionIdx.bin - scope assumed 3), ... currently used. endAction(): Ends action. closeWriter(): Close writer. Must called model description finished. Five binary files created using following format: actionIdx.bin: File integers containing indexes defining actions format \"sIdx scope idx scope idx scope idx -1 sIdx scope idx scope idx -1 sIdx scope -1 ...\". sIdx corresponds index/line number stateIdx.bin (index starts 0). Next pairs (scope idx) follow indicating possible transitions. Scope can 4 values: 2 - transition child process (stage zero child process), 1 - transition next stage current process, 0 - transition next stage father process. idx pair denote index state stage considered, e.g. scope=1 idx=2 consider state number 3 next stage current process. Finally, scope = 3 transition state specified state sIdx given. , scope=3 idx=5 transition state specified line 6 stateIdxLbl.bin. usefull considering shared child processes. actionIdxLbl.bin: File characters format \"aIdx label aIdx label ...\" aIdx corresponds index/line number actionIdx.bin (index starts 0). Note delimiter used. actionWeight.bin: File doubles containing weights actions format \"c1 c2 c3 c1 c2 c3 ...\" assuming three weights action. actionWeightLbl.bin: File characters containing labels weights format \"lable1 label2 label3\" assuming three weights action. transProb.bin: File doubles containing probabilities transitions defined actions actionIdx.bin. format \"p1 p2 p3 -1 p1 -1 p1 p2 -1 ...\". -1 used indicate new action considered (new line).","code":""},{"path":"http://relund.github.io/mdp/reference/binaryActionWriter.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Function for writing actions of a HMDP model to binary files. The function defineds\nsubfunctions which can be used to define actions saved in a set of binary\nfiles. It is assumed that the states have been defined using binaryMDPWriter\nand that the id of the states is known (can be retrived using e.g. stateIdxDf). — binaryActionWriter","text":"Note indexes starting zero (C/C++ style).","code":""},{"path":"http://relund.github.io/mdp/reference/binaryActionWriter.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Function for writing actions of a HMDP model to binary files. The function defineds\nsubfunctions which can be used to define actions saved in a set of binary\nfiles. It is assumed that the states have been defined using binaryMDPWriter\nand that the id of the states is known (can be retrived using e.g. stateIdxDf). — binaryActionWriter","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/binaryActionWriter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function for writing actions of a HMDP model to binary files. The function defineds\nsubfunctions which can be used to define actions saved in a set of binary\nfiles. It is assumed that the states have been defined using binaryMDPWriter\nand that the id of the states is known (can be retrived using e.g. stateIdxDf). — binaryActionWriter","text":"","code":"#  # # Create a small HMDP with two levels # w<-binaryMDPWriter() # w$setWeights(c(\"Duration\",\"Net reward\",\"Items\")) # w$process() #   w$stage() #     w$state(label=\"M0\") #       w$action(label=\"A0\",weights=c(0,0,0),prob=c(2,0,1)) #         w$process() #           w$stage() #             w$state(label=\"D\") #               w$action(label=\"A0\",weights=c(0,0,1),prob=c(1,0,0.5,1,1,0.5)) #               w$endAction() #             w$endState() #           w$endStage() #           w$stage() #             w$state(label=\"C0\") #               w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #               w$endAction() #               w$action(label=\"A1\",weights=c(1,2,1),prob=c(1,0,0.5,1,1,0.5)) #               w$endAction() #             w$endState() #             w$state(label=\"C1\") #               w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #               w$endAction() #               w$action(label=\"A1\",weights=c(1,2,1),prob=c(1,0,0.5,1,1,0.5)) #               w$endAction() #             w$endState() #           w$endStage() #           w$stage() #             w$state(label=\"C0\") #               w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1)) #               w$endAction() #             w$endState() #             w$state(label=\"C1\") #               w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1)) #               w$endAction() #             w$endState() #           w$endStage() #         w$endProcess() #       w$endAction() #       w$action(label=\"A1\",weights=c(0,0,0),prob=c(2,0,1)) #         w$process() #           w$stage() #             w$state(label=\"D\") #               w$action(label=\"A0\",weights=c(0,0,1),prob=c(1,0,1)) #               w$endAction() #             w$endState() #           w$endStage() #           w$stage() #             w$state(label=\"C0\") #               w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #               w$endAction() #               w$action(label=\"A1\",weights=c(1,2,1),prob=c(1,0,0.5,1,1,0.5)) #               w$endAction() #             w$endState() #           w$endStage() #           w$stage() #             w$state(label=\"C0\") #               w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1)) #               w$endAction() #             w$endState() #             w$state(label=\"C1\") #               w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1)) #               w$endAction() #               w$action(label=\"A1\",weights=c(0,10,5),prob=c(0,0,0.5,0,1,0.5)) #               w$endAction() #             w$endState() #           w$endStage() #         w$endProcess() #       w$endAction() #     w$endState() #     w$state(label=\"M1\") #       w$action(label=\"A0\",weights=c(0,0,0),prob=c(2,0,1)) #         w$process() #           w$stage() #             w$state(label=\"D\") #               w$action(label=\"A0\",weights=c(0,0,1),prob=c(1,0,0.5,1,1,0.5)) #               w$endAction() #             w$endState() #           w$endStage() #           w$stage() #             w$state(label=\"C0\") #               w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #               w$endAction() #             w$endState() #             w$state(label=\"C1\") #               w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #               w$endAction() #             w$endState() #           w$endStage() #           w$stage() #             w$state(label=\"C0\") #               w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1)) #               w$endAction() #             w$endState() #             w$state(label=\"C1\") #               w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1)) #               w$endAction() #             w$endState() #           w$endStage() #         w$endProcess() #       w$endAction() #     w$endState() #   w$endStage() # w$endProcess() # w$closeWriter() #  # #stateIdxDf() # #actionIdxDf() # #actionWeightMat() # #transProbMat() # #a<-actionInfo() # #a                   # ordered by action id # #a[order(a$sId),]    # ordered by state id"},{"path":"http://relund.github.io/mdp/reference/binaryMDPWriter.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for writing an HMDP model to binary files. The function defineds\nsubfunctions which can be used to define an HMDP model saved in a set of binary\nfiles. — binaryMDPWriter","title":"Function for writing an HMDP model to binary files. The function defineds\nsubfunctions which can be used to define an HMDP model saved in a set of binary\nfiles. — binaryMDPWriter","text":"Binary files efficent storing large models. Compared HMP (XML) format binary files use less storage space loads model faster.","code":""},{"path":"http://relund.github.io/mdp/reference/binaryMDPWriter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for writing an HMDP model to binary files. The function defineds\nsubfunctions which can be used to define an HMDP model saved in a set of binary\nfiles. — binaryMDPWriter","text":"","code":"binaryMDPWriter(   prefix = \"\",   binNames = c(\"stateIdx.bin\", \"stateIdxLbl.bin\", \"actionIdx.bin\", \"actionIdxLbl.bin\",     \"actionWeight.bin\", \"actionWeightLbl.bin\", \"transProb.bin\", \"externalProcesses.bin\"),   getLog = TRUE )"},{"path":"http://relund.github.io/mdp/reference/binaryMDPWriter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for writing an HMDP model to binary files. The function defineds\nsubfunctions which can be used to define an HMDP model saved in a set of binary\nfiles. — binaryMDPWriter","text":"prefix character string prefix added binNames. binNames character vector giving names binary files storing model.","code":""},{"path":"http://relund.github.io/mdp/reference/binaryMDPWriter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for writing an HMDP model to binary files. The function defineds\nsubfunctions which can be used to define an HMDP model saved in a set of binary\nfiles. — binaryMDPWriter","text":"list functions.","code":""},{"path":"http://relund.github.io/mdp/reference/binaryMDPWriter.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Function for writing an HMDP model to binary files. The function defineds\nsubfunctions which can be used to define an HMDP model saved in a set of binary\nfiles. — binaryMDPWriter","text":"functions can used : setWeights(labels, ...): Set labels weights used actions. labels vector label names, ... used. function must called starting building model. process(): Starts (sub)process. May also used specify traditional MDP using matrices (MDPtoolbox style). style follows: P list matrices (one action) size SxS (S = number states). row must sum one used entries row must zero used. R matrix size SxA (= number actions) D matrix size SxA durations (optional specified assume durations 1). endProcess(): Ends (sub)process. stage(label=NULL): Starts stage. Currently label used binary format. endStage(): Ends (sub)process. state(label=NULL): Starts state. Returns (invisible) states id number can used later reference given scope 3. endState(): Ends stage. action(scope=NULL, id=NULL, pr=NULL, prob=NULL, weights, label=NULL, end=FALSE, ...):  Starts action. Parameter weights must vector action weights. two ways enter transition prob prob contains triples (scope,id,pr), Vectors id pr equal size. scope specified, scopes assumed 1. (see description actionIdx.bin ). end=TRUE endAction() nesseary. ... currently used. endAction(): Ends action. use set end=TRUE specify action. includeProcess(prefix, label=NULL, weights, prob, termStates): Include external process. External processes loaded memory needed. , external processes usefull considering large models problems memory. Parameter prefix prefix external process. next parameters specify child jump action process, .e. weights must vector action weights, prob must contain triples (scope,idx,pr) (see description actionIdx.bin ), Finally termStates must specify number states last stage external process. Note inside includeProcess ... endIncludeProcess must specify father jump actions last stage external process. external process represented using first last stage, together jump actions. Returns (invisible) state id's first stage external process can used later reference given scope 3. endIncludeProcess(): Ends includeProcess. closeWriter(): Close writer. Must called model description finished. Eight binary files created using following format: stateIdx.bin: File integers containing indexes defining states format \"n0 s0 -1 n0 s0 a0 n1 s1 -1 n0 s0 a0 n1 s1 a1 n2 s2 -1 n0 s0 ...\". -1 used indicate new state considered (new line). stateIdxLbl.bin: File characters format \"sIdx label sIdx label ...\" sIdx corresponds index/line number stateIdxLbl.bin (index starts 0). Note delimiter used. actionIdx.bin: File integers containing indexes defining actions format \"sIdx scope idx scope idx scope idx -1 sIdx scope idx scope idx -1 sIdx scope -1 ...\". sIdx corresponds index/line number stateIdx.bin (index starts 0). Next pairs (scope idx) follow indicating possible transitions. Scope can 4 values: 2 - transition child process (stage zero child process), 1 - transition next stage current process, 0 - transition next stage father process. idx pair denote index state stage considered, e.g. scope=1 idx=2 consider state number 3 next stage current process. Finally, scope = 3 transition state specified state sIdx given. , scope=3 idx=5 transition state specified line 6 stateIdxLbl.bin. usefull considering shared child processes. actionIdxLbl.bin: File characters format \"aIdx label aIdx label ...\" aIdx corresponds index/line number actionIdx.bin (index starts 0). Note delimiter used. actionWeight.bin: File doubles containing weights actions format \"c1 c2 c3 c1 c2 c3 ...\" assuming three weights action. actionWeightLbl.bin: File characters containing labels weights format \"lable1 label2 label3\" assuming three weights action. transProb.bin: File doubles containing probabilities transitions defined actions actionIdx.bin. format \"p1 p2 p3 -1 p1 -1 p1 p2 -1 ...\". -1 used indicate new action considered (new line). externalProcesses.bin: File characters containing links external processes. format \"n0 s0 prefix -1 n0 s0 a0 n1 s1 prefix -1 ...\". -1 used indicate new external process considered stage defined indexes. externalProcesses.bin: File characters format \"stageStr prefix stageStr prefix ...\" stageStr corresponds index (e.g. n0 s0 a0 n1) stage corresponding first stage external process prefix prefix external process. Note delimiter used.","code":""},{"path":"http://relund.github.io/mdp/reference/binaryMDPWriter.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Function for writing an HMDP model to binary files. The function defineds\nsubfunctions which can be used to define an HMDP model saved in a set of binary\nfiles. — binaryMDPWriter","text":"Note indexes starting zero (C/C++ style).","code":""},{"path":"http://relund.github.io/mdp/reference/binaryMDPWriter.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Function for writing an HMDP model to binary files. The function defineds\nsubfunctions which can be used to define an HMDP model saved in a set of binary\nfiles. — binaryMDPWriter","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/binaryMDPWriter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function for writing an HMDP model to binary files. The function defineds\nsubfunctions which can be used to define an HMDP model saved in a set of binary\nfiles. — binaryMDPWriter","text":"","code":"#  # # Create a small HMDP with two levels # w<-binaryMDPWriter() # w$setWeights(c(\"Duration\",\"Net reward\",\"Items\")) # w$process() #   w$stage() #     w$state(label=\"M0\") #       w$action(label=\"A0\",weights=c(0,0,0),prob=c(2,0,1)) #         w$process() #           w$stage() #             w$state(label=\"D\") #               w$action(label=\"A0\",weights=c(0,0,1),prob=c(1,0,0.5,1,1,0.5)) #               w$endAction() #             w$endState() #           w$endStage() #           w$stage() #             w$state(label=\"C0\") #               w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #               w$endAction() #               w$action(label=\"A1\",weights=c(1,2,1),prob=c(1,0,0.5,1,1,0.5)) #               w$endAction() #             w$endState() #             w$state(label=\"C1\") #               w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #               w$endAction() #               w$action(label=\"A1\",weights=c(1,2,1),prob=c(1,0,0.5,1,1,0.5)) #               w$endAction() #             w$endState() #           w$endStage() #           w$stage() #             w$state(label=\"C0\") #               w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1)) #               w$endAction() #             w$endState() #             w$state(label=\"C1\") #               w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1)) #               w$endAction() #             w$endState() #           w$endStage() #         w$endProcess() #       w$endAction() #       w$action(label=\"A1\",weights=c(0,0,0),prob=c(2,0,1)) #         w$process() #           w$stage() #             w$state(label=\"D\") #               w$action(label=\"A0\",weights=c(0,0,1),prob=c(1,0,1)) #               w$endAction() #             w$endState() #           w$endStage() #           w$stage() #             w$state(label=\"C0\") #               w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #               w$endAction() #               w$action(label=\"A1\",weights=c(1,2,1),prob=c(1,0,0.5,1,1,0.5)) #               w$endAction() #             w$endState() #           w$endStage() #           w$stage() #             w$state(label=\"C0\") #               w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1)) #               w$endAction() #             w$endState() #             w$state(label=\"C1\") #               w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1)) #               w$endAction() #               w$action(label=\"A1\",weights=c(0,10,5),prob=c(0,0,0.5,0,1,0.5)) #               w$endAction() #             w$endState() #           w$endStage() #         w$endProcess() #       w$endAction() #     w$endState() #     w$state(label=\"M1\") #       w$action(label=\"A0\",weights=c(0,0,0),prob=c(2,0,1)) #         w$process() #           w$stage() #             w$state(label=\"D\") #               w$action(label=\"A0\",weights=c(0,0,1),prob=c(1,0,0.5,1,1,0.5)) #               w$endAction() #             w$endState() #           w$endStage() #           w$stage() #             w$state(label=\"C0\") #               w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #               w$endAction() #             w$endState() #             w$state(label=\"C1\") #               w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #               w$endAction() #             w$endState() #           w$endStage() #           w$stage() #             w$state(label=\"C0\") #               w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1)) #               w$endAction() #             w$endState() #             w$state(label=\"C1\") #               w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1)) #               w$endAction() #             w$endState() #           w$endStage() #         w$endProcess() #       w$endAction() #     w$endState() #   w$endStage() # w$endProcess() # w$closeWriter() #  # #stateIdxDf() # #actionIdxDf() # #actionWeightMat() # #transProbMat() # #a<-actionInfo() # #a                   # ordered by action id # #a[order(a$sId),]    # ordered by state id"},{"path":"http://relund.github.io/mdp/reference/calcRPO.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the rentention payoff (RPO) or opportunity cost for some states. — calcRPO","title":"Calculate the rentention payoff (RPO) or opportunity cost for some states. — calcRPO","text":"RPO defined difference weight state using action iA maximum weight node using another predecessor different iA.","code":""},{"path":"http://relund.github.io/mdp/reference/calcRPO.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the rentention payoff (RPO) or opportunity cost for some states. — calcRPO","text":"","code":"calcRPO(   mdp,   w,   iA,   sId = ifelse(mdp$timeHorizon >= Inf, mdp$founderStatesLast + 1,     1):ifelse(mdp$timeHorizon >= Inf, mdp$states + mdp$founderStatesLast, mdp$states) - 1,   criterion = \"expected\",   dur = \"\",   rate = 0,   rateBase = 1,   discountFactor = NULL,   g = 0,   discountMethod = \"continuous\" )"},{"path":"http://relund.github.io/mdp/reference/calcRPO.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the rentention payoff (RPO) or opportunity cost for some states. — calcRPO","text":"mdp MDP loaded using loadMDP. w label weight/reward calculate RPO . iA action index calculate RPO respect (size sId). sId Vector id's states want retrive. criterion criterion used. expected used expected reward, discount used discounted rewards, average use average rewards. dur label duration/time discount rates can calculated. rate interest rate. rateBase time-horizon rate valid . discountFactor discountRate one time unit. specified rate rateBase used calculate discount rate. g optimal gain (g) calculated (used criterion = \"average\"). discountMethod Either 'continuous' 'discrete', corresponding discount factor exp(-rate/rateBase) 1/(1+rate/rateBase), respectively. used discountFactor NULL.","code":""},{"path":"http://relund.github.io/mdp/reference/calcRPO.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the rentention payoff (RPO) or opportunity cost for some states. — calcRPO","text":"rpo (matrix/data frame).","code":""},{"path":"http://relund.github.io/mdp/reference/calcRPO.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate the rentention payoff (RPO) or opportunity cost for some states. — calcRPO","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/calcSteadyStatePr.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the steady state transition probabilities for the founder process (level 0). — calcSteadyStatePr","title":"Calculate the steady state transition probabilities for the founder process (level 0). — calcSteadyStatePr","text":"Assume consider ergodic/irreducible time-homogeneous Markov chain specified using policy MDP.","code":""},{"path":"http://relund.github.io/mdp/reference/calcSteadyStatePr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the steady state transition probabilities for the founder process (level 0). — calcSteadyStatePr","text":"","code":"calcSteadyStatePr(mdp, getLog = FALSE)"},{"path":"http://relund.github.io/mdp/reference/calcSteadyStatePr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the steady state transition probabilities for the founder process (level 0). — calcSteadyStatePr","text":"mdp MDP loaded using loadMDP.","code":""},{"path":"http://relund.github.io/mdp/reference/calcSteadyStatePr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the steady state transition probabilities for the founder process (level 0). — calcSteadyStatePr","text":"vector stady state probabilities states founder level.","code":""},{"path":"http://relund.github.io/mdp/reference/calcSteadyStatePr.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate the steady state transition probabilities for the founder process (level 0). — calcSteadyStatePr","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/calcWeights.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate weights based on current policy. Normally run after an optimal policy has been found. — calcWeights","title":"Calculate weights based on current policy. Normally run after an optimal policy has been found. — calcWeights","text":"Calculate weights based current policy. Normally run optimal policy found.","code":""},{"path":"http://relund.github.io/mdp/reference/calcWeights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate weights based on current policy. Normally run after an optimal policy has been found. — calcWeights","text":"","code":"calcWeights(   mdp,   wLbl,   criterion = \"expected\",   durLbl = NULL,   rate = 0,   rateBase = 1,   discountFactor = NULL,   termValues = NULL,   discountMethod = \"continuous\" )"},{"path":"http://relund.github.io/mdp/reference/calcWeights.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate weights based on current policy. Normally run after an optimal policy has been found. — calcWeights","text":"mdp MDP loaded using loadMDP. wLbl label weight consider. criterion criterion used. expected used expected reward, discount used discounted rewards, average use average rewards. durLbl label duration/time discount rates can calculated. rate interest rate. rateBase time-horizon rate valid . discountFactor discountRate one time unit. specified rate rateBase used calculate discount rate. termValues terminal values used (values last stage MDP). discountMethod Either 'continuous' 'discrete', corresponding discount factor exp(-rate/rateBase) 1/(1+rate/rateBase), respectively. used discountFactor NULL.","code":""},{"path":"http://relund.github.io/mdp/reference/calcWeights.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate weights based on current policy. Normally run after an optimal policy has been found. — calcWeights","text":"Nothing.","code":""},{"path":"http://relund.github.io/mdp/reference/calcWeights.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate weights based on current policy. Normally run after an optimal policy has been found. — calcWeights","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/calcWeights.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate weights based on current policy. Normally run after an optimal policy has been found. — calcWeights","text":"","code":"# # Create the small machine repleacement problem used as an example in L.R. # # Nielsen and A.R. Kristensen. Finding the K best policies in a finite-horizon # # Markov decision process. European Journal of Operational Research, # # 175(2):1164-1179, 2006. doi:10.1016/j.ejor.2005.06.011. #  # ## Create the MDP using a dummy replacement node # prefix<-\"machine1_\" # w <- binaryMDPWriter(prefix) # w$setWeights(c(\"Net reward\")) # w$process() #   w$stage()   # stage n=0 #     w$state(label=\"Dummy\")          # v=(0,0) #       w$action(label=\"buy\", weights=-100, prob=c(1,0,0.7, 1,1,0.3), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=1 #     w$state(label=\"good\")           # v=(1,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.6, 1,1,0.4), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(1,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.6, 1,2,0.4), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=2 #     w$state(label=\"good\")           # v=(2,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.5, 1,1,0.5), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(2,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.5, 1,2,0.5), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(2,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(1,3,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=3 #     w$state(label=\"good\")           # v=(3,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.2, 1,1,0.8), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(3,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.2, 1,2,0.8), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(3,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(1,3,1), end=TRUE) #     w$endState() #     w$state(label=\"replaced\")       # v=(3,3) #       w$action(label=\"Dummy\", weights=0, prob=c(1,3,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=4 #     w$state(label=\"good\", end=TRUE)        # v=(4,0) #     w$state(label=\"average\", end=TRUE)     # v=(4,1) #     w$state(label=\"not working\", end=TRUE) # v=(4,2) #     w$state(label=\"replaced\", end=TRUE)    # v=(4,3) #   w$endStage() # w$endProcess() # w$closeWriter() #  # ## Some info about the model # #stateIdxDf(prefix)      # states of the MDP with labels returned as a data frame # #actionInfo(prefix)      # all action information of the MDP returned in a single data frame #  # ## Load the model into memory #  #  # mdp<-loadMDP(prefix) # mdp #  # ## Perform value iteration # # w<-\"Net reward\"             # label of the weight we want to optimize # # scrapValues<-c(30,10,5,0)   # scrap values (the values of the 4 states at stage 4) # # valueIte(mdp, w, termValues=scrapValues) # #  # # ## Print the optimal policy # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # ## Calculate the weights of the policy always to maintain # # #policy<-data.frame(sId=states$sId,iA=0) # # setPolicy(mdp, policy) # # calcWeights(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # ## Modify the MDP in memory: remove the maintain action in the states of stage 1 # # removeAction(mdp, sId=1, iA=0)  # remove action 0 at the state with sId=1 # # removeAction(mdp, sId=2, iA=0) # #  # # ## Perform value iteration on the modified MDP # # valueIte(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # resetActions(mdp)   # reset the MDP such that all actions are used # #  # # ## Modify the weight of action 'buy' # # setActionWeight(mdp, w=-50, sId=0, iA=0, wLbl=w) # #  # # ## Perform value iteration on the modified MDP # # valueIte(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # #  # #  #  #  #  # # The example given in L.R. Nielsen and A.R. Kristensen. Finding the K best # # policies in a finite-horizon Markov decision process. European Journal of # # Operational Research, 175(2):1164-1179, 2006. doi:10.1016/j.ejor.2005.06.011, # # does actually not have any dummy replacement node as in the MDP above. The same # # model can be created using a single dummy node at the end of the process. #  # ## Create the MDP using a single dummy node # prefix<-\"machine2_\" # w <- binaryMDPWriter(prefix) # w$setWeights(c(\"Net reward\")) # w$process() #   w$stage()   # stage n=0 #     w$state(label=\"Dummy\")          # v=(0,0) #       w$action(label=\"buy\", weights=-100, prob=c(1,0,0.7, 1,1,0.3), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=1 #     w$state(label=\"good\")           # v=(1,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.6, 1,1,0.4), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(1,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.6, 1,2,0.4), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=2 #     w$state(label=\"good\")           # v=(2,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.5, 1,1,0.5), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(2,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.5, 1,2,0.5), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(2,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(3,12,1), end=TRUE)     # transition to the node with sId=12 (Dummy) #     w$endState() #   w$endStage() #   w$stage()   # stage n=3 #     w$state(label=\"good\")           # v=(3,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.2, 1,1,0.8), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(3,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.2, 1,2,0.8), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(3,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(3,12,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=4 #     w$state(label=\"good\")        # v=(4,0) #       w$action(label=\"rep\", weights=30, prob=c(1,0,1), end=TRUE) #     w$endState() #     w$state(label=\"average\")     # v=(4,1) #       w$action(label=\"rep\", weights=10, prob=c(1,0,1), end=TRUE) #     w$endState() #     w$state(label=\"not working\") # v=(4,2) #       w$action(label=\"rep\", weights=5, prob=c(1,0,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=5 #     w$state(label=\"Dummy\", end=TRUE)        # v=(5,0) #   w$endStage() # w$endProcess() # w$closeWriter() #  # ## Some info about the model # #stateIdxDf(prefix)      # states of the MDP with labels returned as a data frame # #actionInfo(prefix)      # all action information of the MDP returned in a single data frame #  # ## Have a look at the state-expanded hypergraph # mdp<-loadMDP(prefix) # # hypergf(mdp) # #  # # ## Find optimal policy # # w<-\"Net reward\" # # valueIte(mdp, w, termValues=0) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy"},{"path":"http://relund.github.io/mdp/reference/checkWDurIdx.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function. Check if the indexes given are okay. Should not be used except you know what you are doing — checkWDurIdx","title":"Internal function. Check if the indexes given are okay. Should not be used except you know what you are doing — checkWDurIdx","text":"Internal function. Check indexes given okay. used except know ","code":""},{"path":"http://relund.github.io/mdp/reference/checkWDurIdx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function. Check if the indexes given are okay. Should not be used except you know what you are doing — checkWDurIdx","text":"","code":".checkWDurIdx(iW, iDur, wLth)"},{"path":"http://relund.github.io/mdp/reference/checkWDurIdx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function. Check if the indexes given are okay. Should not be used except you know what you are doing — checkWDurIdx","text":"iW Index weight want optimize. iDur Index duration/time. wLth Number weights model.","code":""},{"path":"http://relund.github.io/mdp/reference/checkWDurIdx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function. Check if the indexes given are okay. Should not be used except you know what you are doing — checkWDurIdx","text":"Nothing.","code":""},{"path":"http://relund.github.io/mdp/reference/checkWDurIdx.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Internal function. Check if the indexes given are okay. Should not be used except you know what you are doing — checkWDurIdx","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/checkWIdx.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function. Check if the index of the weight is okay. Should not be used except you know what you are doing — checkWIdx","title":"Internal function. Check if the index of the weight is okay. Should not be used except you know what you are doing — checkWIdx","text":"Internal function. Check index weight okay. used except know ","code":""},{"path":"http://relund.github.io/mdp/reference/checkWIdx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function. Check if the index of the weight is okay. Should not be used except you know what you are doing — checkWIdx","text":"","code":".checkWIdx(iW, wLth)"},{"path":"http://relund.github.io/mdp/reference/checkWIdx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function. Check if the index of the weight is okay. Should not be used except you know what you are doing — checkWIdx","text":"iW Index weight want optimize. wLth Number weights model.","code":""},{"path":"http://relund.github.io/mdp/reference/checkWIdx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal function. Check if the index of the weight is okay. Should not be used except you know what you are doing — checkWIdx","text":"Nothing.","code":""},{"path":"http://relund.github.io/mdp/reference/checkWIdx.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Internal function. Check if the index of the weight is okay. Should not be used except you know what you are doing — checkWIdx","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/convertBinary2HMP.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a HMDP model stored in binary format to a hmp (xml) file.\nThe function simply parse the binary files and create hmp files using\nthe hmpMDPWriter. — convertBinary2HMP","title":"Convert a HMDP model stored in binary format to a hmp (xml) file.\nThe function simply parse the binary files and create hmp files using\nthe hmpMDPWriter. — convertBinary2HMP","text":"Convert HMDP model stored binary format hmp (xml) file. function simply parse binary files create hmp files using hmpMDPWriter.","code":""},{"path":"http://relund.github.io/mdp/reference/convertBinary2HMP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a HMDP model stored in binary format to a hmp (xml) file.\nThe function simply parse the binary files and create hmp files using\nthe hmpMDPWriter. — convertBinary2HMP","text":"","code":"convertBinary2HMP(prefix=\"\",   binNames=c(\"stateIdx.bin\",\"stateIdxLbl.bin\",\"actionIdx.bin\",\"actionIdxLbl.bin\",\"actionWeight.bin\",\"actionWeightLbl.bin\",\"transProb.bin\"),   out=paste(prefix,'converted.hmp',sep=\"\"), duration=1)"},{"path":"http://relund.github.io/mdp/reference/convertBinary2HMP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a HMDP model stored in binary format to a hmp (xml) file.\nThe function simply parse the binary files and create hmp files using\nthe hmpMDPWriter. — convertBinary2HMP","text":"prefix character string prefix added binary files. binNames character vector length 7 giving names binary files storing model. name hmp file (e.g. mdp.hmp). duration Weight number storing duration (NULL none).","code":""},{"path":"http://relund.github.io/mdp/reference/convertBinary2HMP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a HMDP model stored in binary format to a hmp (xml) file.\nThe function simply parse the binary files and create hmp files using\nthe hmpMDPWriter. — convertBinary2HMP","text":"NULL (invisible).","code":""},{"path":"http://relund.github.io/mdp/reference/convertBinary2HMP.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Convert a HMDP model stored in binary format to a hmp (xml) file.\nThe function simply parse the binary files and create hmp files using\nthe hmpMDPWriter. — convertBinary2HMP","text":"Note indexes starting zero (C/C++ style).","code":""},{"path":[]},{"path":"http://relund.github.io/mdp/reference/convertBinary2HMP.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Convert a HMDP model stored in binary format to a hmp (xml) file.\nThe function simply parse the binary files and create hmp files using\nthe hmpMDPWriter. — convertBinary2HMP","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/convertBinary2HMP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a HMDP model stored in binary format to a hmp (xml) file.\nThe function simply parse the binary files and create hmp files using\nthe hmpMDPWriter. — convertBinary2HMP","text":"","code":"# wDir <- getwd() # setwd(system.file(\"models\", package = \"MDP2\")) #  # ## convert the machine example to a hmp file # prefix1<-\"machine1_\" # convertBinary2HMP(prefix1, duration=NULL) # # have a look at the hmp file # xmlTreeParse(\"machine1_converted.hmp\",useInternalNodes=TRUE) #  # ## convert the machine example hmp file to binary files # prefix2<-\"machine_cov_\" # convertHMP2Binary(\"machine1.hmp\",prefix2) # stateIdxDf(prefix1) # stateIdxDf(prefix2)  ## convert the machine example with a single dummy node to a hmp file #convertBinary2HMP(\"machine2_\")  # error since using scope = 3 not supported in hmp files"},{"path":"http://relund.github.io/mdp/reference/convertHMP2Binary.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a HMDP model stored in a hmp (xml) file to binary file format. — convertHMP2Binary","title":"Convert a HMDP model stored in a hmp (xml) file to binary file format. — convertHMP2Binary","text":"function simply parse hmp file create binary files using binaryMDPWriter.","code":""},{"path":"http://relund.github.io/mdp/reference/convertHMP2Binary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a HMDP model stored in a hmp (xml) file to binary file format. — convertHMP2Binary","text":"","code":"convertHMP2Binary(file, prefix = \"\")"},{"path":"http://relund.github.io/mdp/reference/convertHMP2Binary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a HMDP model stored in a hmp (xml) file to binary file format. — convertHMP2Binary","text":"file name hmp file (e.g. mdp.hmp). prefix character string prefix added binary files.","code":""},{"path":"http://relund.github.io/mdp/reference/convertHMP2Binary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a HMDP model stored in a hmp (xml) file to binary file format. — convertHMP2Binary","text":"NULL (invisible).","code":""},{"path":"http://relund.github.io/mdp/reference/convertHMP2Binary.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Convert a HMDP model stored in a hmp (xml) file to binary file format. — convertHMP2Binary","text":"Note indexes starting zero (C/C++ style).","code":""},{"path":[]},{"path":"http://relund.github.io/mdp/reference/convertHMP2Binary.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Convert a HMDP model stored in a hmp (xml) file to binary file format. — convertHMP2Binary","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/convertHMP2Binary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a HMDP model stored in a hmp (xml) file to binary file format. — convertHMP2Binary","text":"","code":"# wDir <- getwd() # setwd(system.file(\"models\", package = \"MDP2\")) #  # ## convert the machine example to a hmp file # prefix1<-\"machine1_\" # convertBinary2HMP(prefix1, duration=NULL) # # have a look at the hmp file # xmlTreeParse(\"machine1_converted.hmp\",useInternalNodes=TRUE) #  # ## convert the machine example hmp file to binary files # prefix2<-\"machine_cov_\" # convertHMP2Binary(\"machine1.hmp\",prefix2) # stateIdxDf(prefix1) # stateIdxDf(prefix2)  ## convert the machine example with a single dummy node to a hmp file #convertBinary2HMP(\"machine2_\")  # error since using scope = 3 not supported in hmp files"},{"path":"http://relund.github.io/mdp/reference/getPolicy.html","id":null,"dir":"Reference","previous_headings":"","what":"Get parts of the optimal policy. — getPolicy","title":"Get parts of the optimal policy. — getPolicy","text":"Get parts optimal policy.","code":""},{"path":"http://relund.github.io/mdp/reference/getPolicy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get parts of the optimal policy. — getPolicy","text":"","code":"getPolicy(   mdp,   sId = ifelse(mdp$timeHorizon >= Inf, mdp$founderStatesLast + 1,     1):ifelse(mdp$timeHorizon >= Inf, mdp$states + mdp$founderStatesLast, mdp$states) - 1,   stageStr = NULL,   stateLabels = TRUE,   actionLabels = TRUE,   actionIdx = TRUE,   rewards = TRUE,   stateStr = FALSE,   external = NULL,   ... )"},{"path":"http://relund.github.io/mdp/reference/getPolicy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get parts of the optimal policy. — getPolicy","text":"mdp MDP loaded using loadMDP. sId Vector id's states want retrieve. stageStr Stage string. specified find sId based stage string. stateLabels Add state labels. actionLabels Add action labels policy. actionIdx Add action index. rewards Add rewards calculated state. stateStr Add state string state. external vector stage strings corresponding external processes want optimal policy . ... Parameters passed find optimal policy external processes. Note external specified must contain stage strings mdp$external. Moreover must specify arguments passed valueIte used recreating optimal policy e.g. g value label reward duration. See vignette external processes.","code":""},{"path":"http://relund.github.io/mdp/reference/getPolicy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get parts of the optimal policy. — getPolicy","text":"policy (data frame).","code":""},{"path":"http://relund.github.io/mdp/reference/getPolicy.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get parts of the optimal policy. — getPolicy","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/getPolicy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get parts of the optimal policy. — getPolicy","text":"","code":"# # Create the small machine repleacement problem used as an example in L.R. # # Nielsen and A.R. Kristensen. Finding the K best policies in a finite-horizon # # Markov decision process. European Journal of Operational Research, # # 175(2):1164-1179, 2006. doi:10.1016/j.ejor.2005.06.011. #  # ## Create the MDP using a dummy replacement node # prefix<-\"machine1_\" # w <- binaryMDPWriter(prefix) # w$setWeights(c(\"Net reward\")) # w$process() #   w$stage()   # stage n=0 #     w$state(label=\"Dummy\")          # v=(0,0) #       w$action(label=\"buy\", weights=-100, prob=c(1,0,0.7, 1,1,0.3), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=1 #     w$state(label=\"good\")           # v=(1,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.6, 1,1,0.4), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(1,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.6, 1,2,0.4), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=2 #     w$state(label=\"good\")           # v=(2,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.5, 1,1,0.5), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(2,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.5, 1,2,0.5), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(2,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(1,3,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=3 #     w$state(label=\"good\")           # v=(3,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.2, 1,1,0.8), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(3,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.2, 1,2,0.8), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(3,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(1,3,1), end=TRUE) #     w$endState() #     w$state(label=\"replaced\")       # v=(3,3) #       w$action(label=\"Dummy\", weights=0, prob=c(1,3,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=4 #     w$state(label=\"good\", end=TRUE)        # v=(4,0) #     w$state(label=\"average\", end=TRUE)     # v=(4,1) #     w$state(label=\"not working\", end=TRUE) # v=(4,2) #     w$state(label=\"replaced\", end=TRUE)    # v=(4,3) #   w$endStage() # w$endProcess() # w$closeWriter() #  # ## Some info about the model # #stateIdxDf(prefix)      # states of the MDP with labels returned as a data frame # #actionInfo(prefix)      # all action information of the MDP returned in a single data frame #  # ## Load the model into memory #  #  # mdp<-loadMDP(prefix) # mdp #  # ## Perform value iteration # # w<-\"Net reward\"             # label of the weight we want to optimize # # scrapValues<-c(30,10,5,0)   # scrap values (the values of the 4 states at stage 4) # # valueIte(mdp, w, termValues=scrapValues) # #  # # ## Print the optimal policy # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # ## Calculate the weights of the policy always to maintain # # #policy<-data.frame(sId=states$sId,iA=0) # # setPolicy(mdp, policy) # # calcWeights(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # ## Modify the MDP in memory: remove the maintain action in the states of stage 1 # # removeAction(mdp, sId=1, iA=0)  # remove action 0 at the state with sId=1 # # removeAction(mdp, sId=2, iA=0) # #  # # ## Perform value iteration on the modified MDP # # valueIte(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # resetActions(mdp)   # reset the MDP such that all actions are used # #  # # ## Modify the weight of action 'buy' # # setActionWeight(mdp, w=-50, sId=0, iA=0, wLbl=w) # #  # # ## Perform value iteration on the modified MDP # # valueIte(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # #  # #  #  #  #  # # The example given in L.R. Nielsen and A.R. Kristensen. Finding the K best # # policies in a finite-horizon Markov decision process. European Journal of # # Operational Research, 175(2):1164-1179, 2006. doi:10.1016/j.ejor.2005.06.011, # # does actually not have any dummy replacement node as in the MDP above. The same # # model can be created using a single dummy node at the end of the process. #  # ## Create the MDP using a single dummy node # prefix<-\"machine2_\" # w <- binaryMDPWriter(prefix) # w$setWeights(c(\"Net reward\")) # w$process() #   w$stage()   # stage n=0 #     w$state(label=\"Dummy\")          # v=(0,0) #       w$action(label=\"buy\", weights=-100, prob=c(1,0,0.7, 1,1,0.3), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=1 #     w$state(label=\"good\")           # v=(1,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.6, 1,1,0.4), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(1,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.6, 1,2,0.4), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=2 #     w$state(label=\"good\")           # v=(2,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.5, 1,1,0.5), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(2,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.5, 1,2,0.5), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(2,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(3,12,1), end=TRUE)     # transition to the node with sId=12 (Dummy) #     w$endState() #   w$endStage() #   w$stage()   # stage n=3 #     w$state(label=\"good\")           # v=(3,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.2, 1,1,0.8), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(3,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.2, 1,2,0.8), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(3,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(3,12,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=4 #     w$state(label=\"good\")        # v=(4,0) #       w$action(label=\"rep\", weights=30, prob=c(1,0,1), end=TRUE) #     w$endState() #     w$state(label=\"average\")     # v=(4,1) #       w$action(label=\"rep\", weights=10, prob=c(1,0,1), end=TRUE) #     w$endState() #     w$state(label=\"not working\") # v=(4,2) #       w$action(label=\"rep\", weights=5, prob=c(1,0,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=5 #     w$state(label=\"Dummy\", end=TRUE)        # v=(5,0) #   w$endStage() # w$endProcess() # w$closeWriter() #  # ## Some info about the model # #stateIdxDf(prefix)      # states of the MDP with labels returned as a data frame # #actionInfo(prefix)      # all action information of the MDP returned in a single data frame #  # ## Have a look at the state-expanded hypergraph # mdp<-loadMDP(prefix) # # hypergf(mdp) # #  # # ## Find optimal policy # # w<-\"Net reward\" # # valueIte(mdp, w, termValues=0) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy"},{"path":"http://relund.github.io/mdp/reference/getWIdx.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the index of a weight in the model. Note that index always start from zero (C++ style), i.e. the first weight, the first state at a stage etc has index 0. — getWIdx","title":"Return the index of a weight in the model. Note that index always start from zero (C++ style), i.e. the first weight, the first state at a stage etc has index 0. — getWIdx","text":"Return index weight model. Note index always start zero (C++ style), .e. first weight, first state stage etc index 0.","code":""},{"path":"http://relund.github.io/mdp/reference/getWIdx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the index of a weight in the model. Note that index always start from zero (C++ style), i.e. the first weight, the first state at a stage etc has index 0. — getWIdx","text":"","code":"getWIdx(mdp, wLbl)"},{"path":"http://relund.github.io/mdp/reference/getWIdx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the index of a weight in the model. Note that index always start from zero (C++ style), i.e. the first weight, the first state at a stage etc has index 0. — getWIdx","text":"mdp MDP loaded using loadMDP. wLbl label/string weight.","code":""},{"path":"http://relund.github.io/mdp/reference/getWIdx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the index of a weight in the model. Note that index always start from zero (C++ style), i.e. the first weight, the first state at a stage etc has index 0. — getWIdx","text":"index (integer).","code":""},{"path":"http://relund.github.io/mdp/reference/getWIdx.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Return the index of a weight in the model. Note that index always start from zero (C++ style), i.e. the first weight, the first state at a stage etc has index 0. — getWIdx","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/hmpMDPWriter.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for writing an HMDP model to a hmp file (XML). The function define\nsubfunctions which can be used to define an HMDP model stored in a hmp file. — hmpMDPWriter","title":"Function for writing an HMDP model to a hmp file (XML). The function define\nsubfunctions which can be used to define an HMDP model stored in a hmp file. — hmpMDPWriter","text":"HMP files XML format human readable using e.g. text editor. HMP files suitable storing large HMDP models since text files verbose. Moreover, approximation weights probabilities may occur since parser writing hmp file may output digits. consider large models use binary file format instead.","code":""},{"path":"http://relund.github.io/mdp/reference/hmpMDPWriter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for writing an HMDP model to a hmp file (XML). The function define\nsubfunctions which can be used to define an HMDP model stored in a hmp file. — hmpMDPWriter","text":"","code":"hmpMDPWriter(   file = \"r.hmp\",   rate = 0.1,   rateBase = 1,   precision = 1e-05,   desc = \"HMP file created using hmpMDPWriter in R\" )"},{"path":"http://relund.github.io/mdp/reference/hmpMDPWriter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for writing an HMDP model to a hmp file (XML). The function define\nsubfunctions which can be used to define an HMDP model stored in a hmp file. — hmpMDPWriter","text":"file name file storing model (e.g. mdp.hmp). rate interest rate (used consider discounting). rateBase time rate taken , e.g. rate 0.1 rateBase 365 days interest rate 10 percent year. precision precision used checking probabilities sum one. desc Description model.","code":""},{"path":"http://relund.github.io/mdp/reference/hmpMDPWriter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for writing an HMDP model to a hmp file (XML). The function define\nsubfunctions which can be used to define an HMDP model stored in a hmp file. — hmpMDPWriter","text":"list functions.","code":""},{"path":"http://relund.github.io/mdp/reference/hmpMDPWriter.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Function for writing an HMDP model to a hmp file (XML). The function define\nsubfunctions which can be used to define an HMDP model stored in a hmp file. — hmpMDPWriter","text":"functions can used : setWeights(labels, duration): Set labels weights used actions. labels vector label names, duration number defining label corresponds duration/time, e.g. first entry labels time duration = 1. function must called starting building model. process(): Starts (sub)process. endProcess(): Ends (sub)process. stage(label=NULL): Starts stage. endStage(): Ends (sub)process. state(label=NULL): Starts state. Returns states index number sIdx. endState(): Ends stage. action(label=NULL, weights, prob, statesNext=NULL): Starts action. Parameter weights must vector action weights, prob must contain triples (scope, idx,pr). scope can 3 values: 0 - transition next stage father process, 1 - transition next stage current process, 2 - transition child process (stage zero child process). idx pair denote index state stage considered, e.g. scope=1 idx=2 consider state number 3 next stage current process (number zero). Note scope = 3 supported hmp file format! statesNext number states next stage process (needed transition father). endAction(): Ends action. closeWriter(): Close writer. Must called model description finished.","code":""},{"path":"http://relund.github.io/mdp/reference/hmpMDPWriter.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Function for writing an HMDP model to a hmp file (XML). The function define\nsubfunctions which can be used to define an HMDP model stored in a hmp file. — hmpMDPWriter","text":"Note indexes starting zero (C/C++ style).","code":""},{"path":"http://relund.github.io/mdp/reference/hmpMDPWriter.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Function for writing an HMDP model to a hmp file (XML). The function define\nsubfunctions which can be used to define an HMDP model stored in a hmp file. — hmpMDPWriter","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/hmpMDPWriter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function for writing an HMDP model to a hmp file (XML). The function define\nsubfunctions which can be used to define an HMDP model stored in a hmp file. — hmpMDPWriter","text":"","code":"# # Create a small HMDP with two levels # w<-hmpMDPWriter() # w$setWeights(c(\"Duration\",\"Net reward\",\"Items\"),duration=1) # w$process() #   w$stage() #   w$state(label=\"M0\") #     w$action(label=\"A0\",weights=c(0,0,0),prob=c(2,0,1)) #     w$process() #       w$stage() #       w$state(label=\"D\") #         w$action(label=\"A0\",weights=c(0,0,1),prob=c(1,0,0.5,1,1,0.5)) #         w$endAction() #       w$endState() #       w$endStage() #       w$stage() #       w$state(label=\"C0\") #         w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #         w$endAction() #         w$action(label=\"A1\",weights=c(1,2,1),prob=c(1,0,0.5,1,1,0.5)) #         w$endAction() #       w$endState() #       w$state(label=\"C1\") #         w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #         w$endAction() #         w$action(label=\"A1\",weights=c(1,2,1),prob=c(1,0,0.5,1,1,0.5)) #         w$endAction() #       w$endState() #       w$endStage() #       w$stage() #       w$state(label=\"C0\") #         w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1), statesNext=0) #         w$endAction() #       w$endState() #       w$state(label=\"C1\") #         w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1), statesNext=0) #         w$endAction() #       w$endState() #       w$endStage() #     w$endProcess() #     w$endAction() #     w$action(label=\"A1\",weights=c(0,0,0),prob=c(2,0,1)) #     w$process() #       w$stage() #       w$state(label=\"D\") #         w$action(label=\"A0\",weights=c(0,0,1),prob=c(1,0,1)) #         w$endAction() #       w$endState() #       w$endStage() #       w$stage() #       w$state(label=\"C0\") #         w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #         w$endAction() #         w$action(label=\"A1\",weights=c(1,2,1),prob=c(1,0,0.5,1,1,0.5)) #         w$endAction() #       w$endState() #       w$endStage() #       w$stage() #       w$state(label=\"C0\") #         w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1), statesNext=0) #         w$endAction() #       w$endState() #       w$state(label=\"C1\") #         w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1), statesNext=0) #         w$endAction() #         w$action(label=\"A1\",weights=c(0,10,5),prob=c(0,0,0.5,0,1,0.5), statesNext=0) #         w$endAction() #       w$endState() #       w$endStage() #     w$endProcess() #     w$endAction() #   w$endState() #   w$state(label=\"M1\") #     w$action(label=\"A0\",weights=c(0,0,0),prob=c(2,0,1)) #     w$process() #       w$stage() #       w$state(label=\"D\") #         w$action(label=\"A0\",weights=c(0,0,1),prob=c(1,0,0.5,1,1,0.5)) #         w$endAction() #       w$endState() #       w$endStage() #       w$stage() #       w$state(label=\"C0\") #         w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #         w$endAction() #       w$endState() #       w$state(label=\"C1\") #         w$action(label=\"A0\",weights=c(0,0,0),prob=c(1,0,1)) #         w$endAction() #       w$endState() #       w$endStage() #       w$stage() #       w$state(label=\"C0\") #         w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1), statesNext=0) #         w$endAction() #       w$endState() #       w$state(label=\"C1\") #         w$action(label=\"A0\",weights=c(1,4,0),prob=c(0,0,1), statesNext=0) #         w$endAction() #       w$endState() #       w$endStage() #     w$endProcess() #     w$endAction() #   w$endState() #   w$endStage() # w$endProcess() # w$closeWriter() #  # # have a look at the hmp file # xmlTreeParse(\"r.hmp\",useInternalNodes=TRUE)"},{"path":"http://relund.github.io/mdp/reference/infoMDP.html","id":null,"dir":"Reference","previous_headings":"","what":"Information about the MDP — infoMDP","title":"Information about the MDP — infoMDP","text":"Information MDP","code":""},{"path":"http://relund.github.io/mdp/reference/infoMDP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Information about the MDP — infoMDP","text":"","code":"infoMDP(   mdp,   sId = 1:ifelse(mdp$timeHorizon < Inf, mdp$states, mdp$states + mdp$founderStatesLast) -     1,   stateStr = NULL,   stageStr = NULL,   withDF = TRUE,   withHarc = FALSE,   asStrings = TRUE )"},{"path":"http://relund.github.io/mdp/reference/infoMDP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Information about the MDP — infoMDP","text":"mdp MDP loaded using loadMDP. sId id state(s) considered. stateStr character vector containing index state(s) (e.g. \"n0,s0,a0,n1,s1\"). Parameter sId ignored NULL. stageStr character vector containing index stage(s) (e.g. \"n0,s0,a0,n1\"). Parameter sId idxS ignored NULL. withDF Include two data frames information actions states. withHarc Include hyperarcs data frame. row contains hyperarc first column denoting head (sId) rest tails (sId). asStrings Write state vector, transitions probabilities strings.","code":""},{"path":"http://relund.github.io/mdp/reference/infoMDP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Information about the MDP — infoMDP","text":"list states containing actions.","code":""},{"path":"http://relund.github.io/mdp/reference/infoMDP.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Information about the MDP — infoMDP","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/infoMDP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Information about the MDP — infoMDP","text":"","code":"# # Create the small machine repleacement problem used as an example in L.R. # # Nielsen and A.R. Kristensen. Finding the K best policies in a finite-horizon # # Markov decision process. European Journal of Operational Research, # # 175(2):1164-1179, 2006. doi:10.1016/j.ejor.2005.06.011. #  # ## Create the MDP using a dummy replacement node # prefix<-\"machine1_\" # w <- binaryMDPWriter(prefix) # w$setWeights(c(\"Net reward\")) # w$process() #   w$stage()   # stage n=0 #     w$state(label=\"Dummy\")          # v=(0,0) #       w$action(label=\"buy\", weights=-100, prob=c(1,0,0.7, 1,1,0.3), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=1 #     w$state(label=\"good\")           # v=(1,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.6, 1,1,0.4), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(1,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.6, 1,2,0.4), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=2 #     w$state(label=\"good\")           # v=(2,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.5, 1,1,0.5), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(2,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.5, 1,2,0.5), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(2,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(1,3,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=3 #     w$state(label=\"good\")           # v=(3,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.2, 1,1,0.8), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(3,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.2, 1,2,0.8), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(3,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(1,3,1), end=TRUE) #     w$endState() #     w$state(label=\"replaced\")       # v=(3,3) #       w$action(label=\"Dummy\", weights=0, prob=c(1,3,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=4 #     w$state(label=\"good\", end=TRUE)        # v=(4,0) #     w$state(label=\"average\", end=TRUE)     # v=(4,1) #     w$state(label=\"not working\", end=TRUE) # v=(4,2) #     w$state(label=\"replaced\", end=TRUE)    # v=(4,3) #   w$endStage() # w$endProcess() # w$closeWriter() #  # ## Some info about the model # #stateIdxDf(prefix)      # states of the MDP with labels returned as a data frame # #actionInfo(prefix)      # all action information of the MDP returned in a single data frame #  # ## Load the model into memory #  #  # mdp<-loadMDP(prefix) # mdp #  # ## Perform value iteration # # w<-\"Net reward\"             # label of the weight we want to optimize # # scrapValues<-c(30,10,5,0)   # scrap values (the values of the 4 states at stage 4) # # valueIte(mdp, w, termValues=scrapValues) # #  # # ## Print the optimal policy # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # ## Calculate the weights of the policy always to maintain # # #policy<-data.frame(sId=states$sId,iA=0) # # setPolicy(mdp, policy) # # calcWeights(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # ## Modify the MDP in memory: remove the maintain action in the states of stage 1 # # removeAction(mdp, sId=1, iA=0)  # remove action 0 at the state with sId=1 # # removeAction(mdp, sId=2, iA=0) # #  # # ## Perform value iteration on the modified MDP # # valueIte(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # resetActions(mdp)   # reset the MDP such that all actions are used # #  # # ## Modify the weight of action 'buy' # # setActionWeight(mdp, w=-50, sId=0, iA=0, wLbl=w) # #  # # ## Perform value iteration on the modified MDP # # valueIte(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # #  # #  #  #  #  # # The example given in L.R. Nielsen and A.R. Kristensen. Finding the K best # # policies in a finite-horizon Markov decision process. European Journal of # # Operational Research, 175(2):1164-1179, 2006. doi:10.1016/j.ejor.2005.06.011, # # does actually not have any dummy replacement node as in the MDP above. The same # # model can be created using a single dummy node at the end of the process. #  # ## Create the MDP using a single dummy node # prefix<-\"machine2_\" # w <- binaryMDPWriter(prefix) # w$setWeights(c(\"Net reward\")) # w$process() #   w$stage()   # stage n=0 #     w$state(label=\"Dummy\")          # v=(0,0) #       w$action(label=\"buy\", weights=-100, prob=c(1,0,0.7, 1,1,0.3), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=1 #     w$state(label=\"good\")           # v=(1,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.6, 1,1,0.4), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(1,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.6, 1,2,0.4), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=2 #     w$state(label=\"good\")           # v=(2,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.5, 1,1,0.5), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(2,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.5, 1,2,0.5), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(2,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(3,12,1), end=TRUE)     # transition to the node with sId=12 (Dummy) #     w$endState() #   w$endStage() #   w$stage()   # stage n=3 #     w$state(label=\"good\")           # v=(3,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.2, 1,1,0.8), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(3,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.2, 1,2,0.8), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(3,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(3,12,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=4 #     w$state(label=\"good\")        # v=(4,0) #       w$action(label=\"rep\", weights=30, prob=c(1,0,1), end=TRUE) #     w$endState() #     w$state(label=\"average\")     # v=(4,1) #       w$action(label=\"rep\", weights=10, prob=c(1,0,1), end=TRUE) #     w$endState() #     w$state(label=\"not working\") # v=(4,2) #       w$action(label=\"rep\", weights=5, prob=c(1,0,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=5 #     w$state(label=\"Dummy\", end=TRUE)        # v=(5,0) #   w$endStage() # w$endProcess() # w$closeWriter() #  # ## Some info about the model # #stateIdxDf(prefix)      # states of the MDP with labels returned as a data frame # #actionInfo(prefix)      # all action information of the MDP returned in a single data frame #  # ## Have a look at the state-expanded hypergraph # mdp<-loadMDP(prefix) # # hypergf(mdp) # #  # # ## Find optimal policy # # w<-\"Net reward\" # # valueIte(mdp, w, termValues=0) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy"},{"path":"http://relund.github.io/mdp/reference/loadMDP.html","id":null,"dir":"Reference","previous_headings":"","what":"Load the HMDP model defined in the binary files. The model are created in memory\nusing the external C++ library. — loadMDP","title":"Load the HMDP model defined in the binary files. The model are created in memory\nusing the external C++ library. — loadMDP","text":"Load HMDP model defined binary files. model created memory using external C++ library.","code":""},{"path":"http://relund.github.io/mdp/reference/loadMDP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load the HMDP model defined in the binary files. The model are created in memory\nusing the external C++ library. — loadMDP","text":"","code":"loadMDP(   prefix = \"\",   binNames = c(\"stateIdx.bin\", \"stateIdxLbl.bin\", \"actionIdx.bin\", \"actionIdxLbl.bin\",     \"actionWeight.bin\", \"actionWeightLbl.bin\", \"transProb.bin\", \"externalProcesses.bin\"),   eps = 1e-05,   check = TRUE,   verbose = FALSE,   getLog = TRUE )"},{"path":"http://relund.github.io/mdp/reference/loadMDP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load the HMDP model defined in the binary files. The model are created in memory\nusing the external C++ library. — loadMDP","text":"prefix character string prefix added binNames. Used identify specific model. binNames character vector length 7 giving names binary files storing model. eps sum transition probabilities must differ eps one. check Check MDP seems correct. verbose output running algorithms. getLog Output log messages.","code":""},{"path":"http://relund.github.io/mdp/reference/loadMDP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load the HMDP model defined in the binary files. The model are created in memory\nusing the external C++ library. — loadMDP","text":"list containing relevant information model pointer ptr model rc object memory.","code":""},{"path":"http://relund.github.io/mdp/reference/loadMDP.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Load the HMDP model defined in the binary files. The model are created in memory\nusing the external C++ library. — loadMDP","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/loadMDP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load the HMDP model defined in the binary files. The model are created in memory\nusing the external C++ library. — loadMDP","text":"","code":"# # Create the small machine repleacement problem used as an example in L.R. # # Nielsen and A.R. Kristensen. Finding the K best policies in a finite-horizon # # Markov decision process. European Journal of Operational Research, # # 175(2):1164-1179, 2006. doi:10.1016/j.ejor.2005.06.011. #  # ## Create the MDP using a dummy replacement node # prefix<-\"machine1_\" # w <- binaryMDPWriter(prefix) # w$setWeights(c(\"Net reward\")) # w$process() #   w$stage()   # stage n=0 #     w$state(label=\"Dummy\")          # v=(0,0) #       w$action(label=\"buy\", weights=-100, prob=c(1,0,0.7, 1,1,0.3), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=1 #     w$state(label=\"good\")           # v=(1,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.6, 1,1,0.4), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(1,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.6, 1,2,0.4), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=2 #     w$state(label=\"good\")           # v=(2,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.5, 1,1,0.5), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(2,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.5, 1,2,0.5), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(2,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(1,3,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=3 #     w$state(label=\"good\")           # v=(3,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.2, 1,1,0.8), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(3,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.2, 1,2,0.8), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(3,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(1,3,1), end=TRUE) #     w$endState() #     w$state(label=\"replaced\")       # v=(3,3) #       w$action(label=\"Dummy\", weights=0, prob=c(1,3,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=4 #     w$state(label=\"good\", end=TRUE)        # v=(4,0) #     w$state(label=\"average\", end=TRUE)     # v=(4,1) #     w$state(label=\"not working\", end=TRUE) # v=(4,2) #     w$state(label=\"replaced\", end=TRUE)    # v=(4,3) #   w$endStage() # w$endProcess() # w$closeWriter() #  # ## Some info about the model # #stateIdxDf(prefix)      # states of the MDP with labels returned as a data frame # #actionInfo(prefix)      # all action information of the MDP returned in a single data frame #  # ## Load the model into memory #  #  # mdp<-loadMDP(prefix) # mdp #  # ## Perform value iteration # # w<-\"Net reward\"             # label of the weight we want to optimize # # scrapValues<-c(30,10,5,0)   # scrap values (the values of the 4 states at stage 4) # # valueIte(mdp, w, termValues=scrapValues) # #  # # ## Print the optimal policy # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # ## Calculate the weights of the policy always to maintain # # #policy<-data.frame(sId=states$sId,iA=0) # # setPolicy(mdp, policy) # # calcWeights(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # ## Modify the MDP in memory: remove the maintain action in the states of stage 1 # # removeAction(mdp, sId=1, iA=0)  # remove action 0 at the state with sId=1 # # removeAction(mdp, sId=2, iA=0) # #  # # ## Perform value iteration on the modified MDP # # valueIte(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # resetActions(mdp)   # reset the MDP such that all actions are used # #  # # ## Modify the weight of action 'buy' # # setActionWeight(mdp, w=-50, sId=0, iA=0, wLbl=w) # #  # # ## Perform value iteration on the modified MDP # # valueIte(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # #  # #  #  #  #  # # The example given in L.R. Nielsen and A.R. Kristensen. Finding the K best # # policies in a finite-horizon Markov decision process. European Journal of # # Operational Research, 175(2):1164-1179, 2006. doi:10.1016/j.ejor.2005.06.011, # # does actually not have any dummy replacement node as in the MDP above. The same # # model can be created using a single dummy node at the end of the process. #  # ## Create the MDP using a single dummy node # prefix<-\"machine2_\" # w <- binaryMDPWriter(prefix) # w$setWeights(c(\"Net reward\")) # w$process() #   w$stage()   # stage n=0 #     w$state(label=\"Dummy\")          # v=(0,0) #       w$action(label=\"buy\", weights=-100, prob=c(1,0,0.7, 1,1,0.3), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=1 #     w$state(label=\"good\")           # v=(1,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.6, 1,1,0.4), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(1,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.6, 1,2,0.4), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=2 #     w$state(label=\"good\")           # v=(2,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.5, 1,1,0.5), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(2,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.5, 1,2,0.5), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(2,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(3,12,1), end=TRUE)     # transition to the node with sId=12 (Dummy) #     w$endState() #   w$endStage() #   w$stage()   # stage n=3 #     w$state(label=\"good\")           # v=(3,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.2, 1,1,0.8), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(3,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.2, 1,2,0.8), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(3,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(3,12,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=4 #     w$state(label=\"good\")        # v=(4,0) #       w$action(label=\"rep\", weights=30, prob=c(1,0,1), end=TRUE) #     w$endState() #     w$state(label=\"average\")     # v=(4,1) #       w$action(label=\"rep\", weights=10, prob=c(1,0,1), end=TRUE) #     w$endState() #     w$state(label=\"not working\") # v=(4,2) #       w$action(label=\"rep\", weights=5, prob=c(1,0,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=5 #     w$state(label=\"Dummy\", end=TRUE)        # v=(5,0) #   w$endStage() # w$endProcess() # w$closeWriter() #  # ## Some info about the model # #stateIdxDf(prefix)      # states of the MDP with labels returned as a data frame # #actionInfo(prefix)      # all action information of the MDP returned in a single data frame #  # ## Have a look at the state-expanded hypergraph # mdp<-loadMDP(prefix) # # hypergf(mdp) # #  # # ## Find optimal policy # # w<-\"Net reward\" # # valueIte(mdp, w, termValues=0) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy"},{"path":"http://relund.github.io/mdp/reference/plotHypergraph.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot parts of the state expanded hypergraph (experimental). — plotHypergraph","title":"Plot parts of the state expanded hypergraph (experimental). — plotHypergraph","text":"plot created based grid. grid point numbered left right topdown, .e. given grid coordinates (row,col) ((1,1) top left corner) grid id (row-1)*cols+col. must assign state grid point using states data frame (see ).","code":""},{"path":"http://relund.github.io/mdp/reference/plotHypergraph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot parts of the state expanded hypergraph (experimental). — plotHypergraph","text":"","code":"plotHypergraph(   gridDim,   states = NULL,   actions = NULL,   showGrid = FALSE,   fileN = NULL,   devOff = TRUE,   radx = 0.02,   rady = 0.03,   cex = 1,   marX = 0.03,   marY = 0.05,   ... )"},{"path":"http://relund.github.io/mdp/reference/plotHypergraph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot parts of the state expanded hypergraph (experimental). — plotHypergraph","text":"gridDim 2-dim vector (rows,cols) representing size grid. states data frame containing 3 columns: sId = state id, gId = grid id label = text plotted. actions data frame columns following order action type (head, tail1, tail2,...,label,lwd,lty,col,highlight), since number tails may differ NAs may appear. number (head tails) must correspond state id. Column label contains labels = text plotted (important must appear last tail column). Next columns line width, line type line color. Column highlight contains boolean true highlight hyperarc (useful want show policy). showGrid true show grid points (good debugging). fileN specified plot saved pdf file. devOff false make dev.(), .e. can add graphic file. ... Graphical parameters e.g. cex=0.5 control text size.","code":""},{"path":"http://relund.github.io/mdp/reference/plotHypergraph.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot parts of the state expanded hypergraph (experimental). — plotHypergraph","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/policyIteAve.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform policy iteration (average reward criterion) on the MDP. — policyIteAve","title":"Perform policy iteration (average reward criterion) on the MDP. — policyIteAve","text":"policy can afterwards recieved using functions getPolicy getPolicyW.","code":""},{"path":"http://relund.github.io/mdp/reference/policyIteAve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform policy iteration (average reward criterion) on the MDP. — policyIteAve","text":"","code":"policyIteAve(mdp, w, dur, maxIte = 100, getLog = TRUE)"},{"path":"http://relund.github.io/mdp/reference/policyIteAve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform policy iteration (average reward criterion) on the MDP. — policyIteAve","text":"mdp MDP loaded using loadMDP. w label weight optimize. dur label duration/time discount rates can calculated. maxIte Max number iterations. model satisfy unichain assumption algorithm may loop. getLog Output log messages.","code":""},{"path":"http://relund.github.io/mdp/reference/policyIteAve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform policy iteration (average reward criterion) on the MDP. — policyIteAve","text":"optimal gain (g) calculated.","code":""},{"path":[]},{"path":"http://relund.github.io/mdp/reference/policyIteAve.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Perform policy iteration (average reward criterion) on the MDP. — policyIteAve","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/policyIteDiscount.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform policy iteration (discounted reward criterion) on the MDP. — policyIteDiscount","title":"Perform policy iteration (discounted reward criterion) on the MDP. — policyIteDiscount","text":"policy can afterwards received using functions getPolicy getPolicyW.","code":""},{"path":"http://relund.github.io/mdp/reference/policyIteDiscount.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform policy iteration (discounted reward criterion) on the MDP. — policyIteDiscount","text":"","code":"policyIteDiscount(   mdp,   w,   dur,   rate = 0,   rateBase = 1,   discountFactor = NULL,   maxIte = 100,   discountMethod = \"continuous\",   getLog = TRUE )"},{"path":"http://relund.github.io/mdp/reference/policyIteDiscount.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform policy iteration (discounted reward criterion) on the MDP. — policyIteDiscount","text":"mdp MDP loaded using loadMDP. w label weight optimize. dur label duration/time discount rates can calculated. rate interest rate. rateBase time-horizon rate valid . discountFactor discountRate one time unit. specified rate rateBase used calculate discount rate. maxIte Max number iterations. model satisfy unichain assumption algorithm may loop. discountMethod Either 'continuous' 'discrete', corresponding discount factor exp(-rate/rateBase) 1/(1+rate/rateBase), respectively. used discountFactor NULL. getLog Output log messages.","code":""},{"path":"http://relund.github.io/mdp/reference/policyIteDiscount.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform policy iteration (discounted reward criterion) on the MDP. — policyIteDiscount","text":"Nothing.","code":""},{"path":[]},{"path":"http://relund.github.io/mdp/reference/policyIteDiscount.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Perform policy iteration (discounted reward criterion) on the MDP. — policyIteDiscount","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/randomHMDP.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a ","title":"Generate a ","text":"Generate \"random\" HMDP stored set binary files.","code":""},{"path":"http://relund.github.io/mdp/reference/randomHMDP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a ","text":"","code":"randomHMDP(   prefix = \"\",   levels = 3,   timeHorizon = c(Inf, 3, 4),   states = c(2, 4, 5),   actions = c(1, 2),   childProcessPr = 0.5,   externalProcessPr = 0,   rewards = c(0, 100),   durations = c(1, 10),   rewardName = \"Reward\",   durationName = \"Duration\" )"},{"path":"http://relund.github.io/mdp/reference/randomHMDP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a ","text":"prefix character string prefix added til file(s). levels Maximum number levels. Set childProcessPr=1 want exact number levels. timeHorizon time horizon level (vector). founder timehorizon can Inf. states Number states stage given level (vector length levels) actions Min max number actions state. childProcessPr Probability creating child process define action. externalProcessPr Probability creating external process given create child process. works levels>2 currently generate external processes include external processes. rewards Min max reward used. durations Min max duration used. rewardName Weight name used reward. durationName Weight name used duration.","code":""},{"path":"http://relund.github.io/mdp/reference/randomHMDP.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Generate a ","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/saveMDP.html","id":null,"dir":"Reference","previous_headings":"","what":"Save the MDP to binary files — saveMDP","title":"Save the MDP to binary files — saveMDP","text":"Currently save external files.","code":""},{"path":"http://relund.github.io/mdp/reference/saveMDP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save the MDP to binary files — saveMDP","text":"","code":"saveMDP(mdp, prefix = \"\", getLog = TRUE)"},{"path":"http://relund.github.io/mdp/reference/saveMDP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save the MDP to binary files — saveMDP","text":"mdp MDP loaded using loadMDP. prefix character string prefix added binNames. Used identify specific model. getLog Output log message.","code":""},{"path":"http://relund.github.io/mdp/reference/saveMDP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save the MDP to binary files — saveMDP","text":"rpo (matrix/data frame).","code":""},{"path":"http://relund.github.io/mdp/reference/saveMDP.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Save the MDP to binary files — saveMDP","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/setPolicy.html","id":null,"dir":"Reference","previous_headings":"","what":"Modify the current policy by setting policy action of states. — setPolicy","title":"Modify the current policy by setting policy action of states. — setPolicy","text":"policy contain states actions previous optimal policy used.","code":""},{"path":"http://relund.github.io/mdp/reference/setPolicy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Modify the current policy by setting policy action of states. — setPolicy","text":"","code":"setPolicy(mdp, policy)"},{"path":"http://relund.github.io/mdp/reference/setPolicy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Modify the current policy by setting policy action of states. — setPolicy","text":"mdp MDP loaded using loadMDP. policy data frame two columns state id action index.","code":""},{"path":"http://relund.github.io/mdp/reference/setPolicy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Modify the current policy by setting policy action of states. — setPolicy","text":"Nothing.","code":""},{"path":"http://relund.github.io/mdp/reference/setPolicy.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Modify the current policy by setting policy action of states. — setPolicy","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/stateIdxDf.html","id":null,"dir":"Reference","previous_headings":"","what":"Info about the states in the HMDP model under consideration. — stateIdxDf","title":"Info about the states in the HMDP model under consideration. — stateIdxDf","text":"Info states HMDP model consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/stateIdxDf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Info about the states in the HMDP model under consideration. — stateIdxDf","text":"","code":"stateIdxDf(prefix = \"\", file = \"stateIdx.bin\", labels = \"stateIdxLbl.bin\")"},{"path":"http://relund.github.io/mdp/reference/stateIdxDf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Info about the states in the HMDP model under consideration. — stateIdxDf","text":"prefix character string prefix added til file(s). file HMDP binary file containing description consideration. labels HMDP binary file containing labels consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/stateIdxDf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Info about the states in the HMDP model under consideration. — stateIdxDf","text":"data frame columns stateIdxMat plus another column containing labels.","code":""},{"path":"http://relund.github.io/mdp/reference/stateIdxDf.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Info about the states in the HMDP model under consideration. — stateIdxDf","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/stateIdxMat.html","id":null,"dir":"Reference","previous_headings":"","what":"Info about the states in the HMDP model under consideration. — stateIdxMat","title":"Info about the states in the HMDP model under consideration. — stateIdxMat","text":"Info states HMDP model consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/stateIdxMat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Info about the states in the HMDP model under consideration. — stateIdxMat","text":"","code":"stateIdxMat(prefix = \"\", file = \"stateIdx.bin\")"},{"path":"http://relund.github.io/mdp/reference/stateIdxMat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Info about the states in the HMDP model under consideration. — stateIdxMat","text":"prefix character string prefix added til file(s). file HMDP binary file containing description consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/stateIdxMat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Info about the states in the HMDP model under consideration. — stateIdxMat","text":"matrix columns (sId, n0, s0, a0, ...) sId state row id, n0 index stage level 0, s0 index state a0 index action. HMDP one level columns index (d1, s1, a1, ...) added.","code":""},{"path":"http://relund.github.io/mdp/reference/stateIdxMat.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Info about the states in the HMDP model under consideration. — stateIdxMat","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/transProbMat.html","id":null,"dir":"Reference","previous_headings":"","what":"Info about the transition probabilities in the HMDP model under consideration. — transProbMat","title":"Info about the transition probabilities in the HMDP model under consideration. — transProbMat","text":"Info transition probabilities HMDP model consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/transProbMat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Info about the transition probabilities in the HMDP model under consideration. — transProbMat","text":"","code":"transProbMat(prefix = \"\", file = \"transProb.bin\")"},{"path":"http://relund.github.io/mdp/reference/transProbMat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Info about the transition probabilities in the HMDP model under consideration. — transProbMat","text":"prefix character string prefix added til file(s). file HMDP binary file containing description consideration.","code":""},{"path":"http://relund.github.io/mdp/reference/transProbMat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Info about the transition probabilities in the HMDP model under consideration. — transProbMat","text":"matrix columns (aId, ...) aId action row id ... probabilities action.","code":""},{"path":"http://relund.github.io/mdp/reference/transProbMat.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Info about the transition probabilities in the HMDP model under consideration. — transProbMat","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/valueIte.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform value iteration on the MDP. — valueIte","title":"Perform value iteration on the MDP. — valueIte","text":"MDP finite time-horizon arguments times eps ignored.","code":""},{"path":"http://relund.github.io/mdp/reference/valueIte.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform value iteration on the MDP. — valueIte","text":"","code":"valueIte(   mdp,   w,   dur = NULL,   rate = 0,   rateBase = 1,   discountFactor = NULL,   maxIte = 100,   eps = 1e-05,   termValues = NULL,   g = NULL,   getLog = TRUE,   discountMethod = \"continuous\" )"},{"path":"http://relund.github.io/mdp/reference/valueIte.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform value iteration on the MDP. — valueIte","text":"mdp MDP loaded using loadMDP. w label weight optimize. dur label duration/time discount rates can calculated. rate Interest rate. rateBase time-horizon rate valid . discountFactor discountRate one time unit. specified rate rateBase used calculate discount rate. maxIte max number iterations value iteration performed. eps Stopping criterion. max(w(t)-w(t+1))<epsilon stop algorithm, .e policy becomes epsilon optimal (see 1 p161). termValues terminal values used (values last stage MDP). g Average reward. specified single iteration using opdate equations average reward criterion specified g value. getLog Output log messages. discountMethod Either 'continuous' 'discrete', corresponding discount factor exp(-rate/rateBase) 1/(1+rate/rateBase), respectively. used discountFactor NULL.","code":""},{"path":"http://relund.github.io/mdp/reference/valueIte.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform value iteration on the MDP. — valueIte","text":"NULL (invisible)","code":""},{"path":"http://relund.github.io/mdp/reference/valueIte.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Perform value iteration on the MDP. — valueIte","text":"1 Puterman, M.; Markov Decision Processes, Wiley-Interscience, 1994.","code":""},{"path":"http://relund.github.io/mdp/reference/valueIte.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Perform value iteration on the MDP. — valueIte","text":"Lars Relund lars@relund.dk","code":""},{"path":"http://relund.github.io/mdp/reference/valueIte.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform value iteration on the MDP. — valueIte","text":"","code":"# # Create the small machine repleacement problem used as an example in L.R. # # Nielsen and A.R. Kristensen. Finding the K best policies in a finite-horizon # # Markov decision process. European Journal of Operational Research, # # 175(2):1164-1179, 2006. doi:10.1016/j.ejor.2005.06.011. #  # ## Create the MDP using a dummy replacement node # prefix<-\"machine1_\" # w <- binaryMDPWriter(prefix) # w$setWeights(c(\"Net reward\")) # w$process() #   w$stage()   # stage n=0 #     w$state(label=\"Dummy\")          # v=(0,0) #       w$action(label=\"buy\", weights=-100, prob=c(1,0,0.7, 1,1,0.3), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=1 #     w$state(label=\"good\")           # v=(1,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.6, 1,1,0.4), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(1,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.6, 1,2,0.4), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=2 #     w$state(label=\"good\")           # v=(2,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.5, 1,1,0.5), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(2,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.5, 1,2,0.5), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(2,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(1,3,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=3 #     w$state(label=\"good\")           # v=(3,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.2, 1,1,0.8), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(3,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.2, 1,2,0.8), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(3,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(1,3,1), end=TRUE) #     w$endState() #     w$state(label=\"replaced\")       # v=(3,3) #       w$action(label=\"Dummy\", weights=0, prob=c(1,3,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=4 #     w$state(label=\"good\", end=TRUE)        # v=(4,0) #     w$state(label=\"average\", end=TRUE)     # v=(4,1) #     w$state(label=\"not working\", end=TRUE) # v=(4,2) #     w$state(label=\"replaced\", end=TRUE)    # v=(4,3) #   w$endStage() # w$endProcess() # w$closeWriter() #  # ## Some info about the model # #stateIdxDf(prefix)      # states of the MDP with labels returned as a data frame # #actionInfo(prefix)      # all action information of the MDP returned in a single data frame #  # ## Load the model into memory #  #  # mdp<-loadMDP(prefix) # mdp #  # ## Perform value iteration # # w<-\"Net reward\"             # label of the weight we want to optimize # # scrapValues<-c(30,10,5,0)   # scrap values (the values of the 4 states at stage 4) # # valueIte(mdp, w, termValues=scrapValues) # #  # # ## Print the optimal policy # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # ## Calculate the weights of the policy always to maintain # # #policy<-data.frame(sId=states$sId,iA=0) # # setPolicy(mdp, policy) # # calcWeights(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # ## Modify the MDP in memory: remove the maintain action in the states of stage 1 # # removeAction(mdp, sId=1, iA=0)  # remove action 0 at the state with sId=1 # # removeAction(mdp, sId=2, iA=0) # #  # # ## Perform value iteration on the modified MDP # # valueIte(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # # resetActions(mdp)   # reset the MDP such that all actions are used # #  # # ## Modify the weight of action 'buy' # # setActionWeight(mdp, w=-50, sId=0, iA=0, wLbl=w) # #  # # ## Perform value iteration on the modified MDP # # valueIte(mdp, w, termValues=scrapValues) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy # #  # #  # #  #  #  #  # # The example given in L.R. Nielsen and A.R. Kristensen. Finding the K best # # policies in a finite-horizon Markov decision process. European Journal of # # Operational Research, 175(2):1164-1179, 2006. doi:10.1016/j.ejor.2005.06.011, # # does actually not have any dummy replacement node as in the MDP above. The same # # model can be created using a single dummy node at the end of the process. #  # ## Create the MDP using a single dummy node # prefix<-\"machine2_\" # w <- binaryMDPWriter(prefix) # w$setWeights(c(\"Net reward\")) # w$process() #   w$stage()   # stage n=0 #     w$state(label=\"Dummy\")          # v=(0,0) #       w$action(label=\"buy\", weights=-100, prob=c(1,0,0.7, 1,1,0.3), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=1 #     w$state(label=\"good\")           # v=(1,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.6, 1,1,0.4), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(1,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.6, 1,2,0.4), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=2 #     w$state(label=\"good\")           # v=(2,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.5, 1,1,0.5), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(2,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.5, 1,2,0.5), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(2,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(3,12,1), end=TRUE)     # transition to the node with sId=12 (Dummy) #     w$endState() #   w$endStage() #   w$stage()   # stage n=3 #     w$state(label=\"good\")           # v=(3,0) #       w$action(label=\"mt\", weights=55, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=70, prob=c(1,0,0.2, 1,1,0.8), end=TRUE) #     w$endState() #     w$state(label=\"average\")        # v=(3,1) #       w$action(label=\"mt\", weights=40, prob=c(1,0,1), end=TRUE) #       w$action(label=\"nmt\", weights=50, prob=c(1,1,0.2, 1,2,0.8), end=TRUE) #     w$endState() #     w$state(label=\"not working\")    # v=(3,2) #       w$action(label=\"mt\", weights=30, prob=c(1,0,1), end=TRUE) #       w$action(label=\"rep\", weights=5, prob=c(3,12,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=4 #     w$state(label=\"good\")        # v=(4,0) #       w$action(label=\"rep\", weights=30, prob=c(1,0,1), end=TRUE) #     w$endState() #     w$state(label=\"average\")     # v=(4,1) #       w$action(label=\"rep\", weights=10, prob=c(1,0,1), end=TRUE) #     w$endState() #     w$state(label=\"not working\") # v=(4,2) #       w$action(label=\"rep\", weights=5, prob=c(1,0,1), end=TRUE) #     w$endState() #   w$endStage() #   w$stage()   # stage n=5 #     w$state(label=\"Dummy\", end=TRUE)        # v=(5,0) #   w$endStage() # w$endProcess() # w$closeWriter() #  # ## Some info about the model # #stateIdxDf(prefix)      # states of the MDP with labels returned as a data frame # #actionInfo(prefix)      # all action information of the MDP returned in a single data frame #  # ## Have a look at the state-expanded hypergraph # mdp<-loadMDP(prefix) # # hypergf(mdp) # #  # # ## Find optimal policy # # w<-\"Net reward\" # # valueIte(mdp, w, termValues=0) # # policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId # # #states<-stateIdxDf(prefix)              # information about the states # # #policy<-merge(states,policy)            # merge the two data frames # # #policyW<-getPolicyW(mdp, w)             # the optimal rewards of the policy # # #policy<-merge(policy,policyW)           # add the rewards # # policy"},{"path":"http://relund.github.io/mdp/reference/weightNames.html","id":null,"dir":"Reference","previous_headings":"","what":"Names of weights used in actions. — weightNames","title":"Names of weights used in actions. — weightNames","text":"Names weights used actions.","code":""},{"path":"http://relund.github.io/mdp/reference/weightNames.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Names of weights used in actions. — weightNames","text":"","code":"weightNames(prefix = \"\", labels = \"actionWeightLbl.bin\")"},{"path":"http://relund.github.io/mdp/reference/weightNames.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Names of weights used in actions. — weightNames","text":"prefix character string prefix added binary file names. labels HMDP binary file containing weight labels.","code":""},{"path":"http://relund.github.io/mdp/reference/weightNames.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Names of weights used in actions. — weightNames","text":"Vector weight names.","code":""},{"path":"http://relund.github.io/mdp/reference/weightNames.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Names of weights used in actions. — weightNames","text":"Lars Relund lars@relund.dk","code":""}]
