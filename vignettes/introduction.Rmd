---
title: "An introduction to the MDP2 package in R"
author: "Lars Relund <lars@relund.dk>"
date: "`r Sys.Date()`"
bibliography: litt.bib
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{An introduction to the MDP2 package in R}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

<style> 
p {text-align: justify;} 
//.sourceCode {background-color: white;}
pre {
  border-style: solid;
  border-width: 1px;
  border-color: grey;
  //background-color: grey !important;
}
img {width: 100%;}
</style>

<!-- scale math down -->
<script type="text/x-mathjax-config"> 
    MathJax.Hub.Config({ 
        "HTML-CSS": { scale: 80 }
        });
</script>

```{r setup, include=FALSE}
library(knitr)
options(formatR.arrow = TRUE, width=90) # , scipen=999, digits=5
#thm <- knit_theme$get("edit-kwrite")   # whitengrey, bright, print, edit-flashdevelop, edit-kwrite
#knit_theme$set(thm)
opts_chunk$set(fig.align='center', 
               fig.width=10, fig.height=5, 
               fig.show='hold', 
               out.extra='style="max-width:100%;"',
               # tidy = TRUE,
               prompt=T,
               comment=NA,
               cache=F, 
               background="red")
```


The `MDP2` package in R is a package for solving Markov decision processes (MDPs) with discrete 
time-steps, states and actions. Both traditional MDPs [@Puterman94], semi-Markov decision processes
(semi-MDPs) [@Tijms03] and hierarchical-MDPs (HMDPs) [@Kristensen00] can be solved under a finite
and infinite time-horizon.

Building and solving an MDP is done in two steps. First, the MDP is built and saved in a set 
of binary files. Next, you load the MDP into memory from the binary files and apply various
algorithms to the model.

The package implement well-known algorithms such as policy iteration and value iteration
under different criteria e.g. average reward per time unit and expected total discounted reward. The
model is stored using an underlying data structure based on the *state-expanded directed hypergraph*
of the MDP (@Relund06) implemented in `C++` for fast running times. <!-- Under development is also
support for MLHMP which is a Java implementation of algorithms for solving MDPs (@Kristensen03). 
-->

The newest version of the package can be installed from GitHub 

```{r Install github, echo=TRUE, eval=FALSE}
install_github("relund/mdp")
```
We load the package using 

```{r Load package, echo=TRUE, warning=FALSE, results='hide', message=FALSE}
library(MDP2)
```

Help about the package can be seen by writing

```{r Package help, echo=TRUE, eval=FALSE}
?MDP2
```

To illustrate the package capabilities, we use a few examples, namely, an infinite and
finite-horizon semi-MDP and a HMDP. Before each example a short introduction to these models are
given.


## An infinite Semi-MDP 

An *infinite-horizon semi-MDP* considers a sequential decision problem over an infinite number of 
*stages*. Let $I$ denote the finite set of system states at stage $n$. Note we assume that the 
semi-MDP is *homogeneous*, i.e the state space is independent of stage number. When *state* $i \in 
I$ is observed, an *action* $a$ from the finite set of allowable actions $A(i)$ must be chosen which
generates *reward* $r(i,a)$. Moreover, let $\tau(i,a)$ denote the *stage length* of action $a$, i.e.
the expected time until the next decision epoch (stage $n+1$) given action $a$ and state $i$.
Finally, let $p_{ij}(a)$ denote the *transition probability* of obtaining state $j\in I$ at stage
$n+1$ given that action $a$ is chosen in state $i$ at stage $n$. A policy is a decision 
rule/function that assigns to each state in the process an action.

Let us consider example 6.1.1 in @Tijms03. At the beginning of each day a piece of equipment is
inspected to reveal its actual working condition. The equipment will be found in one of the working
conditions $i = 1,\ldots, N$ where the working condition $i$ is better than the working condition 
$i+1$. The equipment deteriorates in time. If the present working condition is $i$ and no repair is
done, then at the beginning of the next day the equipment has working condition $j$ with probability
$q_{ij}$. It is assumed that $q_{ij}=0$ for $j<i$ and $\sum_{j\geq i}q_{ij}=1$. The working
condition $i=N$ represents a malfunction that requires an enforced repair taking two days. For the
intermediate states $i$ with $1<i<N$ there is a choice between preventively repairing the equipment
and letting the equipment operate for the present day. A preventive repair takes only one day. A
repaired system has the working condition $i=1$. The cost of an enforced repair upon failure is
$C_{f}$ and the cost of a preemptive repair in working condition $i$ is $C_{p}(i)$. We wish to
determine a maintenance rule which minimizes the long-run average repair cost per day.

To formulate this problem as an infinite horizon semi-MDP the set of possible states of the system
is chosen as
$$
I=\{1,2,\ldots,N\}.
$$
State $i$ corresponds to the situation in which an inspection reveals working condition $i$. Define actions
$$
a=\left\{\begin{array}{ll}
0 & \text{if no repair.}\\
1 & \text{if preventive repair.}\\
2 & \text{if forced repair.}\\
\end{array}\right.
$$
The set of possible actions in state $i$ is chosen as $A(1)=\{0\},\ A(i)=\{0,1\}$ for $1<i<N, 
A(N)=\{2\}$. The one-step transition probabilities $p_{ij}(a)$ are given by $p_{ij}(0) = q_{ij}$ for
$1\leq i<N$, $p_{i1}(1) = 1$ for $1<i<N$, $p_{N1}(2)=1$  and zero otherwise. The one-step costs
$c_{i}(a)$ are given by $c_{i}(0)=0,\ c_{i}(1)=C_{p}(i)$ and $c_{N}(2)=C_{f}$. The stage length
until next decision epoch are $\tau(i,a) = 1, 0\leq i < N$ and $\tau(N,a) = 2$.

Assume that the number of possible working conditions equals $N=5$. The repair costs are given by $C_{f}=10,\ C_{p}(2)=7,\ C_{p}(3)=7$ and $C_{p}(4)=5$. The deterioration probabilities $q_{ij}$ are given by

```{r parameters,  include=FALSE}
N<-5; Cf<- -10; Cp<-c(0,-7,-7,-5) # use negative numbers since the MDP optimize based on rewards
Q <- matrix(c(
   0.90, 0.10, 0, 0, 0,
   0, 0.80, 0.10, 0.05, 0.05,
   0, 0, 0.70, 0.10, 0.20,
   0, 0, 0, 0.50, 0.50), nrow=4, byrow=T) 
```

```{r Qtable, results='asis', echo=FALSE}
rownames(Q)<-1:4
colnames(Q)<-1:5
knitr::kable(Q, row.names = T)
```


```{r states, include=FALSE}
states<-data.frame(id=1:N-1,label=paste0("i=",1:N), stringsAsFactors = F)
states
```

```{r transPr, include=FALSE}
# transform state to id
state2Id<-function(i) return(i-1)

# input state i and action a
transPr<-function(a,i) {
   if (a==0) {
      pr<-Q[i,]
      iN<-which(pr>0)
      pr<-pr[iN]       # only consider trans pr > 0
   }
   if (a>0) {
      pr<-1
      iN<-1
   }
   return(list(pr=pr,id=state2Id(iN)))
}
transPr(0,1)
```

```{r buildMDP1, include=FALSE}
# Build the model which is stored in a set of binary files
w<-binaryMDPWriter("hct611-1_")
w$setWeights(c("Duration","Net reward"))
w$process()
   w$stage()
      w$state(label="i=1")
         dat<-transPr(0,1)
         w$action(label="no repair", weights=c(1,0), pr=dat$pr, id=dat$id, end=T)
      w$endState()
      for (ii in 2:(N-1) ) {
         w$state(label=states$label[ii])
            dat<-transPr(0,ii)
            w$action(label="no repair", weights=c(1,0), pr=dat$pr, id=dat$id, end=T)
            dat<-transPr(1,ii)
            w$action(label="preventive repair", weights=c(1,Cp[ii]), pr=dat$pr, id=dat$id, end=T)
         w$endState()
      }
      w$state(label=paste0("i=",N))
         dat<-transPr(2,N)
         w$action(label="forced repair", weights=c(2,Cf), pr=dat$pr, id=dat$id, end=T)
      w$endState()
   w$endStage()
w$endProcess()
w$closeWriter()
```

A state-expanded hypergraph representing the semi-MDP with infinite time-horizon is shown below. Each 
node corresponds to a specific state in the MDP and is given the stage assigned an *unique id* 
(**id must always start from zero**). A directed hyperarc is defined for each possible action. For
instance, the state/node with id 1 corresponds to working condition $i=2$ and the two hyperarcs
with head in this node corresponds to the two actions preventive and no repair. Note the tails of a
hyperare represent a possible transition ($p_{ij}(a)>0$).

```{r plotHgf, echo=FALSE, results='hide', message=FALSE}
mdp<-loadMDP("hct611-1_")
dat <- infoMDP(mdp, withHarc = TRUE)
stateDF<-dat$df
stateDF$label<-paste0(c(1:N,1:N)-1,": i=",c(1:N,1:N) )
#stateDF$label[1:N]<-""
stateDF$gId<-c(seq(2,10,by=2),seq(1,9,by=2))
actionDF<-dat$harcDF
actionDF$lwd<-0.5
actionDF$lty[actionDF$label=="no repair"]<-1
actionDF$lty[actionDF$label=="preventive repair"]<-1
actionDF$lty[actionDF$label=="forced repair"]<-1
actionDF$col[actionDF$label=="no repair"]<-"darkorange1"
actionDF$col[actionDF$label=="preventive repair"]<-"deepskyblue3"
actionDF$col[actionDF$label=="forced repair"]<-"chartreuse4"
actionDF$highlight<-FALSE
par(mai=c(0,0,0,0))
plotHypergraph(gridDim=c(N,2), states = stateDF, actions = actionDF)
```

To build the semi-MDP in R, we use the `binaryMDPWriter` where the model can be built using either matrices or an hierarchical structure. We first illustrate how to use the hierarchical structure. First, we load the parameters:

```{r view_param, eval=FALSE, ref.label='parameters', echo=TRUE}
```

and make a data frame for the states:

```{r view_states, eval=TRUE, ref.label='states', echo=TRUE}
```

To build the model we need transition probabilities and the state ids for the corresponding transitions. We here do this using a function:

```{r view_transPr, eval=TRUE, ref.label='transPr', echo=TRUE}
```

We can now build the model using the `binaryMDPWriter`:

```{r view_buldMDP1, eval=FALSE, ref.label='buildMDP1', echo=TRUE, tidy=FALSE}
```

Note that we build the model with two weights applied to each action "Duration" and "Net reward". 
That is, when we specify an action, we must add two weights. "Duration" equals 1 day except in state
$i=N$ where a forced repair takes 2 days. The process is built using first a `process` which
contains a `stage` (we only specify one stage, since we have a homogeneous semi-MDP over an infinite
horizon) which contains `state`s which contains `action`s. Transitions of an `action` are specified
using the `pr` and `id` parameter. The model is saved in a set of files with prefix "`hct611-1_`".

The model can be loaded using 

```{r load}
mdp<-loadMDP("hct611-1_")
mdp # overall info
info<-infoMDP(mdp)  # more detailed info
info$df
```

Note the loaded model gives each node in the state-expanded hypergraph a *unique id* such that you
can identify all the states. These ids are not equal to the ids used when you built the model, since
the order of the nodes in the hypergraph data structure is optimized! Given the model in memory, we
now can find the optimal policy under various policies. Let us first try to optimize the average
reward per time unit.

```{r solve1_ave}
# Optimal policy under average reward per time unit criterion
policyIteAve(mdp,"Net reward","Duration")
getPolicy(mdp)
```

Note it is optimal to do a preventive repair in state $i=4$. Let us try to optimize the expected 
total discounted reward using both policy iteration and value iteration.

```{r solve1_discount}
# Optimal policy under expected discounted reward criterion (use both policy and value ite)
policyIteDiscount(mdp,"Net reward","Duration", discountFactor = 0.5)
getPolicy(mdp)
valueIte(mdp,"Net reward","Duration", discountFactor = 0.5, eps = 1e-10, maxIte = 1000)
getPolicy(mdp)
```

Note given a discount factor of 0.5, it is optimal to not do a preventive repair in state $i=4$. 

The model can also be built by specifying a set of matrices. Note this way of specifying **only 
work** for infinite-horizon semi-MDPs (and not finite-horizon or hierarchical models). Specify a
list of probability matrices  (one for each action) where each row/state contains the transition 
probabilities (all zero if the action is not used in a state), a matrix with rewards and a matrix
with stage lengths (row = state, column = action). Let us try to build and solve the model again.

```{r buildMDP2}
## define probability matrices
P<-list()
# a=1 (no repair)
P[[1]]<-as.matrix(rbind(Q,0))
# a=2 (preventive repair)
Z <- matrix(0, nrow = N, ncol = N)
Z[2,1]<-Z[3,1]<-Z[4,1]<-1
P[[2]]<-Z
# a=3 (forced repair)
Z <- matrix(0, nrow = N, ncol = N)
Z[5,1]<-1
P[[3]]<-Z
# reward 6x3 matrix with one column for each action
R <- matrix(0, nrow = N, ncol = 3)
R[2:4,2]<-Cp[2:4]
R[5,3]<-Cf
# state lengths
D <- matrix(1, nrow = N, ncol = 3)
D[5,3]<-2

# build model
w<-binaryMDPWriter("hct611-2_")
w$setWeights(c("Duration","Net reward"))
w$process(P,R,D)
w$closeWriter()
```

```{r solve2}
mdp<-loadMDP("hct611-2_")
policyIteAve(mdp,"Net reward","Duration")
getPolicy(mdp)
```




## A finite-horizon Semi-MDP

A *finite-horizon semi-MDP* considers a sequential decision problem over $N$ *stages*. Let $I_{n}$
denote the finite set of system states at stage $n$. When *state* $i \in I_{n}$ is observed, an
*action* $a$ from the finite set of allowable actions $A_n(i)$ must be chosen, and this decision
generates *reward* $r_{n}(i,a)$. Moreover, let $\tau_n(i,a)$ denote the *stage length* of action
$a$, i.e. the expected time until the next decision epoch (stage $n+1$) given action $a$ and state
$i$. Finally, let $p_{ij}(a,n)$ denote the *transition probability* of obtaining state $j\in I_{n+1}$
at stage $n+1$ given that action $a$ is chosen in state $i$ at stage $n$. 

Consider a small machine repair problem used as an example in @Relund06 where the machine is always
replaced after 4 years. The state of the machine may be: good, average, and not working. Given the
machine's state we may maintain the machine. In this case the machine's state will be good at the
next decision epoch. Otherwise, the machine's state will not be better at next decision epoch. When
the machine is bought it may be either in state good or average. Moreover, if the machine is not
working it must be replaced.

The problem of when to replace the machine can be modelled using a Markov decision process with 
$N=5$ decision epochs. We use system states `good`,  `average`, `not working` and dummy state
`replaced` together with actions buy (`buy`), maintain (`mt`), no maintenance (`nmt`), and replace
(`rep`). The set of states at stage zero $S_{0}$ contains a single dummy state `dummy` representing
the machine before knowing its initial state. The only possible action is `buy`.

The cost of buying the machine is 100 with transition probability of 0.7 to state `good` and 0.3 to
state `average`. The reward (scrap value) of replacing a machine is 30, 10, and 5 in state `good`,  `average` and `not working`, respectively. The reward of the machine given action `mt` are 55, 40, and 30 in state `good`,  `average` and `not working`, respectively. Moreover, the system enters state 0 with probability 1 at the next stage.
Finally, the reward, transition states and probabilities given action $a=$`nmt` are given by:

  $n:s$               $1:$ `good`        $1:$ `average`        $2:$ `good`           $2:$ `average`         $3:$ `good`           $3:$ `average`
  -------------     ---------------    ------------------    ------------------    -------------------    ------------------    ------------------   
  $r_n(i,a)$          70                 50                    70                    50                     70                    50
  $j$                 $\{0,1\}$          $\{1,2\}$             $\{0,1\}$             $\{1,2\}$              $\{0,1\}$             $\{1,2\}$
  $p_{ij}(a,n)$       $\{0.6,0.4\}$      $\{0.6,0.4\}$         $\{0.5,0.5\}$         $\{0.5,0.5\}$          $\{0.2,0.8\}$         $\{0.2,0.8\}$


The semi-MDP with time-horizon $N=5$ is illustrated below. Each node corresponds to a specific state
and a directed hyperarc is defined for each possible action. For instance, action `mt` (maintain)
corresponds to a deterministic transition to state `good` and action `nmt` (not maintain)
corresponds to a transition to a condition/state not better than the current condition/state. We buy
the machine in stage 1 and may choose to replace the machine.

```{r buildMDP3, include=FALSE}
prefix<-"machine1_"
w <- binaryMDPWriter(prefix)
w$setWeights(c("Net reward"))
w$process()
	w$stage()   # stage n=0
		w$state(label="Dummy")       
			w$action(label="buy", weights=-100, pr=c(0.7,0.3), id=c(0,1), end=TRUE)
		w$endState()
	w$endStage()
	w$stage()   # stage n=1
		w$state(label="good")           
			w$action(label="mt", weights=55, pr=1, id=0, end=TRUE)
			w$action(label="nmt", weights=70, pr=c(0.6,0.4), id=c(0,1), end=TRUE)
		w$endState()
		w$state(label="average")        
			w$action(label="mt", weights=40, pr=1, id=0, end=TRUE)
			w$action(label="nmt", weights=50, pr=c(0.6,0.4), id=c(1,2), end=TRUE)
		w$endState()
	w$endStage()
	w$stage()   # stage n=2
		w$state(label="good")          
			w$action(label="mt", weights=55, pr=1, id=0, end=TRUE)
			w$action(label="nmt", weights=70, pr=c(0.5,0.5), id=c(0,1), end=TRUE)
		w$endState()
		w$state(label="average")       
			w$action(label="mt", weights=40, pr=1, id=0, end=TRUE)
			w$action(label="nmt", weights=50, pr=c(0.5,0.5), id=c(1,2), end=TRUE)
		w$endState()
		w$state(label="not working")    
			w$action(label="mt", weights=30, pr=1, id=0, end=TRUE)
			w$action(label="rep", weights=5, pr=1, id=3, end=TRUE)
		w$endState()
	w$endStage()
	w$stage()   # stage n=3
		w$state(label="good")           
			w$action(label="mt", weights=55, pr=1, id=0, end=TRUE)
			w$action(label="nmt", weights=70, pr=c(0.2,0.8), id=c(0,1), end=TRUE)
		w$endState()
		w$state(label="average")       
			w$action(label="mt", weights=40, pr=1, id=0, end=TRUE)
			w$action(label="nmt", weights=50, pr=c(0.2,0.8), id=c(1,2), end=TRUE)
		w$endState()
		w$state(label="not working")    
			w$action(label="mt", weights=30, pr=1, id=0, end=TRUE)
			w$action(label="rep", weights=5, pr=1, id=3, end=TRUE)
		w$endState()
		w$state(label="replaced")       
			w$action(label="Dummy", weights=0, pr=1, id=3, end=TRUE)
		w$endState()
	w$endStage()
	w$stage()   # stage n=4
		w$state(label="good", end=TRUE)        
		w$state(label="average", end=TRUE)     
		w$state(label="not working", end=TRUE) 
		w$state(label="replaced", end=TRUE)   
	w$endStage()
w$endProcess()
w$closeWriter()
```

```{r plotHgf3, echo=FALSE}
scrapValues<-c(30,10,5,0)   # scrap values (the values of the 4 states at stage 4)
mdp<-loadMDP("machine1_", getLog = FALSE)
dat<-infoMDP(mdp, withHarc = TRUE)
stateDF<-dat$df
stateDF$gId<-c(5,10,15,20,4,9,14,19,3,8,13,2,7,1)
actionDF<-dat$harcDF
actionDF$lwd<-0.5
actionDF$lwd<-1
actionDF$col<-"deepskyblue3"
actionDF$highlight<-FALSE
par(mai=c(0,0.1,0,0.1))
plotHypergraph(gridDim=c(4,5), states = stateDF, actions = actionDF, radx = 0.06, marX = 0.06)
```

We build the semi-MDP using `binaryMDPWriter`:

```{r view_buildMDP3, eval=FALSE, ref.label='buildMDP3', echo=TRUE, tidy=FALSE}
```

Note that at each stage the states are numbered using id's starting from zero such that e.g. 
`w$action(label="nmt", weights=50, pr=c(0.2,0.8), id=c(1,2), end=TRUE)` define an action with transition to states with id's 1 and 2 at the next stage with probability 0.2 and 0.8, respectively.

Let us try to load the model and get some info:

```{r load3}
mdp<-loadMDP("machine1_")
mdp # overall info
info<-infoMDP(mdp)  # more detailed info
info$df
```

Let us use value iteration to find the optimal policy maximizing the expected total reward under the
assumption that terminal values are `r paste(scrapValues,collapse=",")`.

```{r solve3}
scrapValues<-c(30,10,5,0)   # scrap values (the values of the 4 states at stage 4)
valueIte(mdp, "Net reward" , termValues=scrapValues)
getPolicy(mdp)
```

The optimal policy is illustrated below:

```{r plotPolicy3, echo=FALSE, results='hide', message=FALSE}
library(tidyverse)
actionDF <- left_join(getPolicy(mdp), dat$harcDF, by = c("sId" = "head", "actionLabel" = "label")) %>% 
   filter(aIdx >= 0) %>% 
   select(head = sId, contains("tail"), label = actionLabel)
actionDF$lwd<-1
actionDF$col<-"deepskyblue3"
actionDF$highlight<-FALSE
par(mai=c(0,0.1,0,0.1))
plotHypergraph(gridDim=c(4,5), states = stateDF, actions = actionDF, radx = 0.06, marX = 0.06)
```

Note given the optimal policy the machine will never make a transition to states `not working` and `replaced`.
We may evaluate a certain policy, e.g. the policy always to maintain the machine:

```{r Set policy (machine rep),echo=TRUE,eval=TRUE}
policy<-data.frame(sId=c(8,11),aIdx=c(0,0))
setPolicy(mdp, policy)
getPolicy(mdp)
```

If the policy specified in `setPolicy` does not contain all states then the actions from the
previous optimal policy are used. In the output above we can see that the policy now is to maintain always. However, the reward of the policy has not been updated. Let us calculate the expected reward:

```{r Calc reward (machine rep),echo=TRUE}
calcWeights(mdp, "Net reward", termValues=scrapValues)
getPolicy(mdp)    
```

That is, the expected reward is 90.5 compared to 102.2 which was the reward of the optimal policy.


## An infinite-horizon HMDP

A hierarchical MDP is an MDP with parameters defined in a special way, but nevertheless in 
accordance with all usual rules and conditions relating to such processes (@Kristensen00). The basic
idea of the hierarchical structure is that stages of the process can be expanded to a so-called 
*child process*, which again may expand stages further to new child processes leading to multiple 
levels. To illustrate consider the HMDP shown in the figure below. The process has three levels. At
`Level 2` we have a set of finite-horizon semi-MDPs (one for each oval box) which all can be 
represented using a state-expanded hypergraph (hyperarcs not shown, only hyperarcs connecting 
processes are shown). A semi-MDP at `Level 2` is uniquely defined by a given state $s$ and action
$a$ of its *parent process* at `Level 1` (illustrated by the arcs with head and tail node at `Level
1` and `Level 2`, respectively). Moreover, when a child process at `Level 2` terminates a transition
from a state $s\in \mathcal{S}_{N}$ of the child process to a state at the next stage of the parent
process occur (illustrated by the (hyper)arcs having head and tail at `Level 2` and `Level 1`, 
respectively). 

![A state-expanded hypergraph for an HMDP.](vignette_files/hmdp_index.png)
*A hypergraph representation of the first stage of a hierarchical MDP. Level 0 indicate the
founder level, and the nodes indicates states at the different levels. A child
process (oval box) is represented using its state-expanded hypergraph (hyperarcs
not shown) and is uniquely defined by a given state and action of its parent
process.*

Since a child process is always defined by a stage, state and action of the parent process we have
that for instance a state at Level 1 can be identified using an index vector 
$\nu=(n_{0},s_{0},a_{0},n_{1},s_{1})$ where $s_1$ is the state id at the given stage $n_1$ in the 
process defined by the action $a_0$ in state $s_0$ at stage $n_0$. Note all values are ids starting 
from zero, e.g. if $s_1=0$ it is the first state at the corresponding stage and if $a_0=2$ it is the
third action at the corresponding state. In general a state $s$ and action $a$ at level $l$ can be
uniquely identified using
$$
\begin{aligned}
\nu_{s}&=(n_{0},s_{0},a_{0},n_{1},s_{1},\ldots,n_{l},s_{l}) \\
\nu_{a}&=(n_{0},s_{0},a_{0},n_{1},s_{1},\ldots,n_{l},s_{l},a_{l}).
\end{aligned}
$$
The index vectors for state $v_0$, $v_1$ and $v_2$ are illustrated in the figure. As under a 
semi-MDP another way to identify a state in the state-expanded hypergraph is using an unique id. 

Let us try to solve a small problem from livestock farming, namely the cow replacement problem where
we want to represent the age of the cow, i.e. the lactation number of the cow. During a lactation a
cow may have a high, average or low yield. We assume that a cow is always replaced after 4 
lactations.

In addition to lactation and milk yield we also want to take the genetic merit into account which is
either bad, average or good. When a cow is replaced we assume that the probability of a bad, average
or good heifer is equal.

We formulate the problem as a HMDP with 2 levels. At level 0 the states are the genetic merit and
the length of a stage is a life of a cow. At level 1 a stage describe a lactation and states
describe the yield. Decisions at level 1 are `keep` or `replace`.

Note the MDP runs over an infinite time-horizon at the founder level where each state (genetic
merit) define a semi-MDP at level 1 with 4 lactations.

To generate the MDP we need to know the weights and transition probabilities which are provided in a
csv file. To ease the understanding we provide 2 functions for reading from the csv:

```{r Generate cow MDP functions,echo=TRUE}
cowDf<-read.csv("vignette_files/cow.csv")
head(cowDf)

lev1W<-function(s0Idx,n1Idx,s1Idx,a1Lbl) {
	r<-subset(cowDf,s0==s0Idx & n1==n1Idx & s1==s1Idx & label==a1Lbl)
	return(as.numeric(r[5:7]))
}
lev1W(2,2,1,'Keep')     # good genetic merit, lactation 2, avg yield, keep action

lev1Pr<-function(s0Idx,n1Idx,s1Idx,a1Lbl) {
	r<-subset(cowDf,s0==s0Idx & n1==n1Idx & s1==s1Idx & label==a1Lbl)
	return(as.numeric(r[8:16]))
}
lev1Pr(2,2,1,'Replace') # good genetic merit, lactation 2, avg yield, replace action
```

We can now generate the model with three weights
```{r Generate cow MDP,echo=TRUE, tidy=FALSE}
lblS0<-c('Bad genetic level','Avg genetic level','Good genetic level')
lblS1<-c('Low yield','Avg yield','High yield')
prefix<-"cow_"
w<-binaryMDPWriter(prefix)
w$setWeights(c("Duration", "Net reward", "Yield"))
w$process()
	w$stage()   # stage 0 at founder level
		for (s0 in 0:2) {
			w$state(label=lblS0[s0+1])   # state at founder
				w$action(label="Keep", weights=c(0,0,0), prob=c(2,0,1))   # action at founder
					w$process()
						w$stage()   # dummy stage at level 1
							 w$state(label="Dummy")
								w$action(label="Dummy", weights=c(0,0,0), 
								         prob=c(1,0,1/3, 1,1,1/3, 1,2,1/3), end=TRUE)
							 w$endState()
						w$endStage()
						for (d1 in 1:4) {
							w$stage()   # stage at level 1
								for (s1 in 0:2) {
									w$state(label=lblS1[s1+1])
										if (d1!=4) {
											w$action(label="Keep", weights=lev1W(s0,d1,s1,"Keep"), 
											         prob=lev1Pr(s0,d1,s1,"Keep"), end=TRUE)
										}
										w$action(label="Replace", weights=lev1W(s0,d1,s1,"Replace"), 
										         prob=lev1Pr(s0,d1,s1,"Replace"), end=TRUE)
									w$endState()
								}
							w$endStage()
						}
					w$endProcess()
				w$endAction()
			w$endState()
		}
	w$endStage()
w$endProcess()
w$closeWriter()
```

Note that the model is built using the `prob` parameter which contains triples of (scope,id,pr).
Scope can be: 2 = a transition to a child process (stage zero in the child process), 1 = a
transition to next stage in the current process and 0 = a transition to the next stage in the father
process. For instance if `prob = c(1,2,0.3, 0,0,0.7)` we specify transitions to the third state (id
= 2) at the next stage of the process and to the first state (id = 0) at the next stage of the
father process with probabilities 0.3 and 0.7, respectively.

A plot of the state-expanded hypergraph are given below where action `keep` is drawn with orange color and action `replace` with blue color.

```{r plotHMDP, echo=FALSE, results='hide', message=FALSE}
mdp<-loadMDP(prefix)
dat<-infoMDP(mdp, withHarc = TRUE)
stateDF<-dat$df
stateDF$label[stateDF$label==""]<-c("Bad","Avg","Good")
stateDF$label[stateDF$label=="Low yield"]<-"L"
stateDF$label[stateDF$label=="Avg yield"]<-"A"
stateDF$label[stateDF$label=="High yield"]<-"H"
stateDF$label[stateDF$label=="Dummy"]<-"D"
stateDF$label[stateDF$label=="Bad genetic level"]<-"Bad"
stateDF$label[stateDF$label=="Avg genetic level"]<-"Avg"
stateDF$label[stateDF$label=="Good genetic level"]<-"Good"
stateDF$gId <- NA
stateDF$gId[1:3]<-c(7,14,21)
stateDF$gId[43:45]<-c(1,8,15)
getGId<-function(process,stage,state) {
   if (process==0) start=23
   if (process==1) start=51
   if (process==2) start=79
   return(start + stage + 7*state)
}
idx<-43
for (process in 0:2)
   for (stage in 0:4)
      for (state in 0:2) {
         if (stage==0 & state>0) break
         idx<-idx-1
         #cat(idx,process,stage,state,getGId(process,stage,state),"\n")
         stateDF$gId[idx]<-getGId(process,stage,state)
      }
actionDF<-dat$harcDF
actionDF$label[actionDF$label=="Replace"]<-"R"
actionDF$label[actionDF$label=="Keep"]<-"K"
actionDF$label[actionDF$label=="Dummy"]<-"D"
actionDF$lwd<-0.5
actionDF$lty[actionDF$label=="K"]<-1
actionDF$lty[actionDF$label=="R"]<-1
actionDF$lty[actionDF$label=="D"]<-1
actionDF$col[actionDF$label=="K"]<-"darkorange1"
actionDF$col[actionDF$label=="R"]<-"deepskyblue3"
actionDF$col[actionDF$label=="D"]<-"black"
actionDF$col[67:69]<-"black"
actionDF$highlight<-FALSE
actionDF$label<-""
par(mai=c(0,0,0,0))
plotHypergraph(gridDim=c(14,7), states = stateDF, actions = actionDF, cex = 0.8)
```

We find the optimal policy under the expected discounted reward criterion the
HMDP using policy iteration:

```{r Optimize (cow), tidy.opts=list(comment=FALSE)}
## solve under discount criterion
mdp<-loadMDP(prefix)
wLbl<-"Net reward"         # the weight we want to optimize (net reward)
durLbl<-"Duration"         # the duration/time label
policyIteDiscount(mdp, wLbl, durLbl, rate=0.1)
getPolicy(mdp)
# rpo<-calcRPO(mdp, wLbl, iA=rep(0,42), criterion="discount", dur=durLbl, rate=rate, rateBase=rateBase)
# policy<-merge(policy,rpo)
# policy
```

A plot of the optimal policy can be seen below.

```{r plotPolicy, echo=FALSE, results='hide', message=FALSE}
actionDF <- left_join(getPolicy(mdp), dat$harcDF, by = c("sId" = "head", "actionLabel" = "label")) %>% 
   filter(aIdx >= 0) %>% 
   select(head = sId, contains("tail"), label = actionLabel)
# actionDF<-cbind(dat$actionDF,dat$harcDF)
# actionDF<-merge(actionDF,getPolicy(mdp))[,c("head","tail2","tail3","tail4","label","stateLabel")]
actionDF$label[actionDF$label=="Replace"]<-"R"
actionDF$label[actionDF$label=="Keep"]<-"K"
actionDF$label[actionDF$label=="Dummy"]<-"D"
actionDF$lwd<-0.5
actionDF$lty[actionDF$label=="K"]<-1
actionDF$lty[actionDF$label=="R"]<-1
actionDF$lty[actionDF$label=="D"]<-1
actionDF$col[actionDF$label=="K"]<-"darkorange1"
actionDF$col[actionDF$label=="R"]<-"deepskyblue3"
actionDF$col[actionDF$label=="D"]<-"black"
actionDF$col[35:37]<-"black"
actionDF$highlight<-FALSE
actionDF$label<-""
par(mai=c(0,0,0,0))
plotHypergraph(gridDim=c(14,7), states = stateDF, actions = actionDF, cex = 0.8)
```

We may also find the policy with maximizing average reward per lactation: 

```{r avePerLac, tidy.opts=list(comment=FALSE)}
wLbl<-"Net reward"         # the weight we want to optimize (net reward)
durLbl<-"Duration"         # the duration/time label
policyIteAve(mdp, wLbl, durLbl)
getPolicy(mdp)
```

Since other weights are defined for each action we can calculate the average
reward per litre milk under the optimal policy:

```{r Piglets/time (sow rep), echo=TRUE}
calcWeights(mdp, w=wLbl, criterion="average", dur = "Yield")
```

or the average yield per lactation:

```{r Reward/piglet (sow rep), echo=TRUE}
calcWeights(mdp, w="Yield", criterion="average", dur = durLbl)
```




```{r Delete bin, include=FALSE}
do.call(file.remove,list(list.files(pattern = ".bin")))
```


## References
