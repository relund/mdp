<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="MDP2">
<title>Solving a finite-horizon semi-MDP • MDP2</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Solving a finite-horizon semi-MDP">
<meta property="og:description" content="MDP2">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">MDP2</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.1.2</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/building.html">Building an MDP model</a>
    <a class="dropdown-item" href="../articles/finite-mdp.html">Solving a finite-horizon semi-MDP</a>
    <a class="dropdown-item" href="../articles/infinite-hmdp.html">An infinite-horizon HMDP</a>
    <a class="dropdown-item" href="../articles/infinite-mdp.html">Solving an infinite-horizon semi-MDP</a>
    <a class="dropdown-item" href="../articles/mdp2.html">The MDP2 package</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/relund/mdp/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Solving a finite-horizon semi-MDP</h1>
                        <h4 data-toc-skip class="author">Lars Relund <a href="mailto:lars@relund.dk" class="email">lars@relund.dk</a>
</h4>
            
            <h4 data-toc-skip class="date">2023-01-29</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/relund/mdp/blob/HEAD/vignettes/finite-mdp.Rmd" class="external-link"><code>vignettes/finite-mdp.Rmd</code></a></small>
      <div class="d-none name"><code>finite-mdp.Rmd</code></div>
    </div>

    
    
<style> 
p {text-align: justify;} 
//.sourceCode {background-color: white;}
pre {
  // border-style: solid;
  // border-width: 1px;
  // border-color: grey;
  //background-color: grey !important;
}
img {
   //width: 100%;
   border: 0;
}
</style>
<!-- scale math down --><script type="text/x-mathjax-config"> 
    MathJax.Hub.Config({ 
        "HTML-CSS": { scale: 80 }
        });
</script><p>The <code>MDP2</code> package in R is a package for solving Markov
decision processes (MDPs) with discrete time-steps, states and actions.
Both traditional MDPs <span class="citation">(Puterman 1994)</span>,
semi-Markov decision processes (semi-MDPs) <span class="citation">(Tijms
2003)</span> and hierarchical-MDPs (HMDPs) <span class="citation">(Kristensen and Jørgensen 2000)</span> can be solved
under a finite and infinite time-horizon.</p>
<p>The package implement well-known algorithms such as policy iteration
and value iteration under different criteria e.g. average reward per
time unit and expected total discounted reward. The model is stored
using an underlying data structure based on the <em>state-expanded
directed hypergraph</em> of the MDP (<span class="citation">Nielsen and
Kristensen (2006)</span>) implemented in <code>C++</code> for fast
running times. <!-- Under development is also
support for MLHMP which is a Java implementation of algorithms for solving MDPs (@Kristensen03). 
--></p>
<p>Building and solving an MDP is done in two steps. First, the MDP is
built and saved in a set of binary files. Next, you load the MDP into
memory from the binary files and apply various algorithms to the
model.</p>
<p>For building the MDP models see <code><a href="../articles/building.html">vignette("building")</a></code>. In
this vignette we focus on the second step, i.e. finding the optimal
policy. Here we consider a finite-horizon semi-MDP.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://relund.github.io/mdp/" class="external-link">MDP2</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="a-finite-horizon-semi-mdp">A finite-horizon semi-MDP<a class="anchor" aria-label="anchor" href="#a-finite-horizon-semi-mdp"></a>
</h2>
<p>A <em>finite-horizon semi-MDP</em> considers a sequential decision
problem over <span class="math inline">\(N\)</span> <em>stages</em>. Let
<span class="math inline">\(I_{n}\)</span> denote the finite set of
system states at stage <span class="math inline">\(n\)</span>. When
<em>state</em> <span class="math inline">\(i \in I_{n}\)</span> is
observed, an <em>action</em> <span class="math inline">\(a\)</span> from
the finite set of allowable actions <span class="math inline">\(A_n(i)\)</span> must be chosen, and this decision
generates <em>reward</em> <span class="math inline">\(r_{n}(i,a)\)</span>. Moreover, let <span class="math inline">\(\tau_n(i,a)\)</span> denote the <em>stage
length</em> of action <span class="math inline">\(a\)</span>, i.e. the
expected time until the next decision epoch (stage <span class="math inline">\(n+1\)</span>) given action <span class="math inline">\(a\)</span> and state <span class="math inline">\(i\)</span>. Finally, let <span class="math inline">\(p_{ij}(a,n)\)</span> denote the <em>transition
probability</em> of obtaining state <span class="math inline">\(j\in
I_{n+1}\)</span> at stage <span class="math inline">\(n+1\)</span> given
that action <span class="math inline">\(a\)</span> is chosen in state
<span class="math inline">\(i\)</span> at stage <span class="math inline">\(n\)</span>.</p>
</div>
<div class="section level2">
<h2 id="example">Example<a class="anchor" aria-label="anchor" href="#example"></a>
</h2>
<p>Consider a small machine repair problem used as an example in <span class="citation">Nielsen and Kristensen (2006)</span> where the machine
is always replaced after 4 years. The state of the machine may be: good,
average, and not working. Given the machine’s state we may maintain the
machine. In this case the machine’s state will be good at the next
decision epoch. Otherwise, the machine’s state will not be better at
next decision epoch. When the machine is bought it may be either in
state good or average. Moreover, if the machine is not working it must
be replaced.</p>
<p>The problem of when to replace the machine can be modeled using a
Markov decision process with <span class="math inline">\(N=5\)</span>
decision epochs. We use system states <code>good</code>,
<code>average</code>, <code>not working</code> and dummy state
<code>replaced</code> together with actions buy (<code>buy</code>),
maintain (<code>mt</code>), no maintenance (<code>nmt</code>), and
replace (<code>rep</code>). The set of states at stage zero <span class="math inline">\(S_{0}\)</span> contains a single dummy state
<code>dummy</code> representing the machine before knowing its initial
state. The only possible action is <code>buy</code>.</p>
<p>The cost of buying the machine is 100 with transition probability of
0.7 to state <code>good</code> and 0.3 to state <code>average</code>.
The reward (scrap value) of replacing a machine is 30, 10, and 5 in
state <code>good</code>, <code>average</code> and
<code>not working</code>, respectively. The reward of the machine given
action <code>mt</code> are 55, 40, and 30 in state <code>good</code>,
<code>average</code> and <code>not working</code>, respectively.
Moreover, the system enters state 0 with probability 1 at the next
stage. Finally, the reward, transition states and probabilities given
action <span class="math inline">\(a=\)</span><code>nmt</code> are given
by:</p>
<table class="table">
<thead><tr class="header">
<th align="left"><span class="math inline">\(n:s\)</span></th>
<th align="center">
<span class="math inline">\(1:\)</span>
<code>good</code>
</th>
<th align="center">
<span class="math inline">\(1:\)</span>
<code>average</code>
</th>
<th align="center">
<span class="math inline">\(2:\)</span>
<code>good</code>
</th>
<th align="center">
<span class="math inline">\(2:\)</span>
<code>average</code>
</th>
<th align="center">
<span class="math inline">\(3:\)</span>
<code>good</code>
</th>
<th align="center">
<span class="math inline">\(3:\)</span>
<code>average</code>
</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(r_n(i,a)\)</span></td>
<td align="center">70</td>
<td align="center">50</td>
<td align="center">70</td>
<td align="center">50</td>
<td align="center">70</td>
<td align="center">50</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(j\)</span></td>
<td align="center"><span class="math inline">\(\{0,1\}\)</span></td>
<td align="center"><span class="math inline">\(\{1,2\}\)</span></td>
<td align="center"><span class="math inline">\(\{0,1\}\)</span></td>
<td align="center"><span class="math inline">\(\{1,2\}\)</span></td>
<td align="center"><span class="math inline">\(\{0,1\}\)</span></td>
<td align="center"><span class="math inline">\(\{1,2\}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(p_{ij}(a,n)\)</span></td>
<td align="center"><span class="math inline">\(\{0.6,0.4\}\)</span></td>
<td align="center"><span class="math inline">\(\{0.6,0.4\}\)</span></td>
<td align="center"><span class="math inline">\(\{0.5,0.5\}\)</span></td>
<td align="center"><span class="math inline">\(\{0.5,0.5\}\)</span></td>
<td align="center"><span class="math inline">\(\{0.2,0.8\}\)</span></td>
<td align="center"><span class="math inline">\(\{0.2,0.8\}\)</span></td>
</tr>
</tbody>
</table>
<p>Let us try to load the model and get some info:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prefix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">"models"</span>, package <span class="op">=</span> <span class="st">"MDP2"</span><span class="op">)</span>, <span class="st">"/machine1_"</span><span class="op">)</span></span>
<span><span class="va">mdp</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/loadMDP.html">loadMDP</a></span><span class="op">(</span><span class="va">prefix</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; Read binary files (0.000171799 sec.)</span></span>
<span><span class="co">#&gt; Build the HMDP (4.58e-05 sec.)</span></span></code></pre>
<pre><code><span><span class="co">#&gt; Checking MDP and found no errors (1.2e-06 sec.)</span></span></code></pre>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/getInfo.html">getInfo</a></span><span class="op">(</span><span class="va">mdp</span>, withList <span class="op">=</span> <span class="cn">F</span>, dfLevel <span class="op">=</span> <span class="st">"action"</span>, asStringsActions <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>  </span></code></pre></div>
<pre><code><span><span class="co">#&gt; $df</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 18 × 8</span></span></span>
<span><span class="co">#&gt;      sId stateStr label        aIdx label_action weights trans pr     </span></span>
<span><span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>        <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span>     4 3,0      good            0 mt           55      0     1      </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span>     4 3,0      good            1 nmt          70      0,1   0.2,0.8</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span>     5 3,1      average         0 mt           40      0     1      </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span>     5 3,1      average         1 nmt          50      1,2   0.2,0.8</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span>     6 3,2      not working     0 mt           30      0     1      </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span>     6 3,2      not working     1 rep          5       3     1      </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span>     7 3,3      replaced        0 Dummy        0       3     1      </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span>     8 2,0      good            0 mt           55      4     1      </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span>     8 2,0      good            1 nmt          70      4,5   0.5,0.5</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span>     9 2,1      average         0 mt           40      4     1      </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">11</span>     9 2,1      average         1 nmt          50      5,6   0.5,0.5</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">12</span>    10 2,2      not working     0 mt           30      4     1      </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">13</span>    10 2,2      not working     1 rep          5       7     1      </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">14</span>    11 1,0      good            0 mt           55      8     1      </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">15</span>    11 1,0      good            1 nmt          70      8,9   0.6,0.4</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">16</span>    12 1,1      average         0 mt           40      8     1      </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">17</span>    12 1,1      average         1 nmt          50      9,10  0.6,0.4</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">18</span>    13 0,0      Dummy           0 buy          -100    11,12 0.7,0.3</span></span></code></pre>
<p>The state-expanded hypergraph representing the semi-MDP with finite
time-horizon can be plotted using</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">mdp</span>, hyperarcColor <span class="op">=</span> <span class="st">"label"</span>, radx <span class="op">=</span> <span class="fl">0.06</span>, marX <span class="op">=</span> <span class="fl">0.065</span>, marY <span class="op">=</span> <span class="fl">0.055</span><span class="op">)</span></span></code></pre></div>
<p><img src="finite-mdp_files/figure-html/unnamed-chunk-3-1.png" width="864" style="max-width:100%;"></p>
<p>Each node corresponds to a specific state and a directed hyperarc is
defined for each possible action. For instance, action <code>mt</code>
(maintain) corresponds to a deterministic transition to state
<code>good</code> and action <code>nmt</code> (not maintain) corresponds
to a transition to a condition/state not better than the current
condition/state. We buy the machine in stage 1 and may choose to replace
the machine.</p>
<p>Let us use value iteration to find the optimal policy maximizing the
expected total reward:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">scrapValues</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">30</span>, <span class="fl">10</span>, <span class="fl">5</span>, <span class="fl">0</span><span class="op">)</span>   <span class="co"># scrap values (the values of the 4 states at the last stage)</span></span>
<span><span class="fu"><a href="../reference/runValueIte.html">runValueIte</a></span><span class="op">(</span><span class="va">mdp</span>, <span class="st">"Net reward"</span>, termValues <span class="op">=</span> <span class="va">scrapValues</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; Run value iteration with epsilon = 0 at most 1 time(s)</span></span>
<span><span class="co">#&gt; using quantity 'Net reward' under reward criterion.</span></span>
<span><span class="co">#&gt;  Finished. Cpu time 7.7e-06 sec.</span></span></code></pre>
<p>The optimal policy is:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pol</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/getPolicy.html">getPolicy</a></span><span class="op">(</span><span class="va">mdp</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">tail</a></span><span class="op">(</span><span class="va">pol</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 6 × 6</span></span></span>
<span><span class="co">#&gt;     sId stateStr stateLabel   aIdx actionLabel weight</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>        <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span>     8 2,0      good            1 nmt           148.</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span>     9 2,1      average         0 mt            125 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span>    10 2,2      not working     0 mt            115 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">4</span>    11 1,0      good            1 nmt           208.</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">5</span>    12 1,1      average         0 mt            188.</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">6</span>    13 0,0      Dummy           0 buy           102.</span></span></code></pre>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">mdp</span>, hyperarcShow <span class="op">=</span> <span class="st">"policy"</span>, nodeLabel <span class="op">=</span> <span class="st">"weight"</span>, </span>
<span>     radx <span class="op">=</span> <span class="fl">0.06</span>, marX <span class="op">=</span> <span class="fl">0.065</span>, marY <span class="op">=</span> <span class="fl">0.055</span><span class="op">)</span></span></code></pre></div>
<p><img src="finite-mdp_files/figure-html/unnamed-chunk-4-1.png" width="864" style="max-width:100%;"></p>
<p>Note given the optimal policy the total expected reward is 102.2 and
the machine will never make a transition to states
<code>not working</code> and <code>replaced</code>.</p>
<p>We may evaluate a certain policy, e.g. the policy always to maintain
the machine:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">policy</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>sId<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">8</span>,<span class="fl">11</span><span class="op">)</span>, aIdx<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0</span><span class="op">)</span><span class="op">)</span> <span class="co"># set the policy for sId 8 and 11 to mt</span></span>
<span><span class="fu"><a href="../reference/setPolicy.html">setPolicy</a></span><span class="op">(</span><span class="va">mdp</span>, <span class="va">policy</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/getPolicy.html">getPolicy</a></span><span class="op">(</span><span class="va">mdp</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 14 × 6</span></span></span>
<span><span class="co">#&gt;      sId stateStr stateLabel   aIdx actionLabel weight</span></span>
<span><span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>        <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span>     0 4,0      good           -<span style="color: #BB0000;">1</span> <span style="color: #949494;">""</span>             30 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span>     1 4,1      average        -<span style="color: #BB0000;">1</span> <span style="color: #949494;">""</span>             10 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span>     2 4,2      not working    -<span style="color: #BB0000;">1</span> <span style="color: #949494;">""</span>              5 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span>     3 4,3      replaced       -<span style="color: #BB0000;">1</span> <span style="color: #949494;">""</span>              0 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span>     4 3,0      good            0 <span style="color: #949494;">"</span>mt<span style="color: #949494;">"</span>           85 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span>     5 3,1      average         0 <span style="color: #949494;">"</span>mt<span style="color: #949494;">"</span>           70 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span>     6 3,2      not working     0 <span style="color: #949494;">"</span>mt<span style="color: #949494;">"</span>           60 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span>     7 3,3      replaced        0 <span style="color: #949494;">"</span>Dummy<span style="color: #949494;">"</span>         0 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span>     8 2,0      good            0 <span style="color: #949494;">"</span>mt<span style="color: #949494;">"</span>          148.</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span>     9 2,1      average         0 <span style="color: #949494;">"</span>mt<span style="color: #949494;">"</span>          125 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">11</span>    10 2,2      not working     0 <span style="color: #949494;">"</span>mt<span style="color: #949494;">"</span>          115 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">12</span>    11 1,0      good            0 <span style="color: #949494;">"</span>mt<span style="color: #949494;">"</span>          208.</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">13</span>    12 1,1      average         0 <span style="color: #949494;">"</span>mt<span style="color: #949494;">"</span>          188.</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">14</span>    13 0,0      Dummy           0 <span style="color: #949494;">"</span>buy<span style="color: #949494;">"</span>         102.</span></span></code></pre>
<p>If the policy specified in <code>setPolicy</code> does not contain
all states then the actions from the previous optimal policy are used.
In the output above we can see that the policy now is to maintain
always. However, the reward of the policy has not been updated. Let us
calculate the expected reward:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/runCalcWeights.html">runCalcWeights</a></span><span class="op">(</span><span class="va">mdp</span>, <span class="st">"Net reward"</span>, termValues <span class="op">=</span> <span class="va">scrapValues</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">tail</a></span><span class="op">(</span><span class="fu"><a href="../reference/getPolicy.html">getPolicy</a></span><span class="op">(</span><span class="va">mdp</span><span class="op">)</span><span class="op">)</span>    </span></code></pre></div>
<pre><code><span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 6 × 6</span></span></span>
<span><span class="co">#&gt;     sId stateStr stateLabel   aIdx actionLabel weight</span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>        <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span>     8 2,0      good            0 mt           140  </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span>     9 2,1      average         0 mt           125  </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span>    10 2,2      not working     0 mt           115  </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">4</span>    11 1,0      good            0 mt           195  </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">5</span>    12 1,1      average         0 mt           180  </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">6</span>    13 0,0      Dummy           0 buy           90.5</span></span></code></pre>
<p>That is, the expected reward is 90.5 compared to 102.2 which was the
reward of the optimal policy.</p>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Kristensen00" class="csl-entry">
Kristensen, A. R., and E. Jørgensen. 2000. <span>“Multi-Level Hierarchic
<span>M</span>arkov Processes as a Framework for Herd Management
Support.”</span> <em>Annals of Operations Research</em> 94: 69–89. <a href="https://doi.org/10.1023/A:1018921201113" class="external-link">https://doi.org/10.1023/A:1018921201113</a>.
</div>
<div id="ref-Relund06" class="csl-entry">
Nielsen, L. R., and A. R. Kristensen. 2006. <span>“Finding the
<span><span class="math inline">\(K\)</span></span> Best Policies in a
Finite-Horizon <span>M</span>arkov Decision Process.”</span>
<em>European Journal of Operational Research</em> 175 (2): 1164–79. <a href="https://doi.org/10.1016/j.ejor.2005.06.011" class="external-link">https://doi.org/10.1016/j.ejor.2005.06.011</a>.
</div>
<div id="ref-Puterman94" class="csl-entry">
Puterman, M. L. 1994. <em>Markov Decision Processes</em>. Wiley Series
in Probability and Mathematical Statistics. Wiley-Interscience.
</div>
<div id="ref-Tijms03" class="csl-entry">
Tijms, Henk. C. 2003. <em>A First Course in Stochastic Models</em>. John
Wiley &amp; Sons Ltd.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Lars Relund Nielsen.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
