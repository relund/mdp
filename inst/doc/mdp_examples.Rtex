% \VignetteIndexEntry{MDP examples}

\documentclass[12pt,a4paper]{article}
\usepackage{a4wide}
\usepackage[latin1]{inputenc} % Support for danish letters
\usepackage{mathptmx} % Use Times as roman font and in math
\usepackage[scaled=0.95]{helvet}  % Use helvetica as sans serif font and scale it down to 0.95%
%\renewcommand{\familydefault}{phv}
\usepackage{courier} % Use courier as typewriter font
\usepackage[T1]{fontenc} % The font encoding used. Always load after the last font package.
%\usepackage{theorem}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{tabularx}
%\usepackage{booktabs}
\usepackage{fancyref,frefext}
\renewcommand{\fancyrefdefaultformat}{plain}
\usepackage[english]{babel}
\usepackage[numbers,sort&compress,longnamesfirst]{natbib}   % Natbib for better citations
\input{mdp_examples_files/listingsRw}
%input "mdp_examples.Rtex"
\usepackage[pdftex,colorlinks=false,linkcolor=blue,breaklinks=true]{hyperref}


%% ---------------------------------------------------------------------
%% Make DOI's clickable in the pdf files
%% ---------------------------------------------------------------------
\makeatletter
\newcommand*{\doi}{\begingroup\lccode`\~=`\#\relax\lowercase{\def~{\#}}%
\lccode`\~=`\<\relax\lowercase{\def~{\textless}}\lccode`\~=`\>\relax%
\lowercase{\def~{\textgreater}}\lccode`\~=0\relax\catcode`\#=\active%
\catcode`\<=\active\catcode`\>=\active\@doi}%
\def\@doi#1{\let\#\relax\let\textless\relax\let\textgreater\relax\edef\x{%
\toks0={{doi:#1}}}\x\edef\#{\@percentchar23}\edef\textless{\string<}%
\edef\textgreater{\string>}\edef\x{\toks1={\noexpand\href{http://dx.doi.org/#1}}}%
\x\edef\x{\endgroup\the\toks1 \the\toks0}\x}%
\makeatother
%% ---------------------------------------------------------------------

%% ---------------------------------------------------------------------
%% Controlling floats better
%% ---------------------------------------------------------------------
\renewcommand{\topfraction}{0.9}        % A top float which fills x% of the page
\renewcommand{\bottomfraction}{0.8}     % A bottom float which fills x% of the page
\renewcommand{\textfraction}{0.1}       % Text must fill at least x%
\renewcommand{\floatpagefraction}{0.8}  % A float page must fill at least x%
%% ---------------------------------------------------------------------


%\includeonly{}
%\parindent0pt


\newcommand{\ie}{i.e.\ }
\newcommand{\eg}{e.g.\ }
\newcommand{\cpp}{\texttt{C++}\ }
\begin{document}

%% Makes the title, abstract and keywords left aligned

% redefine the \maketitle command so left aligned
% authors must be separated with \\ in \author
\makeatletter
\def\@maketitle{%
  \newpage
  \null
  \vskip 2em%
  \let \footnote \thanks
	{\Large\bfseries\noindent\@title\par}%
	\vskip 1.5em%
	{\large
	  \lineskip .5em%
		\raggedright\@author}
	\vskip 1em%
	{\noindent\small\@date}%
  \par
  \vskip 1.5em}
\makeatother

% redefine abstract environment so left justified
\renewenvironment{abstract}{\parindent0pt\textbf{Abstract:}}{}

\title{Solving MDPs using the MDP package in R}

\author{%
Lars Relund Nielsen\thanks{Corresponding author}\\
{\small Department of Genetics and Biotechnology, University of Aarhus, P.O.
Box 50, DK-8830 Tjele, Denmark, \url{lars@relund.dk}.} \\[0.5em]
}%

\date{\today}

\maketitle

%-------------------


\SweaveOpts{prefix.string=mdp_examples_files/plot}
\SweaveOpts{keep.source=TRUE}

\begin{Scode}{label=Initilize, echo=false}
options(width=110)

library(lattice)
pstheme <- canonical.theme(color = FALSE) ## in-built B&W theme
pstheme$strip.background$col <- "transparent" ## change strip bg
pstheme$superpose.line$lty<-1:10
pstheme$superpose.line$col<-rep(c(1,8),each=4)
pstheme$superpose.line$lwd<-c(rep(1,8))
pstheme$superpose.symbol$pch<-c(1,3,6,0,5,16,17)

pdftheme <- canonical.theme()
pdftheme$strip.background$col <- "transparent" ## change strip bg
pdftheme$background$col <- "transparent" ## change bg
#pdftheme$superpose.line$lty<-1:10
#pdftheme$superpose.symbol$pch<-rep(c(1,4),each=4)

lattice.options(default.theme = pdftheme) ## set as default

## function for saving trellis graphics to both pdf and eps
savePlot<-function(name, plot, w=8.25, h=11.75, thm="pdftheme", fontf="Helvetica", eps=FALSE, landscape=FALSE) {
	if (landscape) {tmp<-w; w<-h; h<-tmp }
	trellis.device("pdf",file=paste("mdp_examples_files/",name,".pdf",sep=""),family=fontf,theme=thm,width=w,height=h)
	print(plot)
	dev.off()
	if (eps) {
		trellis.device("postscript",file=paste("mdp_examples_files/",name,".eps",sep=""),family=fontf,theme=thm,paper="special",horizontal=F,width=w,height=h)
		print(plot)
		dev.off()
	}
}
\end{Scode}


\section{Introduction}

The MDP package in R is a package for solving Markov decision processes (MDPs)
with discrete time-steps, states and actions. Both ordinary \citep{Puterman94}
and hierarchial MDPs \citep{Kristensen00} can be solved. In this paper we use
the term MDP for both types of MDPs.

Generating and solving an MDP is done in two steps. First, the MDP is generated
and saved in a set of binary files. Next, you load the MDP into memory from the
binary files and solve it.

The package uses algorithms based on the \emph{state-expanded directed
hypergraph} of the MDP \citep{Relund06} which are all implemented in \cpp for
fast running times. Under development is also support for MLHMP which is a Java
implementation of algorithms for solving MDPs \citep{Kristensen03}. A
hypergraph representing an MDP with time-horizon $N=5$ is shown in
\Fref[plain]{fig:state_hgf}. Each node corresponds to a specific state in the
MDP and a directed hyperarc is defined for each possible action. For instance,
node $v_{2,1}$ corresponds to a state number 1 at stage 2. The two hyperarcs
with head in node $v_{3,0}$ show that two actions are possible given state
number 0 at stage 3. Action \texttt{mt} corresponds to a deterministic
transition to state number zero at stage 4 and action \texttt{nmt} corresponds
to a transition to state number 0 or 1 at stage 4 with a certain probability
greater than zero.

\begin{figure}[ptb]
\begin{center}
	\includegraphics[width=0.8\linewidth]{mdp_examples_files/state_hgf}
\end{center}
\caption{ A state-expanded hypergraph for an MDP with time horizon $N=5$. At stage $n$
each node $v_{n,i}$ corresponds to a state in $\mathcal{S}_n$. The hyperarcs
correspond to actions, e.g. if the system at stage $3$ is in state number 1 then
there are two possible actions. Action \texttt{mt} results in a deterministic
transition to state zero (because there is only one tail) at stage $4$ and
\texttt{nmt} results in a transition to either state number 1 or 2 with a certain
probability.
}%
\label{fig:state_hgf}%
\end{figure}

States and actions can be identified using either an unique id or index vector
$\nu$. In an ordinary MDP the index vector consists of the stage and state
number, \ie state corresponding to node $v_{3,1}$ in \Fref{fig:state_hgf} is
uniquely identified using $\nu=(n,s)=(3,1)$. Similar, action \texttt{buy} is
uniquely identified using $\nu=(n,s,a)=(0,0,0)$. Note that \textbf{index always
start from zero}.

\begin{figure}[ptb]
\begin{center}
\includegraphics[width=\linewidth]{mdp_examples_files/hmdp_index}
\end{center}
\caption{
A hypergraph representation of the first stage of a hierarchical MDP. Level 0 indicate the
founder level, and the nodes indicates states at the different levels. A child
process (oval box) is represented using its state-expanded hypergraph (hyperarcs
not shown) and is uniquely defined by a given state and action of its parent
process.
}%
\label{fig:HMDP}%
\end{figure}

A hierarchical MDP is an MDP with parameters defined in a special way, but
nevertheless in accordance with all usual rules and conditions relating to such
processes \citep{Kristensen00}. The basic idea of the hierarchical structure is
that stages of the process can be expanded to a so-called \emph{child process},
which again may expand stages further to new child processes leading to
multiple levels. To illustrate consider the MDP shown in \Fref{fig:HMDP}. The
process has three levels. At \texttt{Level 2} we have a set of ordinary MDPs
with finite time-horizon (one for each oval box) which all can be represented
using a state-expanded hypergraph (hyperarcs not shown, only hyperarcs
connecting processes are shown). An MDP at \texttt{Level 2} is uniquely defined
by a given state $s$ and action $a$ of its \emph{parent process} at
\texttt{Level 1} (illustrated by the arcs with head and tail node at
\texttt{Level 1} and \texttt{Level 2}, respectively). Moreover, when a child
process at \texttt{Level 2} terminates a transition from a state $s\in
\mathcal{S}_{N}$ of the child process to a state at the next stage of the
parent process occur (illustrated by the (hyper)arcs having head and tail at
\texttt{Level 2} and \texttt{Level 1}, respectively). Since a child process is
always defined by a stage, state and action of the parent process we have that
for instance a state at level 1 have an index vector
$\nu=(n_{0},s_{0},a_{0},n_{1},s_{1})$.

In general a state $s$ and action $a$ at level $l$ can be uniquely identified
using
%
\begin{align*}
	\nu_{s}&=(n_{0},s_{0},a_{0},n_{1},s_{1},\ldots,n_{l},s_{l}) \\
	\nu_{a}&=(n_{0},s_{0},a_{0},n_{1},s_{1},\ldots,n_{l},s_{l},a_{l}).
\end{align*}
%
The index vector of three states is illustrated in \Fref{fig:HMDP}.

Another way to identify a state or action is using an id number. Id numbers can
be seen when printing information about the model i R. This will be further
clarified in the example in \Fref{sec:machine}.

Now let us have a look at package. The package can be installed from R-Forge
using
%
\begin{Scode}{label=Install package,echo=true,eval=false}
install.packages("MDP",repos="http://r-forge.r-project.org")
\end{Scode}
%
and afterwards loaded
%
\begin{Scode}{label=Load package,echo=true}
library(MDP)
\end{Scode}
%
Help about the package can be seen by writing
%
\begin{Scode}{label=Package help,echo=true,eval=false}
?MDP
\end{Scode}
%
We illustrate the package capabilities by some examples in the next sections.



\section{Ordinary MDP with finite time-horizon}\label{sec:machine}

\begin{table}[tb]
\centering%
\begin{tabular}
[c]{lcccccc}\hline
$\left(  n,s\right)  $ & $\left(  1,0\right)  $ & $\left(  1,1\right)  $ &
$\left(  2,0\right)  $ & $\left(  2,1\right)  $ & $\left(  3,0\right)  $ &
$\left(  3,1\right)  $\\\hline
$reward  $ & 70 & 50 & 70 & 50 & 70 & 50\\
$s^{\prime}$ & $\left\{  0,1\right\}  $ & $\left\{  1,2\right\}  $ & $\left\{
0,1\right\}  $ & $\left\{  1,2\right\}  $ & $\left\{  0,1\right\}  $ &
$\left\{  1,2\right\}  $\\
$p_{n}\left(  \cdot\mid s,\texttt{nmt}\right)  $ & $\left\{  \frac{6}{10},\frac{4}%
{10}\right\}  $ & $\left\{  \frac{6}{10},\frac{4}{10}\right\}  $ & $\left\{
\frac{5}{10},\frac{5}{10}\right\}  $ & $\left\{  \frac{5}{10},\frac{5}%
{10}\right\}  $ & $\left\{  \frac{2}{10},\frac{8}{10}\right\}  $ & $\left\{
\frac{2}{10},\frac{8}{10}\right\}  $\\\hline
\end{tabular}
\caption{Input data for the machine replacement problem given action $nmt$.}%
\label{tab:data}%
\end{table}

Consider the machine replacement example from \citet{Relund06} where the
machine is always replaced after 4 years. The state of the machine may be:
good, average, and not working. Given the machine's state we may maintain the
machine. In this case the machine's state will be good at the next decision
epoch. Otherwise, the machine's state will not be better at next decision
epoch. When the machine is bought it may be either in state good or average.
Moreover, if the machine is not working it must be replaced.

The problem of when to replace the machine can be modelled using a Markov
decision process with $N=5$ decision epochs. We use system states \texttt{good}
(state 0),  $\texttt{average} $ (state 1), $\texttt{not working}$ (state 2) and
dummy state \texttt{replaced} together with actions buy (\texttt{buy}),
maintain (\texttt{mt}), no maintenance (\texttt{nmt}), and replace
(\texttt{rep}).

The set of states at stage zero $S_{0}$ contains a single dummy state $s_{0}$
representing the machine before knowing its initial state. The only possible
action is $buy$.

The cost of buying the machine is 100 with transition probability of 0.7 to
state \texttt{good} and 0.3 to state \texttt{average}. The reward (scrap value)
of replacing a machine is 30, 10, and 5 in state 0, 1, and 2, respectively. The
reward of the machine given action \texttt{mt} becomes 55, 40, and 30 given
state 0, 1, and 2, respectively. Moreover, the system enters state 0 with
probability 1 at the next stage. Finally, \Fref{tab:data} shows the reward,
transition states and probabilities given action \texttt{nmt}.

The state-expanded hypergraph is shown in \Fref{fig:state_hgf}. It contains a
hyperarc for each possible action $a$ given stage $n$ and state $s\in S_{n}$.
The head node of a hyperarc corresponds to the state of the system before
action $a$ is taken and the tail nodes to the possible system states after
action $a$ is taken.

\subsection{Generating the MDP}

We generate the model in R using the \texttt{binaryMDPWriter}:

\begin{Scode}{label=Generate replacement model,echo=true}
prefix<-"machine_"
w <- binaryMDPWriter(prefix)
w$setWeights(c("Net reward"))
w$process()
	w$stage()   # stage n=0
		w$state(label="Dummy")          # v=(0,0)
			w$action(label="buy", weights=-100, prob=c(1,0,0.7, 1,1,0.3), end=T)
		w$endState()
	w$endStage()
	w$stage()   # stage n=1
		w$state(label="good")           # v=(1,0)
			w$action(label="mt", weights=55, prob=c(1,0,1), end=T)
			w$action(label="nmt", weights=70, prob=c(1,0,0.6, 1,1,0.4), end=T)
		w$endState()
		w$state(label="average")        # v=(1,1)
			w$action(label="mt", weights=40, prob=c(1,0,1), end=T)
			w$action(label="nmt", weights=50, prob=c(1,1,0.6, 1,2,0.4), end=T)
		w$endState()
	w$endStage()
	w$stage()   # stage n=2
		w$state(label="good")           # v=(2,0)
			w$action(label="mt", weights=55, prob=c(1,0,1), end=T)
			w$action(label="nmt", weights=70, prob=c(1,0,0.5, 1,1,0.5), end=T)
		w$endState()
		w$state(label="average")        # v=(2,1)
			w$action(label="mt", weights=40, prob=c(1,0,1), end=T)
			w$action(label="nmt", weights=50, prob=c(1,1,0.5, 1,2,0.5), end=T)
		w$endState()
		w$state(label="not working")    # v=(2,2)
			w$action(label="mt", weights=30, prob=c(1,0,1), end=T)
			w$action(label="rep", weights=5, prob=c(1,3,1), end=T)
		w$endState()
	w$endStage()
	w$stage()   # stage n=3
		w$state(label="good")           # v=(3,0)
			w$action(label="mt", weights=55, prob=c(1,0,1), end=T)
			w$action(label="nmt", weights=70, prob=c(1,0,0.2, 1,1,0.8), end=T)
		w$endState()
		w$state(label="average")        # v=(3,1)
			w$action(label="mt", weights=40, prob=c(1,0,1), end=T)
			w$action(label="nmt", weights=50, prob=c(1,1,0.2, 1,2,0.8), end=T)
		w$endState()
		w$state(label="not working")    # v=(3,2)
			w$action(label="mt", weights=30, prob=c(1,0,1), end=T)
			w$action(label="rep", weights=5, prob=c(1,3,1), end=T)
		w$endState()
		w$state(label="replaced")       # v=(3,3)
			w$action(label="Dummy", weights=0, prob=c(1,3,1), end=T)
		w$endState()
	w$endStage()
	w$stage()   # stage n=4
		w$state(label="good", end=T)        # v=(4,0)
		w$state(label="average", end=T)     # v=(4,1)
		w$state(label="not working", end=T) # v=(4,2)
		w$state(label="replaced", end=T)    # v=(4,3)
	w$endStage()
w$endProcess()
w$closeWriter()
\end{Scode}
%
A set of binary files (all with prefix \texttt{machine\_}) containing the model
have now been generated. Note how the model is generated in a hierarchical way.
A process contain stages which contain states which again contain actions. An
action is defined by a set of weights (in this case the net reward) and a set
of transition probabilities. The probabilities are defined using a vector of
the form $(q_{0},i_{0},p_{0},\ldots,q_{r},i_{r},p_{r})$ stating that $r$
transitions are possible. Each triple $(q_{j},i_{j},p_{j})$ define a
transition. The number $q_{j}\in \{0,1,2\}$ is the scope of the transition. If
$q_{j} = 0$ then we make a transition to the next stage in the parent process,
if $q_{j} = 1$ we make a transition to the next stage in the current process
and if $q_{j} = 2$ we make a transition to the first stage in the child
process. The number $i_{j}$ define which state index we consider at the next
stage, e.g. if $i_{j}=0$ we consider the state with index 0 (remember index
start from zero). Finally, $p_{j}$ is the probability. For instance,
$(q_{j},i_{j},p_{j})=(1,3,0.2)$ specify that we have a transition with
probability 0.2 to the state with index 3 at the next stage of the current
process.

\subsection{Getting an overview}

Various information about the whole model can be seen in R:
%
\begin{Scode}{label=Model info (machine rep),echo=true}
stateIdxDf(prefix)      # states of the MDP with labels returned as a data frame
actionInfo(prefix)      # all action information of the MDP returned in a single data frame
\end{Scode}
%
Note that the data frame for the states show both each states unique id (a
single number) and index vector (the columns with names n<level> and s<level>).
For the action data frame each action is given an unique id.

\subsection{Finding the optimal policy}

A finite-horizon MDP can be solved using value iteration. First we load the
model:
%
\begin{Scode}{label=Load MDP (machine rep),echo=true}
mdp<-loadMDP(prefix)
mdp
\end{Scode}
%
The object is a list containing basic information about the model and a pointer
to the \cpp object containing the model. Next, we solve the MDP using value
iteration:
%
\begin{Scode}{label=Optimize MDP (machine rep),echo=true}
wLbl<-"Net reward"             # the weight we want to optimize
scrapValues<-c(30,10,5,0)   # scrap values (the values of the 4 states at stage 4)
valueIte(mdp, wLbl, termValues=scrapValues)
\end{Scode}
%
The MDP has now been optimized. The optimal policy can be extracted using:
%
\begin{Scode}{label=Package help,echo=true}
policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId
states<-stateIdxDf(prefix)              # information about the states
policy<-merge(states,policy)            # merge the two data frames
policyW<-getPolicyW(mdp, wLbl)            # the optimal rewards of the policy
policy<-merge(policy,policyW)           # add the rewards
policy
\end{Scode}

\subsection{Modifying the MDP}

It is possible to do some manipulations to the MDP already stored in memory.
You may remove some actions from the MDP. For instance assume that it is not
possible to maintain the machine at stage 1. We remove the \texttt{mt} actions
at stage 1:
%
\begin{Scode}{label=Remove action (machine rep),echo=true}
removeAction(mdp, sId=1, iA=0)  # remove action 0 at the state with sId=1
removeAction(mdp, sId=2, iA=0)
\end{Scode}
%
Next, we try to optimize the MDP:
%
\begin{Scode}{label=Optimize MDP (machine rep),echo=true}
valueIte(mdp, wLbl, termValues=scrapValues)
policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId
states<-stateIdxDf(prefix)              # information about the states
policy<-merge(states,policy)            # merge the two data frames
policyW<-getPolicyW(mdp, wLbl)            # the optimal rewards of the policy
policy<-merge(policy,policyW)           # add the rewards
policy
\end{Scode}
%
We could also have removed the \texttt{mt} actions by fixing the \texttt{nmt}
actions:
%
\begin{Scode}{label=Fix action (machine rep),echo=true,eval=false}
fixAction(mdp, sId=1, iA=1)  # remove all actions at state sId=1 except action 1
fixAction(mdp, sId=2, iA=1)
\end{Scode}
%
You reset the MDP again with:
%
\begin{Scode}{label=Reset MDP (machine rep),echo=true}
resetActions(mdp)   # reset the MDP such that all actions are used
\end{Scode}
%

It is possible to modify the weights of an action, e.g. assume that the cost of
buying the machine is 50 instead of 100:
%
\begin{Scode}{label=Set action weight (machine rep),echo=true,eval=true}
setActionWeight(mdp, w=-50, sId=0, iA=0, wLbl)
\end{Scode}
%
The solution now becomes:
%
\begin{Scode}{label=Optimize MDP (machine rep),echo=false}
valueIte(mdp, wLbl, termValues=scrapValues)
policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId
states<-stateIdxDf(prefix)              # information about the states
policy<-merge(states,policy)            # merge the two data frames
policyW<-getPolicyW(mdp, wLbl)            # the optimal rewards of the policy
policy<-merge(policy,policyW)           # add the rewards
policy
\end{Scode}
%

\subsection{Evaluating a specific policy}

We may evaluate a certain policy, e.g. the policy always to maintain the machine:
%
\begin{Scode}{label=Set policy (machine rep),echo=true,eval=true}
setActionWeight(mdp, w=-100, sId=0, iA=0, wLbl)     # set weight to original
policy<-data.frame(sId=states$sId,iA=0)
policy<-as.matrix(policy)
setPolicy(mdp, policy)
\end{Scode}
%
If the policy matrix does not contain all states then the actions from the
previous optimal policy are used. Now let us calculate the expected reward of
that policy:
%
\begin{Scode}{label=Calc reward (machine rep),echo=true}
calcWeights(mdp, wLbl, termValues=scrapValues)
policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId
states<-stateIdxDf(prefix)              # information about the states
policy<-merge(states,policy)            # merge the two data frames
policyW<-getPolicyW(mdp, wLbl)            # the optimal rewards of the policy
policy<-merge(policy,policyW)           # add the rewards
policy
\end{Scode}
%




\section{Ordinary MDP with infinite time-horizon}\label{sec:sow}

For a sow it is relevant to consider at regular time intervals whether to keep
the sow for a period more or replace it by a new sow. Let a stage denote the
time between two litters. At the time of a stage we observe the state of the
sow which in this simple example is the current litter size \texttt{small},
\texttt{average} or \texttt{big}.

Two actions are possible, namely, \texttt{keep} or \texttt{replace}. Given an
action 3 weights are defined the duration, net reward and the number of
piglets. The weights and transition probabilities of an action are specified
explicit when we generate the MDP:
%
\begin{Scode}{label=Generate sow DMP,echo=true}
prefix="sow_"
w<-binaryMDPWriter(prefix)
w$setWeights(c("Duration", "Net reward", "Piglets"))
w$process()
	w$stage()
		w$state(label="Small litter")
			w$action(label="Keep",weights=c(1,10000,8),prob=c(1,0,0.6, 1,1,0.3, 1,2,0.1))
			w$endAction()
			w$action(label="Replace",weights=c(1,9000,8),prob=c(1,0,1/3, 1,1,1/3, 1,2,1/3))
			w$endAction()
		w$endState()
		w$state(label="Average litter")
			w$action(label="Keep",weights=c(1,12000,11),prob=c(1,0,0.2, 1,1,0.6, 1,2,0.2))
			w$endAction()
			w$action(label="Replace",weights=c(1,11000,11),prob=c(1,0,1/3, 1,1,1/3, 1,2,1/3))
			w$endAction()
		w$endState()
		w$state(label="Big litter")
			w$action(label="Keep",weights=c(1,14000,14),prob=c(1,0,0.1, 1,1,0.3, 1,2,0.6))
			w$endAction()
			w$action(label="Replace",weights=c(1,13000,14),prob=c(1,0,1/3, 1,1,1/3, 1,2,1/3))
			w$endAction()
		w$endState()
	w$endStage()
w$endProcess()
w$closeWriter()
\end{Scode}
%
Note that since we only have one stage at the founder level (level 0) the MDP
have an infinite time-horizon. That is, the MDP model a sow and all it
successors (when a sow is replaced, a new is always inserted).

Let us have a overview over the model
%
\begin{Scode}{label=Model info (sow rep),echo=true}
stateIdxDf(prefix)      # states of the MDP with labels returned as a data frame
actionInfo(prefix)      # all action information of the MDP returned in a single data frame
\end{Scode}
%

\subsection{Finding the optimal policy under different criteria}

Let us try to optimize our model under the expected discounted reward
criterion. Here two optimization techniques are possible. Let us first have a
look at value iteration which provide an approximate solution.
%
\begin{Scode}{label=Value ite (sow rep),echo=true}
mdp<-loadMDP(prefix)
mdp

## solve the MDP using value iteration
wLbl<-"Net reward"         # the weight we want to optimize
durLbl<-"Duration"         # the duration/time label
rate<-0.1               # discount rate
rateBase<-1             # rate base
valueIte(mdp, wLbl, durLbl, rate, rateBase, times = 10000, eps = 0.00001)

policy<-getPolicy(mdp, labels=TRUE)     # optimal policy for each sId
states<-stateIdxDf(prefix)              # information about the states
policy<-merge(states,policy)            # merge the two data frames
policyW<-getPolicyW(mdp, wLbl)            # the optimal rewards of the policy
policy<-merge(policy,policyW)           # add the rewards
policy
\end{Scode}
%
First note that the we optimize the MDP for a specific interest rate which
according to a rate basis, i.e. if the rate is 0.1 and the rate base is 4 then
the discount rate over one time unit is $\exp(-0.1/4) =
\Sexpr{round(exp(-0.1/4),4)}$. The discount rate over t time units then becomes
\begin{equation*}
	\delta(t) = \exp(-\texttt{rate}/\texttt{rateBase})^{t}.
\end{equation*}
Second, the parameter \texttt{times} denote an upper bound on the number of iterations.
Finally, the parameter \texttt{eps} denote the $\epsilon$ for stopping the algorithm. If
the maximum difference between the expected discounted reward of 2 states is
below $\epsilon$ then the algorithm stops, i.e the policy becomes epsilon
optimal (see \citep{Puterman94} p161).

Let us have a look at how value iteration performs for each iteration.
%
\begin{Scode}{label=Value ite steps (sow rep),echo=true,results=hide}
termValues<-c(0,0,0)
iterations<-1:211
df<-data.frame(n=iterations,a1=NA,V1=NA,D1=NA,a2=NA,V2=NA,D2=NA,a3=NA,V3=NA,D3=NA)
for (i in iterations) {
	valueIte(mdp, wLbl, durLbl, rate, rateBase, times = 1, eps = 0.00001, termValues)
	a<-getPolicy(mdp, labels=T)
	w<-getPolicyW(mdp, wLbl)
	res<-rep(NA,10)
	res[1]<-i
	res[2]<-a[1,2]
	res[3]<-round(w[1,2],2)
	res[4]<-round(w[1,2]-termValues[1],2)
	res[5]<-a[2,2]
	res[6]<-round(w[2,2],2)
	res[7]<-round(w[2,2]-termValues[2],2)
	res[8]<-a[3,2]
	res[9]<-round(w[3,2],2)
	res[10]<-round(w[3,2]-termValues[3],2)
	df[i,]<-res
	termValues<-w[,2]
}
df[c(1:3,51:53,151:153,210:211),]
\end{Scode}
%
\begin{Scode}{label=Value ite steps print (sow rep),echo=false}
df[c(1:3,51:53,151:153,210:211),]
\end{Scode}
%
Note value iteration converges very slowly to the optimal value.

Another optimization technique is policy iteration which finds an optimal
policy. Let us solve the MDP under the expected discount criterion.
%
\begin{Scode}{label=Policy ite discount (sow rep),echo=true}
policyIteDiscount(mdp, wLbl, durLbl, rate, rateBase)
policy<-getPolicy(mdp, labels=TRUE)
sIdx<-stateIdxDf(prefix)
policy<-merge(sIdx,policy)
policyW<-getPolicyW(mdp, wLbl)
policy<-merge(policy,policyW)
rpo<-calcRPO(mdp, wLbl, iA=0, criterion="discount", dur=durLbl, rate=rate, rateBase=rateBase)
policy<-merge(policy,rpo)
policy$w1<-round(policy$w1,0)
policy$rpo<-round(policy$rpo,0)
policy
\end{Scode}
%
First, note that policy iteration converges fast only 3 iterations are needed.
Second, we also here try to calculate the \emph{retention payoff} (\emph{RPO})
or opportunity cost with respect to action \texttt{keep} (action index 0). The
RPO is the discounted gain of keeping the sow until her optimal replacement
time instead of replacing her now. For instance if we consider a sow with a big
litter we loose \Sexpr{round(policy$rpo[3],0)} by replacing the sow instead
keeping her to her until her optimal replacement time. That is, if the RPO is
positive the optimal decision is to keep the sow and if the RPO is negative the
optimal decision is to replace the sow.

Other criteria can also be optimized using policy iteration. For instance we
can maximize the average reward over time:
%
\begin{Scode}{label=Policy ite ave reward over time (sow rep),echo=true}
g<-policyIteAve(mdp, wLbl, durLbl)
policy<-getPolicy(mdp, labels=TRUE)
policy<-merge(sIdx,policy)
policyW<-getPolicyW(mdp, wLbl)
policy<-merge(policy,policyW)
rpo<-calcRPO(mdp, wLbl, iA=0, criterion="average", dur = durLbl, g=g)
policy<-merge(policy,rpo)
policy$w1<-round(policy$w1,0)
policy$rpo<-round(policy$rpo,0)
policy
\end{Scode}
%
Here $g$ is the average reward pr time unit and the weights are relative values
compared to the \texttt{big litter} state.

We may also maximize the average reward over piglets:
%
\begin{Scode}{label=Policy ite ave reward over litter(sow rep),echo=true}
durLbl<-"Piglets"
g<-policyIteAve(mdp, wLbl, dur=durLbl)
policy<-getPolicy(mdp, labels=TRUE)
policy<-merge(sIdx,policy)
policyW<-getPolicyW(mdp, wLbl)
policy<-merge(policy,policyW)
rpo<-calcRPO(mdp, wLbl, iA=0, criterion="average", dur = durLbl, g=g)
policy<-merge(policy,rpo)
policy$w1<-round(policy$w1,0)
policy$rpo<-round(policy$rpo,0)
policy
\end{Scode}
%
Here $g$ is the average reward pr piglet and the weights are relative values
compared to the \texttt{big litter} state.

\subsection{Calculating other key figures for the optimal policy}

Consider the optimal policy under the expected discounted reward criterion:
%
\begin{Scode}{label=Policy ite discount (sow rep),echo=true}
policyIteDiscount(mdp, wLbl, durLbl, rate, rateBase)
policy<-getPolicy(mdp, labels=TRUE)
sIdx<-stateIdxDf(prefix)
policy<-merge(sIdx,policy)
policyW<-getPolicyW(mdp, wLbl)
policy<-merge(policy,policyW)
rpo<-calcRPO(mdp, wLbl, iA=0, criterion="discount", dur=durLbl, rate=rate, rateBase=rateBase)
policy<-merge(policy,rpo)
policy$w1<-round(policy$w1,0)
policy$rpo<-round(policy$rpo,0)
policy
\end{Scode}
%
Since other weights are defined for each action we can calculate the average
number of piglets per time unit under the optimal policy:
%
\begin{Scode}{label=Piglets/time (sow rep),echo=true}
g<-calcWeights(mdp, w="Piglets", criterion="average", dur = "Duration")
g
\end{Scode}
%
or the average reward per piglet:
%
\begin{Scode}{label=Reward/piglet (sow rep),echo=true}
g<-calcWeights(mdp, w="Net reward", criterion="average", dur = "Piglets")
g
\end{Scode}
%








\section{Hierarchical MDP with infinite time-horizon}\label{sec:cow}

We consider a cow replacement problem where we want to represent the age of the
cow, i.e. the lactation number of the cow. During a lactation a cow may have a
high, average or low yield. We assume that a cow is always replaced after 4
lactations.

In addition to lactation and milk yield we also want to take the genetic merit
into account which is either bad, average or good. When a cow is replaced we
assume that the probability of a bad, average or good heifer is equal.

We formulate the problem as a hierarchical MDP with 2 levels. At level 0 the
states are the genetic merit and the length of a stage is a life of a cow. At
level 1 a stage describe a lactation and states describe the yield. Decisions
at level 1 are \texttt{keep} or \texttt{replace}.

Note the MDP runs over an infinite time-horizon at the founder level where each
state (genetic merit) define an ordinary MDP at level 1 with 4 lactations.

\subsection{Generating the MDP}

To generate the MDP we need to know the weights and transition probabilities which
are provided in a csv file. To ease the understanding we provide 2 functions
for reading from the csv:
%
\begin{Scode}{label=Generate cow MDP functions,echo=true}
cowDf<-read.csv("mdp_examples_files/cow.csv")
head(cowDf)

lev1W<-function(s0Idx,n1Idx,s1Idx,a1Lbl) {
	r<-subset(cowDf,s0==s0Idx & n1==n1Idx & s1==s1Idx & label==a1Lbl)
	return(as.numeric(r[5:7]))
}
lev1W(2,2,1,'Keep')     # good genetic merit, lactation 2, avg yield, keep action

lev1Pr<-function(s0Idx,n1Idx,s1Idx,a1Lbl) {
	r<-subset(cowDf,s0==s0Idx & n1==n1Idx & s1==s1Idx & label==a1Lbl)
	return(as.numeric(r[8:16]))
}
lev1Pr(2,2,1,'Replace') # good genetic merit, lactation 2, avg yield, replace action
\end{Scode}
%

%
\begin{Scode}{label=Generate cow MDP,echo=true}
lblS0<-c('Bad genetic level','Avg genetic level','Good genetic level')
lblS1<-c('Low yield','Avg yield','High yield')
prefix<-"cow_"
w<-binaryMDPWriter(prefix)
w$setWeights(c("Duration", "Net reward", "Yield"))
w$process()
	w$stage()   # stage 0 at founder level
		for (s0 in 0:2) {
			w$state(label=lblS0[s0+1])   # state at founder
				w$action(label="Keep", weights=c(0,0,0), prob=c(2,0,1))   # action at founder
					w$process()
						w$stage()   # dummy stage at level 1
							 w$state(label="Dummy")
								w$action(label="Dummy", weights=c(0,0,0), prob=c(1,0,1/3, 1,1,1/3, 1,2,1/3))
								w$endAction()
							 w$endState()
						w$endStage()
						for (d1 in 1:4) {
							w$stage()   # stage at level 1
								for (s1 in 0:2) {
									w$state(label=lblS1[s1+1])
										if (d1!=4) {
											w$action(label="Keep", weights=lev1W(s0,d1,s1,"Keep"), prob=lev1Pr(s0,d1,s1,"Keep"))
											w$endAction()
										}
										w$action(label="Replace", weights=lev1W(s0,d1,s1,"Replace"), prob=lev1Pr(s0,d1,s1,"Replace"))
										w$endAction()
									w$endState()
								}
							w$endStage()
						}
					w$endProcess()
				w$endAction()
			w$endState()
		}
	w$endStage()
w$endProcess()
w$closeWriter()
\end{Scode}
%

\subsection{Finding the optimal policy}

We find the optimal policy under the expected discounted reward criterion the
MDP using policy iteration:
%
\begin{Scode}{label=Optimize (cow),echo=true}
## solve under discount criterion
mdp<-loadMDP(prefix)
wLbl<-"Net reward"         # the weight we want to optimize (net reward)
durLbl<-"Duration"         # the duration/time label
rate<-0.1               # discount rate
rateBase<-1             # rate base, i.e. given a duration of t the rate is
sIdx<-stateIdxDf(prefix)
policyIteDiscount(mdp, wLbl, durLbl, rate, rateBase)
policy<-getPolicy(mdp, labels=TRUE)
policy<-merge(sIdx,policy)
policyW<-getPolicyW(mdp, wLbl)
policy<-merge(policy,policyW)
rpo<-calcRPO(mdp, wLbl, iA=0, criterion="discount", dur=durLbl, rate=rate, rateBase=rateBase)
policy<-merge(policy,rpo)
policy
\end{Scode}
%


\subsection{Visual view of the hierarchical structure of the MDP}

The program MLHMP is a Java implementation of some algorithms for solving MDPs
\citep{Kristensen03}. It have a graphical user interface where the hierarchical
structure of the MDP can be visualized. A model can be loaded into MLHMP using
the HMP format which is an XML file containing the model. The MDP package
contain a function for converting the binary files to the HMP format:
%
\begin{Scode}{label=Convert to HMP (cow rep),echo=true}
convertBinary2HMP(prefix)
\end{Scode}
%
The function create the file \texttt{cow\_converted.hmp} which can be opened by
MLHMP.



%Some of the options are (default in parentheses)
%
%\begin{description}
%  \item[cache:]   logical (false). If you use the weaver driver the code chunk will be cached.
%  \item[echo:]    logical (true). Include R code in output?
%  \item[eval:]    logical (true). If false the code chunk is not evaluated, and hence no text
%         or graphical output are produced.
%  \item[results:] character string (verbatim). If verbatim, the output of S commands is
%         included in the verbatim-like Soutput environment. If tex, the output is
%         taken to be already proper latex markup and included as is. If hide then
%         all output is completely suppressed (but the code executed during the
%         weave).
%  \item[print:]   logical (FALSE). If TRUE, each expression in the code chunk is wrapped into
%         a print() statement before evaluation, such that the values of all
%         expressions become visible.
%  \item[term:]    logical (TRUE). If TRUE, visibility of values emulates an interactive R
%         session: values of assignments are not printed, values of single objects
%         are printed. If FALSE, output comes only from explicit print or cat
%         statements.
%  \item[include:] logical (TRUE), indicating whether input statements for text output
%         and includegraphics statements for figures should be auto-generated. Use
%         include = FALSE if the output should appear in a different place than
%         the code chunk (by placing the input line manually).
%  \item[fig:]     logical (FALSE), indicating whether the code chunk produces graphical
%         output. Note that only one figure per code chunk can be processed this
%         way.
%  \item[eps:]     logical (TRUE), indicating whether EPS figures shall be generated.
%         Ignored if fig = FALSE.
%  \item[pdf:]     logical (TRUE), indicating whether PDF figures shall be generated.
%         Ignored if fig = FALSE.
%  \item[width:]   numeric (6), width of figures in inch.
%  \item[height:]  numeric (6), height of figures in inch.
%\end{description}




%-------------------

\bibliographystyle{plainnat}
\bibliography{mdp_examples_files/litt}     % remember to make a local copy when paper finished (use JabRef)

%% Next line included for gather purpose in WinEdt only:
%input "litt.bib"

\end{document}
